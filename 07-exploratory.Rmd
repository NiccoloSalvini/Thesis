# Exploratory Analysis {#exploratory}

<!--  You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).-->

```{r options, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,  
  strip.white = TRUE,
  message = FALSE,
  cache = FALSE,
  echo = FALSE
)

# libraries are in libs.R
# source(here::here("libs.R"))

library(readr)
library(kableExtra)
library(tidyverse)
library(broom)
library(viridis)
library(leaflet)
library(sp)
library(glue)
library(ggpmisc)
library(gstat)
library(stringi)
library(scales)
library(plotly)
library(INLA)
## missing profile
library(caret)
library(tidyverse)
library(naniar)
library(forcats)
library(visdat)
library(ComplexHeatmap)
library(patchwork)
library(ggridges)


## Personal ggplot theme 
theme_nicco = function (base_size = 11, base_family = "") {
  theme_bw() %+replace% 
    theme(
      text = element_text(family = "sans", size = 12),
      plot.title = element_text(face = "bold", size = 14, margin=margin(0,0,30,0)),
      panel.background  = element_blank(),
      axis.ticks = element_line(colour = "grey70", size = 0.2),
      plot.background = element_rect(fill="white", colour=NA),
      panel.border = element_rect(linetype = "blank", fill = NA),
      legend.background = element_rect(fill="transparent", colour=NA),
      legend.key = element_rect(fill="transparent", colour=NA)
    )
}
```

Data comes packed into the REST API end point `*/complete` in .json format. Data can be filtered out On the basis of the options set in the API endpoint argument body. Some of the options might regard the `city` in which it is aevaluated the real estate market, `npages` as the number of pages to be scraped, `type` as the choice between rental of selling. For further documentation on how to structure the API endpoint query refer to section \@ref(APIdocs).  Since to the analysis purposes data should come from the same source (e.g. Milan rental real estate within "circonvallazione") a dedicated endpoint boolean option `.thesis` is passed in the argument body. What the API option under the hood is doing is specifying a structured and already filtered URL to be passed to the scraping endppoint. By securing the same URL to the scraping functions data is forced to come from the same URL source. The idea behind this concept can be thought as refreshing everyday the same immobiliare.it URL. API endpoint by default also specifies 10 pages to be scraped, in this case 120 is provided leading to to 3000 data points. The `*` refers to the EC2 public DNS that is `ec2-18-224-40-67.us-east-2.compute.amazonaws.com`

`http://*/complete/120/milano/affitto/.thesis=true`

As a further source data can also be accessed through the mondgoDB credentials with the cloud ATLAS database by picking up the latest .csv file generated. For run time reasons also related to the bookdown files continuous building the API endpoint is called the day before the presentation so that the latest .csv file is available. As a consequence code chunks outputs are all cached due to heavy computation.
Interactive  maps are done with Leaflet, the result of which is a leaflet map object which can be piped to other leaflet functions. This permits multiple map layers and many control options to be added interactively (LaTex output is statically generated)

A preliminary API data exploratory analysis evidences 34 covariates and 250 rows, which are once again conditioned to the query sent to the API. Immobiliare.it furnishes many information regarding property attributes and estate agency circumstances. Data displays many NA in some of the columns but georeference coordinates, due to the design of scraping functions, are in any case present. 



```{r data_loading}

dati = read_csv("data/data.csv", locale = readr::locale(encoding = "latin1")) %>%
  dplyr::select(-X1) 
```

```{r covar_table, fig.cap="Covariate explanation table"}

### fai la tabella delle colonne
### 
ref =c(
"ID of the apartements",
"latitude coordinate",
"longitude coordinate",
"the complete address: street name and number",
"the condominium monthly expenses",
"the age in which the building was contructed",
"the property floor",
"indipendent property type versus apartement type",
"specification of the type and number of rooms",
"property type residential or not",
"the actual status of the house, ristrutturato, nuovo, abitabile",
"the heating system Cen_Rad_Gas (centralizzato a radiatori, alim a gas), Cen_Rad_Met,",
"air conditioning hot and cold, Autonomo, freddo/caldo, Centralizzato, freddo/caldo",
"the date of publication of the advertisement",
"land registry information",
"apartement main characteristics",
"number of photos displayed in the advertisement",
"real estate agency name",
"If the price is lowered it flags the starting price",
"If the price is lowered it flags the current price",
"If the price is lowered indicates the days passed since the price has changed",
"If the price is lowered indicates the date the price has changed",
"the energy class according to the land registers",
"the type of contract",
"if it is still avaiable or already rented",
"the total number of the building floors",
"number of parking box or garages avaibable in the property",
"estate agency review, long chr string",
"it if has multimedia option, such as 3D house vitualization home experience or videos",
"the monthly price <- response",
"square meters footage",
"the number of rooms in the house, and their types",
"title of published advertisement"
)

names = dati %>%  names
tibble(
  name = names,
  ref  = ref 
) %>% 
  knitr::kable(booktabs = T, longtable = T)
  # 
  # kbl(booktabs = T, longtable = T) %>%
  # kable_styling(full_width = F,font_size = 8) %>%
  # column_spec(1, bold = T) %>%
  # column_spec(2, width = "20em")

```


## Data preparation 

```{r data_prep, message=FALSE}
datiprep = dati %>% 
  select(-TPPROP) %>% 
  ## FLOOR
  mutate(FLOOR = str_remove_all(FLOOR, "°")) %>% 
  separate(FLOOR, c("FLOOR", "ASCENSORE","ACCDISABILI"), sep = ", ", convert = T) %>%
  replace_na(list(ASCENSORE = "no", ACCDISABILI = "no")) %>% 
  mutate(ASCENSORE = recode(ASCENSORE,"con accesso disabili" = "no")) %>%  
  mutate(FLOOR = recode(FLOOR, "1" = "1 piano", "2" = "2 piano", "3" = "2 piano", "4" = "4 piano", "5" = "5 piano", "6" = "6 piano", "7" = "7 piano", "8" = "8 piano", "9" = "9 piano")) %>% 
  ## TOTPIANI
  # mutate(TOTPIANI = str_remove_all(TOTPIANI, " piani")) %>% 
  ## LOCALI
  mutate(LOCALI = if_else(substr(LOCALI,start = 1,stop = 1) == 1, "1", LOCALI)) %>% 
  separate(LOCALI, c("TOTLOCALI", "REST"), sep = "\\s", extra = "merge") %>% 
  separate(REST, c("CAMERELETTO","ALTRO","BAGNO", "CUCINA"), sep = ", ", convert = T, fill = "left")  %>% 
  ## CAMERALETTO
  mutate(CAMERELETTO = if_else(TOTLOCALI== 1, "(1 camera da letto", CAMERELETTO)) %>% 
  mutate(CAMERELETTO = if_else(is.na(CAMERELETTO),ALTRO , CAMERELETTO)) %>% 
  mutate(CAMERELETTO = if_else(CAMERELETTO== "locali",NA_character_ , CAMERELETTO)) %>%  
  mutate(CAMERELETTO = if_else(CAMERELETTO== "camere da letto", paste0(TOTLOCALI," camere da letto"), CAMERELETTO)) %>% 
  mutate(CAMERELETTO = str_remove(CAMERELETTO, "\\(")) %>% 
  ## ALTRO
  mutate(ALTRO = if_else(BAGNO== "1 altro)","1 altro)" , ALTRO)) %>% 
  replace_with_na(replace = list(ALTRO = c("camere da letto", "locali"))) %>% 
  mutate(ALTRO = str_remove(ALTRO, "\\)")) %>% 
  ## BAGNO
  mutate(BAGNO = if_else(CUCINA== "1 bagno", "1 bagno", BAGNO)) %>% 
  replace_with_na(replace = list(BAGNO = "1 altro)")) %>% 
  mutate(BAGNO = if_else(TOTLOCALI== "1", "1 bagno", BAGNO)) %>% 
  ## CUCINA (ne ho perse qualcuna)
  replace_with_na(replace = list(CUCINA = "1 bagno")) %>% 
  mutate(TOTLOCALI = recode(TOTLOCALI, "1" = "Monolocale", "2" = "Bilocale", "3" = "Trilocale", "4" = "Quadrilocale", "5" = "Pentalocale")) ## non proviene più di sei locali 
  
  ## APTCHAR (non implementato)
  # aptchar = datiprep %>% 
  #   select(APTCHAR) %>%
  #   mutate(APTCHAR = str_remove_all(APTCHAR, "\\-")) %>% 
  #   mutate(APTCHAR = str_trim(APTCHAR)) %>% 
  #   separate(APTCHAR, into = LETTERS[1:15],"\\s{2,}")
  #   
  #   

```

Data needs to undergo to many previous cleaning preprocess steps, this is a forced stage since API data comes in human readable format, which is not prepared to be modeled. Cleaning steps mainly regards:

- encoding from UTF-8 to Latin due to Italian characters incorrectly parsed.
- *FLOORS* covariate needs to be separated by its *ASCENSORE* and *ACCDISABILI* components, adding 2 more bivariate covariates.
- *LOCALI* needs to be separated too. 5 category levels drain out: *TOTLOCALI*, *CAMERELETTO*, *ALTRO*, *BAGNO*, *CUCINA*. *NROOM* is a duplicate for *TOTLOCALI*, so it is discarded.
- *APTCHAR* is a list column so that each observation has different categories inside. The preprocess step includes unnesting the list by creating as many bivariate columns as elements in the list. Then new columns flag the existence of the characteristics in the apartment. A slice for the fist APTCHAR observation displays:

`r datiprep %>%  select(APTCHAR) %>%  slice(sample(1))  %>% pull`

### Maps and Geo-Visualisations

Geographic coordinates can be represented on a map in order to reveal first symptoms of spatial autocorrelation. Observations are spread almost equally throughout the surface even though the response var *PRICE* indicates unsurprisingly that higher prices are nearer to the city center.
The map in figure \@ref(fig:leaflet_visuals) is a leaflet object, which can needs to be overlapped with layers indicating different maps projections. This is interactive in the .html version, and static is proposed in the .pdf output version. The map object takes a input the latitute and longitude coordinates coming from THE API, and they do not need any CRS (Coordinate Reference System) projection since leaflet can accept the data type.

```{r LeafletVisuals, out.width="100%", fig.cap="Leaflet Map"}

if(knitr::is_latex_output()){
    knitr::include_graphics("images/leaflet_prezzi.jpg")
} else if(knitr::is_html_output()){  
  dati2 = dati %>%
    dplyr::select(LAT, LONG, PRICE )
  pal = colorBin("viridis", bins = c(1000, 1500, 2000, 2500, 5000))
  
  leaflet(dati2) %>%
    # addProviderTiles(providers$CartoDB.Positron) %>% bug in Lealfet while caching
    setView(lat = 45.474211, lng = 9.191383, zoom = 13) %>%
    addMiniMap() %>% 
    addTiles() %>% 
    addCircles(lng = ~LONG, 
               lat = ~LAT, 
               color = ~ pal(PRICE),
               weight = 10,
               radius = ~log(PRICE)) %>%
    addLegend("bottomright",
      pal = pal, values = ~PRICE,
      title = "Prices") %>%
    addScaleBar(position = c("bottomleft"))
  }
```

(other more tmap and ggplot)

## Counts and First Orientations

Arranged Counts for categorical columns can give a sense of the distribution of categories across the dataset suggesting also which predictor to include in the model. The visualization in figure \@ref(fig:fctCounts) offers the rearranged factor *TOTLOCALI*. 
Bilocali are the most common option for rent, then trilocali comes after. The intuition behind suggests that Milan rental market is oriented to "lighter" accommodations in terms of space and squarefootage. This should comes natural since Milan is both a vivid study and working area, so short stayings are warmly welcomed.

```{r fctCounts, fig.cap="Most common housedolds categories"} 
datiprep %>%
  mutate(TOTLOCALI = as_factor(TOTLOCALI)) %>% 
  count(TOTLOCALI, sort = TRUE) %>% 
  mutate(TOTLOCALI = fct_reorder(TOTLOCALI, n)) %>%
  ggplot(aes(n, TOTLOCALI)) +
  geom_col() +
  labs(x = "# of houses",
       y = "",
       title = "Most common housedolds categories in Milan")+
  theme_nicco()
```

Two of the most requested features for comfort and livability in rents are the heating/cooling systems installed. Moreover rental market demand, regardless of the rent duration, strives for a ready-to-accomodate offer to meet clients needs. In this sense accomodation coming with the newest and most techonological systems are naturally preferred with respect the contrary. 
x-axis in figure \@ref(fig:PricePerAc) represents log_10 price for both of the two plots. Logarithmic scale is needed to smooth distributions and the resulting price interpretation have to considered into relative percent changes. Furthermore factors are reordered with respect to decreasing price.  
y-axis are the different level for the categorical variables recoded from the original data due to simplify lables and to hold plot dimension. Moreover counts per level are expressed between brackets close to their respective factor.
The top plot displays the most prevalent heating systems categories, among which the most prevalent is "Cen_Rad_Met" by far. This fact is extremely important since metano is a green energy source and if the adoption is wide spread and pipelines are well organized than it brings enormous benefit to the city. As a consequence one major concern regards that for many years policies have been oriented to reduce vehicles emission (euro1 euro2...) instead of focusing on house emissions. This was also a consequence of the lack of house data especially in rural areas. According to data there are still a 15% portion of houses powered by oil fired. 
Then in bottom plot Jittering is then applied to point out the number of outliers outside the IQR (Inter Quantile Range) .25 and their impact on the distribution. A first conclusion is that outliers are mainly located in autonomous systems, which leads of course to believe that the most expensive houses are heated by autonomoius heating systems. Indedd in any case this fact that does not affect monthly price. The overlapping IQR signifies that the covariates levels do not impact the response variable.


```{r PricePerAc, fig.cap="Log Monthly Price per Heating/Cooling system?"}

pph =datiprep %>%
  mutate(HEATING = fct_lump(HEATING, 4)) %>%
  filter(HEATING != "Other") %>%
  filter(HEATING != is.na(HEATING)) %>% 
  mutate(HEATING = recode(HEATING,"Centralizzato, a radiatori" = "Cen_Rad",
                          "Centralizzato, a radiatori, alimentato a gas" = "Cen_Rad_Gas",
                          "Centralizzato, a radiatori, alimentato a metano" = "Cen_Rad_Met",
                          "Autonomo, a radiatori, alimentato a metano" = "Aut_Rad_Met",
                          "Autonomo, a radiatori, alimentato a gas" = "Aut_Rad_Gas")) %>% 
  add_count(HEATING, name = "HEATING_count") %>% 
  mutate(HEATING = glue("{ HEATING } ({ HEATING_count })"),
         HEATING  = fct_reorder(HEATING, PRICE)) %>%
  ggplot(aes(PRICE, HEATING)) +
  ggtitle("Log Monthly Price per Heating and AC systems?")+
  geom_boxplot() +
  geom_jitter(width = 0, height = .1, alpha = .25) +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(y = "",
       x = "") +
  theme_nicco()

ppa =datiprep %>%
  mutate(AC = fct_lump(AC, 4)) %>%
  # filter(AC != "Other") %>%
  filter(AC != is.na(AC)) %>% 
  add_count(AC, name = "AC_count") %>% 
  mutate(AC = glue("{ AC } ({ AC_count })"),
         AC  = fct_reorder(AC, PRICE)) %>%
  ggplot(aes(PRICE, AC)) +
  geom_boxplot() +
  geom_jitter(width = 0, height = .1, alpha = .25) +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(x = "Price (EUR)",
       y = "") +
  theme_nicco()

pph / ppa

```


this visualization intersects allows to discover bimodality in the response variable.  Log scales was needed since they are all veru skewd and log scale then is needed also in the model.

(qui ci puoi mettere a confronto per variabile bianria, così vedi cosa includere nel modello esempio sotto dove commentato, )

```{r}
## non filled
non_fill = datiprep %>%
  add_count(TOTLOCALI, name  = "TOTLOCALI_count") %>% 
  mutate(TOTLOCALI = glue("{ TOTLOCALI } ({ TOTLOCALI_count })"),
         TOTLOCALI = fct_reorder(TOTLOCALI, PRICE)) %>% 
  ggplot(aes(PRICE, TOTLOCALI, fill = ACCDISABILI)) + 
  geom_density_ridges(alpha = .5) +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(x = "Price (EUR)",
       y = "",
       title = "How much do Cost items in each category cost?") +
  theme_nicco()


with_fill = datiprep %>%
  add_count(TOTLOCALI, name  = "TOTLOCALI_count") %>% 
  mutate(TOTLOCALI = glue("{ TOTLOCALI } ({ TOTLOCALI_count })"),
         TOTLOCALI = fct_reorder(TOTLOCALI, PRICE)) %>% 
  ggplot(aes(PRICE, TOTLOCALI,fill = ASCENSORE)) + ## qui fill con v avriabile bianria
  geom_density_ridges(alpha = .5) +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(x = "Price (EUR)",
       y = "",
       title = "How much do Cost items in each category cost?") +
  theme_nicco()

with_fill / non_fill
```

What it might be really relevant to research is how monthly prices change with respect to house square footage for each house configuration. The idea is to asses how much adding a further square meter affetcs the monthly price for each n-roomed flat.
One implication is how the property should be developed in order to request a greater amount of money per month. As an example in a situation in which the household has to lot its property into different sub units he can be helped to decide the most proficient choice in economic terms by setting ex ante the square footage extensions for each of the sub-properties.
A further implication can regard economic convenience to enlarge new property acquisitions under the expectation to broadened the square footage (construction firms). Some of the potential enlargements are economically justified, some of the other are not.
The plot  \@ref(fig:GlmPriceSq) has two continuous variables for x (price) and y (sqfeet) axis, the latter is log 10 scaled due to smoothness reasons. Coloration discretizes points for the each $j$ household rooms totlocali. A sort of overlapping  piece-wise linear regression (log-linear due to transformation) is fitted on each totlocali group, whose response variable is price and whose only predictor is the square footage surface (i.e.  $\log_{10}(\mathbf{price_j}) \sim +\beta_{0,j}+\beta_{1,j}\mathbf{sqfeet_j}$). Five different regression models are proposed in the top left. The interesting part regards the models slopes $\hat\beta_{1,j}$. The highest corresponds to "Monolocale" for which the enlargement of a 10 square meters in surface enriches the apartment of a `r 10^(0.00726*10)-1`% monthly price addition. Almost the same is witnessed in "Bilocale" for which a 10 square meters extension gains a `r 10^(0.0049*10)-1`% value. One more major thing to notice is the "ensamble" regression line obtained as the interpolation of the 5 plotted ones. The line suggests a clear slope descending pattern (logarithmic trend) from Pentalocale and beyond whose assumption is strengthened by looking at the decreasing trend in the $\hat\beta_1$ predictor slopes coefficients. Furthermore investing into an extension for "Quadrilocale" and "Trilocale" is _coeteris paribus_ an interchangeable economic choice.

```{r GlmPriceSq}

my_formula = y ~ x

datiprep_sq = datiprep %>%
  rename_all(tolower)  %>% 
  filter(!is.na(sqfeet),
         sqfeet >= .001) %>%
  arrange(desc(sqfeet)) %>%
  add_count(totlocali, name = "totlocali_count") %>% 
  mutate(abs_price = price / sqfeet) 

datiprep_sq %>%
  dplyr::filter(!is.na(totlocali)) %>%
  mutate(totlocali = fct_lump(totlocali, 5)) %>%
  ggplot(aes(sqfeet, price, color = totlocali)) +
  geom_smooth(method = "lm",se = FALSE, formula = my_formula) +
  stat_poly_eq(formula = my_formula, 
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "~~~")), 
               parse = TRUE) +
  geom_point() +
  
  # scale_x_log10(labels = unit_format(suffix = expression("m^2"))) +
  scale_y_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  scale_x_continuous(labels = unit_format(suffix = "m^2", prefix = "")) +
  labs(y = "price (EUR)",
       x = "") +
  theme_nicco()
```

In table (...) resides the answer to the question "which are the most profitable properties per month in terms of the price per square meter footage ratio". The covariate floor together with the totpiani are not part of the model, indeed they can explain the importance and the height of the building justifying extraordinary prices.  The first 4 observations are unsurprisingly "Bilocale", the spatial column location, not a regressor, can lend a hand to acknowledge that the street addresses point to very central and popular zones. The zones are, first City Life, second Brera and third Moscova, proving that in modeling real estate rents the spatial component is fundamental , even more in Milan. 

```{r TopAbsPrice, fig.cap="prima tabella"}
datiprep_sq %>%
  arrange(desc(abs_price)) %>% 
  select(location,totlocali, price, sqfeet,floor, totpiani,abs_price) %>%
  head() %>% 
  kable(booktabs = T)
```

Then as a further point it might be important to investigate a linear model whose response is price and whose covariates are the newly created abs_price and some other presumably important ones  i.e. floor. The model fitted is `log2(price) ~ log2(abs_price) + floor`.
The plot in figure \@ref(fig:TieFighterPlot) tries to give the intuition on unusual effects across different floors. The interpretation starts by considering the house surface effect (i.e. House Surface (doubling)) for each doubling of the square meter footage. Then what it can be seen is that both apartments and ultimo piano are unsually expensive with respect to their square meter footage. On the other hand the piano rialzato and piano terra are unusually inexpensive for their surface.  
In other words the plot has the analytical purpose to demonstrate how monthly price is affected by floor conditioned to their respective square meter footage. The term log2(abs_price) has been converted to more familiar House Surface (doubling) to help with the interpretation.

```{r TieFighterPlot, fig.cap="Coefficient Tie fighter plot for the linear model: log2(price) ~ log2(abs_price) + condom + other_colors"}

datiprep_sq %>%
  mutate(totpiani = fct_lump(totpiani, 5),
         floor = fct_lump(floor,10)) %>% 
  filter(floor != "Other") %>%  
  filter(totpiani != "Other")  %>% 
  lm(log2(price) ~ log2(abs_price) + floor + bagno,  data = .)  %>% 
  tidy(conf.int = TRUE) %>%    
  filter(term != "(Intercept)") %>%
  mutate(term = ifelse(term == "log2(abs_price)", "House Surface (doubling)", term), 
         term = ifelse(str_detect(term,"totpiani"), glue("{term} totali"), term), 
         term = str_remove(term, "^totpiani"),
         term = ifelse(str_detect(term,"floor"), glue("al {term}"), term), 
         term = str_remove(term, "floor")) %>% 
         # term = str_remove(term, "^floor") 
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(estimate, term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = .1) +
  geom_vline(xintercept = 0, color = "red", lty = 2) +
  labs(x = "Coefficient",
       y = "",
       title = "Unusually expensive houses for square meter footage") +
  theme_nicco()
```


(se vuoi dire qualcosa sul condominio)

## Missing Assessement and Imputation

As already pointed out some data might be lost since immobiliare provides the information that in turn are pre filled by estate agencies or privates through standard document formats. Some of the missing can be reverse engineered by other information in the web pages e.g. given the street address it is possible to trace back the lat and long coordinates. Some other information can be encountered in .json files hidden inside each of the single webpages.
The approach followed in this part is to prune redundant data and rare covariates trying to limit the dimensionality of the dataset.

### Missing assessement 

The first problem to assess is why information are missing. As already pointed out in the introduction part and in section \@ref(ContentArchitecture) many of the presumably important covariates (i.e. price lat, long, title ,id ...) undergo to a sequence of forced step inside scraping functions with the aim to avoid to be missed.  If in the end of the sequence the covariates are still missing, the correspondent observation is not considered and it is left out of the scraped dataset. The choice originates from empirical missing patterns suggesting that when important information are missing then the rest of the covariates are more likely to be missing to, as a consequence the observation should be discarded.
The missing profile is crucial since it can also raise suspicion on the scraping failures. By Taking advantage of the missing pattern in observations the maintainer can directly identify the problem and derivatives and immediately debug the error. In order to identify if the nature of the pattern a revision of missing and randomness is introduced by @Little.
Missing can be devided into 3 categories:

- *MCAR* (missing completely at random) likelihood of missing is equal for all the infromation, in other words missing data are one idependetn for the other.
- *MAR* (missing at random) likelihood of missing is not equal.
- *NMAR* (not missing at random) data that is missing due to a specific cause, scarping can be the cause.

MNAR is often the case of daily monitoring clinical studies [@Kuhn], where patient might drop out the experiment because of death and so all the relating data starting from the death time +1 are lost.
To identify the pattern a _heat map_  plot \@ref(fig:Heatmap) clarifies the idea:

```{r Heatmap, fig.cap="Missingness Heatmap plot"}
data1 = datiprep %>% select(-which(colMeans(is.na(.)) < 0.001)) ## % of missing
convert_missing = function(x) ifelse(is.na(x), 0, 1)
scat_missing = apply(data1, 2, convert_missing)

Heatmap(
  scat_missing, 
  name = "Missing", #title of legend
  column_title = "Predictors", row_title = "Samples",
  col = c("black","lightgrey"),
  show_heatmap_legend = FALSE,
  row_names_gp = gpar(fontsize = 0) # Text size for row names
)
```

Looking at the top of the heat map plot, right under the "Predictor" label, the first tree split divides predictors into two sub-groups. The left branch considers from *TOTPIANI* to *CATASTINFO* and there are no evident patterns. Then missingness can be traced back to MAR. Imputation needs to be applied up to *CONDOM* included, the others are discarded due to rarity: i.e. *BUILDAGE*: 14% missing, *CATASTINFO*: 21% and *AC*: 24%. Moreover *CUCINA* and *ALTRO* are generated as "childred" of the original *LOCALI* variable, so it should not surprise that their missing behavior is similar ,whose prevalence is respectively 13% and 14%, for that reason are discarded. 
In the far right hand side *ENCLASS* and *DISP* data are completely missing and a pattern seems to be found. The most obvious reason is a scraping fail in capturing data. Further inspection of the API scraping functions focused on the two covariates is strongly advised. From *LOWRDPRICE.* covariates gorup class it seems to be witnessing a missing underlining pattern NMAR which is clearer by looking at the co_occurrence plot in figure \@ref(fig:cooccurrence). Co-occurrence analysis might suggest frequency of missing predictor in combination and *LOWRDPRICE.* class covariates are displaying this type of behavior. *PAUTO* is missing in the place where *LOWRDPRICE.* class covariates are missing, but this is not happening for the opposite, leading to the conclusion that *PAUTO* should be treated as a rare covariate MAR, therefore *PAUTO* is dropped.
After some further investigation on *LOWRDPRICE.*, the group class flags when the *PRICE* covariate is effectively decreased and this is unusual. That is solved by grouping the covariate's information and to encode it as a two levels categorical covariate if lowered or not. Further methods to feature engineer the *LOWRDPRICE.* class covariates can be with techniques typical of profile data, further references are on @Kuhn.

```{r cooccurrence, fig.cap="Missingness co-occurrence plot"}
gg_miss_upset(data1, nsets = 7) 
```



### Covariates Imputation
```{r ImputationData}
## conversion to dataframe
data_miss = datiprep %>%  
  rename_all(tolower) %>% 
  select(-id,-lat,-long,-location, -buildage, -cucina, -altro, -ac,-pub_date, -catastinfo, -aptchar, -age,-lowrdprice.originalprice,-lowrdprice.currentprice, -lowrdprice.passeddays,-lowrdprice.date,-enclass,-contr ,-disp, -pauto,-review,  -hasmulti,-title) %>% 
  as.data.frame()

```

A relatively simple approach to front missingness is to build a regression model to explain the covariates that have some missing and plug-back-in the respective estimates (e.g. posterior means) from their predictive distributions @Little. This approach is fast and easy to implement in most of the cases, but it ignores the uncertainty behind the imputed values [@Bayesian_INLA_Rubio]. However it has the benefit to be a more than a reasonable choice with respect to the number of computation required, especially with INLA and in a spatial setting. That makes it the first choice method to follow since imputation regards also a small portion of data and predictors. At first it is considered the predictor _condominium_  for which some observation are missing. Indices are:

```{r IndexCondominium}
##  missing data index
condom_na = which(is.na(data_miss$condom)) ## 19  74  77  90  99 113 116 120 179 249
condom_na

```



<!-- and a benchmark model is fitted whose formula is `price ~ 1 + condom + bagno + sqfeet`. Inla handles missing data by ignoring them so the model trains coefficients on a restricted sample of observations. -->

<!-- ```{r} -->
<!-- ## benchmark model -->
<!-- benchmark = inla(price ~ 1+ condom + totlocali + sqfeet, data = data_miss) -->
<!-- benchmark$summary.fixed %>%  -->
<!--   rownames_to_column(var = "terms") %>%  -->
<!--   as_tibble() %>%  -->
<!--   select(terms:sd) %>%  -->
<!--   column_to_rownames(var = "terms")  %>% -->
<!--   head(6) %>%  -->
<!--   knitr::kable(booktabs = T) -->

<!-- ``` -->


A model is fitted based on missing data for which the response var is condominium and predictors are other important explanatory ones, i.e.`condom ~ 1 + sqfeet + totlocali + floor + heating + ascensore`. In addition to the formula in the inla function a further specification has to be provided with the command `compute = TRUE` in the argument control.predictor. The command `compute` estimates the posterior means of the predictive distribution in the response variable for the missing points. The estimated posetior mean quantities are then imputeda are in table \@red(tab:CondomImputation)

```{r CondomImputation, fig.cap="Posterior Means imputation"}
## condom model fitted
cov_imputation = inla(condom ~ 1 + sqfeet + totlocali + heating + ascensore + accdisabili, data = data_miss,
                      control.predictor = list(compute = TRUE),
                      control.compute = list(config = TRUE))

data_imputed = data.frame(data_miss) 
data_imputed$condom[condom_na] = cov_imputation$summary.fitted.values[condom_na, "mean"]
cov_imputation$summary.fitted.values[condom_na, c("mean", "sd")] %>% 
  knitr::kable(booktabs = T)
```





<!-- Afterwards the benchmark model is refitted in imputed data and coefficients are compared with the missing. Results displays... -->

<!-- ```{r refitting} -->

<!-- data_imputed = data.frame(data_miss) -->
<!-- data_imputed$condom[condom_na] = cov_imputation$summary.fitted.values[condom_na, "mean"] -->
<!-- which(is.na(data_imputed$condom)) -->

<!-- refitted = inla(price ~ 1 + condom + totlocali+ sqfeet, data = data_imputed) -->
<!-- refitted %>%  summary() -->
<!--   rownames_to_column(var = "terms") %>%  -->
<!--   as_tibble() %>%  -->
<!--   select(terms:sd) %>%  -->
<!--   column_to_rownames(var = "terms")  %>% -->
<!--   head(6) %>%  -->
<!--   knitr::kable(booktabs = T) -->
<!-- ``` -->


A further method for imputation has been designed by _Gómez-Rubio, Cameletti, and Blangiardo 2019) miss lit_ by adding a sub-model for the imputations to the final model through the inla function. This is directly handled inside the predictor formula adding a parameter in the latent field. However the approach makes the model more complex with a further layer of uncertainty to handle. 
At first the additive regression model with all the covariates is called including the covariates with missing values. The response variable *PRICE* displays no missing values and the model fitted is: 

 
## Model Specification



## Mesh building 

*PARAFRASARE*
The SPDE approach approximates the continuous Gaussian field $w_{i}$ as a discrete Gaussian Markov random field by means of a finite basis function defined on a triangulated mesh of the region of study. The spatial surface can be interpolated performing this approximation with the inla.mesh.2d() function of the R-INLA package. This function creates a Constrained Refined Delaunay Triangulation (CRDT) over the study region, that will be simply referred to as the mesh. Mesh should be intended as a trade off between the accuracy of the GMRF surface representation and the computational cost, in other words the more are the vertices, the finer is the GF approximation, leading to a computational funnel. 

![Traingularization intuition, @Krainski-Rubio source](images/triangle.jpg)

Arguments can tune triangularization through inla.mesh.2d() :

* `loc`:location coordinates that are used as initial mesh vertices
* `boundary`:object describing the boundary of the domain,
* `offset`:  argument is a numeric value (or a length two vector) and it is used
to set the automatic extension distance. If positive, it is the extension distance
in the same scale units. If negative, it is interpreted as a factor relative to the
approximate data diameter; i.e., a value of -0.10 (the default) will add a 10%
of the data diameter as outer extension.
* `cutoff`: points at a closer distance than the supplied value are replaced by a single vertex. Hence, it avoids small triangles 
* `max.edge`: A good mesh needs to have triangles as regular as possible in size and shape.
* `min.angle`argument (which can be scalar or length two vector) can be used to specify the minimum internal angles of the triangles in the inner domain and the outer extension

A convex hull is a polygon of triangles out of the domain area, in other words the extension made to avoid the boundary effect. All meshes in Figure 2.12 have been made to have a convex hull boundary. If borders are available are generally preferred, so non convex hull meshes are avoided.

```{r NonConvexHull, eval=FALSE}
bound = inla.nonconvex.hull(coords)
```

### Shinyapp for mesh assessment

INLA includes a Shiny (Chang et al., 2018) application that can be used to tune the mesh params interactively

```{r meshbuilder, eval = FALSE}
INLA::meshbuilder()
```


The mesh builder has a number of options to define the mesh on the left side. These include options to be passed to functions inla.nonconvex.hull() and inla.mesh.2d() and the resulting mesh displayed on the right part.

### BUilding SPDE model on mesh




## Spatial Kriging (Prediction)

QUI INCERTEZZE
```{r variogram}
semivar = variogram(PRICE~1, 
                       locations= ~LONG+LAT,
                       data=dati)
# 
# fit.vgm = fit.variogram(semivar, vgm("Sph"))
# krg = krige(log(PRICE) ~ 1, data, model = fit.vgm)

```








