# Exploratory Analysis {#exploratory}

<!--  You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).-->

```{r options, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,  
  strip.white = TRUE,
  message = FALSE,
  cache = FALSE,
  echo = FALSE
)

# libraries are in libs.R
# source(here::here("libs.R"))

library(readr)
library(kableExtra)
library(tidyverse)
library(broom)
library(viridis)
library(leaflet)
library(sp)
library(glue)
library(ggpmisc)
library(gstat)
library(stringi)
library(scales)
library(plotly)
library(INLA)
## missing profile
library(caret)
library(tidyverse)
library(naniar)
library(forcats)
library(visdat)
library(ComplexHeatmap)
library(patchwork)
library(ggridges)


## Personal ggplot theme 
theme_nicco = function (base_size = 11, base_family = "") {
  theme_bw() %+replace% 
    theme(
      text = element_text(family = "sans", size = 12),
      plot.title = element_text(face = "bold", size = 14, margin=margin(0,0,30,0)),
      panel.background  = element_blank(),
      axis.ticks = element_line(colour = "grey70", size = 0.2),
      plot.background = element_rect(fill="white", colour=NA),
      panel.border = element_rect(linetype = "blank", fill = NA),
      legend.background = element_rect(fill="transparent", colour=NA),
      legend.key = element_rect(fill="transparent", colour=NA)
    )
}
```

Data comes packed into the REST API end point `*/complete` in .json format. Data can be filtered out On the basis of the options set in the API endpoint argument body. Some of the options might regard the `city` in which it is aevaluated the real estate market, `npages` as the number of pages to be scraped, `type` as the choice between rental of selling. For further documentation on how to structure the API endpoint query refer to section \@ref(APIdocs).  Since to the analysis purposes data should come from the same source (e.g. Milan rental real estate within "circonvallazione") a dedicated endpoint boolean option `.thesis` is passed in the argument body. What the API option under the hood is doing is specifying a structured and already filtered URL to be passed to the scraping endppoint. By securing the same URL to the scraping functions data is forced to come from the same URL source. The idea behind this concept can be thought as refreshing everyday the same immobiliare.it URL. API endpoint by default also specifies 10 pages to be scraped, in this case 120 is provided leading to to 3000 data points. The `*` refers to the EC2 public DNS that is `ec2-18-224-40-67.us-east-2.compute.amazonaws.com`

`http://*/complete/120/milano/affitto/.thesis=true`

As a further source data can also be accessed through the mondgoDB credentials with the cloud ATLAS database by picking up the latest .csv file generated. For run time reasons also related to the bookdown files continuous building the API endpoint is called the day before the presentation so that the latest .csv file is available. As a consequence code chunks outputs are all cached due to heavy computation.
Interactive  maps are done with Leaflet, the result of which is a leaflet map object which can be piped to other leaflet functions. This permits multiple map layers and many control options to be added interactively (LaTex output is statically generated)

A preliminary API data exploratory analysis evidences 34 covariates and 250 rows, which are once again conditioned to the query sent to the API. Immobiliare.it furnishes many information regarding property attributes and estate agency circumstances. Data displays many NA in some of the columns but georeference coordinates, due to the design of scraping functions, are in any case present. 



```{r api_vars}

dati = read_csv("data/data.csv", locale = readr::locale(encoding = "latin1")) %>%
  dplyr::select(-X1) 

### fai la tabella delle colonne
### 
ref =c(
"ID of the apartements",
"latitude coordinate",
"longitude coordinate",
"the complete address: street name and number",
"the condominium monthly expenses",
"the age in which the building was contructed",
"the property floor",
"indipendent property type versus apartement type",
"specification of the type and number of rooms",
"property type residential or not",
"the actual status of the house, ristrutturato, nuovo, abitabile",
"the heating system Cen_Rad_Gas (centralizzato a radiatori, alim a gas), Cen_Rad_Met,",
"air conditioning hot and cold, Autonomo, freddo/caldo, Centralizzato, freddo/caldo",
"the date of publication of the advertisement",
"land registry information",
"apartement main characteristics",
"number of photos displayed in the advertisement",
"real estate agency name",
"If the price is lowered it flags the starting price",
"If the price is lowered it flags the current price",
"If the price is lowered indicates the days passed since the price has changed",
"If the price is lowered indicates the date the price has changed",
"the energy class according to the land registers",
"the type of contract",
"if it is still avaiable or already rented",
"the total number of the building floors",
"number of parking box or garages avaibable in the property",
"estate agency review, long chr string",
"it if has multimedia option, such as 3D house vitualization home experience or videos",
"the monthly price <- response",
"square meters footage",
"the number of rooms in the house, and their types",
"title of published advertisement"
)

names = dati %>%  names
tibble(
  name = names,
  ref  = ref 
) %>% 
  knitr::kable(booktabs = T, longtable = T)
  # 
  # kbl(booktabs = T, longtable = T) %>%
  # kable_styling(full_width = F,font_size = 8) %>%
  # column_spec(1, bold = T) %>%
  # column_spec(2, width = "20em")
```


## Data preparation 

```{r data_prep, message=FALSE}
datiprep = dati %>% 
  select(-TPPROP) %>% 
  ## FLOOR
  mutate(FLOOR = str_remove_all(FLOOR, "°")) %>% 
  separate(FLOOR, c("FLOOR", "ASCENSORE","ACCDISABILI"), sep = ", ", convert = T) %>%
  replace_na(list(ASCENSORE = "no", ACCDISABILI = "no")) %>% 
  mutate(ASCENSORE = recode(ASCENSORE,"con accesso disabili" = "no")) %>%  
  mutate(FLOOR = recode(FLOOR, "1" = "1 piano", "2" = "2 piano", "3" = "2 piano", "4" = "4 piano", "5" = "5 piano", "6" = "6 piano", "7" = "7 piano", "8" = "8 piano", "9" = "9 piano")) %>% 
  ## TOTPIANI
  # mutate(TOTPIANI = str_remove_all(TOTPIANI, " piani")) %>% 
  ## LOCALI
  mutate(LOCALI = if_else(substr(LOCALI,start = 1,stop = 1) == 1, "1", LOCALI)) %>% 
  separate(LOCALI, c("TOTLOCALI", "REST"), sep = "\\s", extra = "merge") %>% 
  separate(REST, c("CAMERELETTO","ALTRO","BAGNO", "CUCINA"), sep = ", ", convert = T, fill = "left")  %>% 
  ## CAMERALETTO
  mutate(CAMERELETTO = if_else(TOTLOCALI== 1, "(1 camera da letto", CAMERELETTO)) %>% 
  mutate(CAMERELETTO = if_else(is.na(CAMERELETTO),ALTRO , CAMERELETTO)) %>% 
  mutate(CAMERELETTO = if_else(CAMERELETTO== "locali",NA_character_ , CAMERELETTO)) %>%  
  mutate(CAMERELETTO = if_else(CAMERELETTO== "camere da letto", paste0(TOTLOCALI," camere da letto"), CAMERELETTO)) %>% 
  mutate(CAMERELETTO = str_remove(CAMERELETTO, "\\(")) %>% 
  ## ALTRO
  mutate(ALTRO = if_else(BAGNO== "1 altro)","1 altro)" , ALTRO)) %>% 
  replace_with_na(replace = list(ALTRO = c("camere da letto", "locali"))) %>% 
  mutate(ALTRO = str_remove(ALTRO, "\\)")) %>% 
  ## BAGNO
  mutate(BAGNO = if_else(CUCINA== "1 bagno", "1 bagno", BAGNO)) %>% 
  replace_with_na(replace = list(BAGNO = "1 altro)")) %>% 
  mutate(BAGNO = if_else(TOTLOCALI== "1", "1 bagno", BAGNO)) %>% 
  ## CUCINA (ne ho perse qualcuna)
  replace_with_na(replace = list(CUCINA = "1 bagno")) %>% 
  mutate(TOTLOCALI = recode(TOTLOCALI, "1" = "Monolocale", "2" = "Bilocale", "3" = "Trilocale", "4" = "Quadrilocale", "5" = "Pentalocale")) ## non proviene più di sei locali 
  
  ## APTCHAR (non implementato)
  # aptchar = datiprep %>% 
  #   select(APTCHAR) %>%
  #   mutate(APTCHAR = str_remove_all(APTCHAR, "\\-")) %>% 
  #   mutate(APTCHAR = str_trim(APTCHAR)) %>% 
  #   separate(APTCHAR, into = LETTERS[1:15],"\\s{2,}")
  #   
  #   

  ## CATASTINFO
  #
  #

```

Data needs to undergo to many previous cleaning preprocess steps, this is a forced stage since API comes in human readable format, which is from one hanb designed to a be readble format, but from the other it is not prepared to be modeled. cleaning steps mainly regards:

- encoding from UTF-8 to Latin due to Italian characters incorrectly parsed.
- *FLOORS* covariate needs to be separated by its *ASCENSORE* and *ACCDISABILI* components, leading to have 2 further bivariate covariates.
- *LOCALI* needs to undergo to be separated in its components too. 5 categories levels drain out: *TOTLOCALI*, *CAMERELETTO*, *ALTRO*, *BAGNO*, *CUCINA*. *TOTLOCALI* is a duplicate for *NROOM* the is discarded.
- *APTCHAR* is a list column so that each observation has different categories insiside. The preprocess step includes unnesting the list by creating as many bivariate columns as elements in the list. Then new columns flag the existemce of the caracteristics in the apartement. A slice for the fist observation displays:

`r datiprep %>%  select(APTCHAR) %>%  slice(sample(1))  %>% pull`

(sistemare classe energetica)


### Maps and Geo-Visualisations

Geographic coordinates can be represented on a map in order to reveal first symptoms of spatial autocorrelation. Observations are spread almost equally throughout the surface even though the response var *PRICE* indicates unsurprisingly that higher prices are nearer to the city center.
The map in figure \@ref(fig:leaflet_visuals) is a leaflet object, which can needs to be overlapped with layers indicating different maps projections. This is interactive in the .html version, and static is proposed in the .pdf output version. The map object takes a input the latitute and longitude coordinates coming from THE API, and they do not need any CRS (Coordinate Reference System) projection since leaflet can accept the data type.

```{r leaflet_visuals, out.width="100%", fig.cap="Leaflet Map"}

if(knitr::is_latex_output()){
    knitr::include_graphics("images/leaflet_prezzi.jpg")
} else if(knitr::is_html_output()){  
  dati2 = dati %>%
    dplyr::select(LAT, LONG, PRICE )
  pal = colorBin("viridis", bins = c(1000, 1500, 2000, 2500, 5000))
  
  leaflet(dati2) %>%
    # addProviderTiles(providers$CartoDB.Positron) %>% bug in Lealfet while caching
    setView(lat = 45.474211, lng = 9.191383, zoom = 13) %>%
    addMiniMap() %>% 
    addTiles() %>% 
    addCircles(lng = ~LONG, 
               lat = ~LAT, 
               color = ~ pal(PRICE),
               weight = 10,
               radius = ~log(PRICE)) %>%
    addLegend("bottomright",
      pal = pal, values = ~PRICE,
      title = "Prices") %>%
    addScaleBar(position = c("bottomleft"))
  }
```

(other more tmap and ggplot)

## Counts and First Orientations

Arranged Counts for categorical columns can give a sense of the distribution of categories across the dataset suggesting also which predictor to include in the model. The visualization in figure \@ref(fig:counts) offers a a reordered *TOTLOCALI* 5 levels categories. Bilocali are the most common option for rent, then trilocali comes at second place. The intuition behind suggests that Milan rental market is oriented to "lighter" accommodation solutions in terms of space and squarefootage. This should comes natural since Milan is both a vivid study and working area, so short stayings are warmly welcomed

```{r fct_counts, fig.cap="Most common housedolds categories for Milan Real Estate Market"} 
datiprep %>%
  mutate(TOTLOCALI = as_factor(TOTLOCALI)) %>% 
  count(TOTLOCALI, sort = TRUE) %>% 
  mutate(TOTLOCALI = fct_reorder(TOTLOCALI, n)) %>%
  ggplot(aes(n, TOTLOCALI)) +
  geom_col() +
  labs(x = "# of houses",
       y = "",
       title = "Most common housedolds categories in Milan")+
  theme_nicco()
```

Two of the most requested features for comfort and livability in rents are the heating/cooling systems installed. Moreover rental market demand regardless of the total house rent duration ready-to accomodate solutions, coming with all the services included.
x-axis in figure \@ref(fig:price_per_heating) represents log_10 price for both of the two plots for smoothing reasons, so the interpretation of price is into relative percent changes. Furthermore factors are reordered with respect to decreasing price.  
y-axis are the different level for the categorical variables recoded to simplify lables. Moreover counts per level are expressed btween brackets next to the the respective levels.
The top plot displays the most prevalent categories  is "Cen_Rad_Met" by far, which is the most polluting as well. Jittering is then applied to highlight observations place with respect to the IQR (Inter Quantile Range) .25 and how outliers affect distribution for categories. A first conclusion is that There are some outliers that are located in autonomous systems, which leads to belive that the most expsive houses are heated by autonomoius heating systems, but that does not represent an implication in monthly price. The overlapping IQR signifies that the covariates levels do not impact the response variable.

```{r price_heating_ac, fig.cap="Log Monthly Price per Heating and AC system"}

pph =datiprep %>%
  mutate(HEATING = fct_lump(HEATING, 5)) %>%
  filter(HEATING != "Other") %>%
  filter(HEATING != is.na(HEATING)) %>% 
  mutate(HEATING = recode(HEATING,"Centralizzato, a radiatori" = "Cen_Rad",
                          "Centralizzato, a radiatori, alimentato a gas" = "Cen_Rad_Gas",
                          "Centralizzato, a radiatori, alimentato a metano" = "Cen_Rad_Met",
                          "Autonomo, a radiatori, alimentato a metano" = "Aut_Rad_Met",
                          "Autonomo, a radiatori, alimentato a gas" = "Aut_Rad_Gas")) %>% 
  add_count(HEATING, name = "HEATING_count") %>% 
  mutate(HEATING = glue("{ HEATING } ({ HEATING_count })"),
         HEATING  = fct_reorder(HEATING, PRICE)) %>%
  ggplot(aes(PRICE, HEATING)) +
  ggtitle("Log Monthly Price per Heating and AC systems?")+
  geom_boxplot() +
  geom_jitter(width = 0, height = .1, alpha = .25) +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(y = "",
       x = "") +
  theme_nicco()
pph
```

```{r price_per_ac, fig.cap="Log Monthly Price per Heating system?"}

ppa =datiprep %>%
  mutate(AC = fct_lump(AC, 4)) %>%
  # filter(AC != "Other") %>%
  filter(AC != is.na(AC)) %>% 
  add_count(AC, name = "AC_count") %>% 
  mutate(AC = glue("{ AC } ({ AC_count })"),
         AC  = fct_reorder(AC, PRICE)) %>%
  ggplot(aes(PRICE, AC)) +
  geom_boxplot() +
  geom_jitter(width = 0, height = .1, alpha = .25) +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(x = "Price (EUR)",
       y = "") +
  theme_nicco()
ppa
pph /ppa

```


```{r}
datiprep %>%
  count(AC, sort = T)
```

What it can be seen form figure \@ref(fig:price_per_floor) Può, essere interessante guardare quali sono gli ultimi piani. 

```{r price_per_floor, fig.cap="Log Monthly Price per apt Floor?"}

ppf = datiprep %>%
  mutate(FLOOR = fct_lump(FLOOR, 10)) %>%
  filter(FLOOR != "Other") %>%
  add_count(FLOOR, name = "FLOOR_count") %>% 
  mutate(FLOOR = glue("{ FLOOR } ({ FLOOR_count })"),
         FLOOR  = fct_reorder(FLOOR, FLOOR_count)) %>%
  ggplot(aes(PRICE, FLOOR)) +
  ggtitle("Log Monthly price per FLOOR?")+
  geom_boxplot() +
  geom_jitter(width = 0, height = .1, alpha = .25) +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(x = "Price (EUR)",
       y = "")  +
  theme_nicco()
ppf

```
What it can be seen form figure \@ref(fig:price_per_totpiani) 

```{r price_per_totpiani, fig.cap="Log Monthly Price per building Height?"}

ppt = datiprep %>%
  mutate(TOTPIANI = fct_lump(TOTPIANI, 5)) %>%
  # filter(TOTPIANI != "Other") %>%
  add_count(TOTPIANI, name = "TOTPIANI_count") %>% 
  mutate(TOTPIANI = glue("{ TOTPIANI } ({ TOTPIANI_count })"),
         TOTPIANI  = fct_reorder(TOTPIANI, PRICE)) %>%
  ggplot(aes(PRICE, TOTPIANI)) +
  ggtitle("Log Monthly price per TOTPIANI system?")+
  geom_boxplot() +
  geom_jitter(width = 0, height = .1, alpha = .25) +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(x = "Log Price (EUR)",
       y = "") +
  theme_nicco()
ppt

```


```{r}
## provare a ridurre la lunghezza delle lables
pph / ppf
```

this visualization intersects allows to discover bimodality in the response variable.  Log scales was needed since they are all veru skewd and log scale then is needed also in the model.

(qui ci puoi mettere a confronto per variabile bianria, così vedi cosa includere nel modello esempio sotto dove commentato, )

```{r}
## non filled
non_fill = datiprep %>%
  add_count(TOTLOCALI, name  = "TOTLOCALI_count") %>% 
  mutate(TOTLOCALI = glue("{ TOTLOCALI } ({ TOTLOCALI_count })"),
         TOTLOCALI = fct_reorder(TOTLOCALI, PRICE)) %>% 
  ggplot(aes(PRICE, TOTLOCALI)) + 
  geom_density_ridges() +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(x = "Price (EUR)",
       y = "",
       title = "How much do Cost items in each category cost?") +
  theme_nicco()

with_fill = datiprep %>%
  add_count(TOTLOCALI, name  = "TOTLOCALI_count") %>% 
  mutate(TOTLOCALI = glue("{ TOTLOCALI } ({ TOTLOCALI_count })"),
         TOTLOCALI = fct_reorder(TOTLOCALI, PRICE)) %>% 
  ggplot(aes(PRICE, TOTLOCALI,fill = ASCENSORE)) + ## qui fill con v avriabile bianria
  geom_density_ridges(alpha = .5) +
  scale_x_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  labs(x = "Price (EUR)",
       y = "",
       title = "How much do Cost items in each category cost?") +
  theme_nicco()

with_fill / non_fill
```

What it might be really relevant to research is how monthly prices change with respect to house squarefootage for each house configuration. The idea is to asses how much adding a further square meter affetcs the monthly price for each n-roomed flat.
One implication is how the property should be developed in order to request a greater amount of money per month. As an example in a situation in which the household has to lot its property into different sub units he can be helped to decide the most proficient choice in economic terms by setting ex ante the squarefootage extensions for each of the properties.
A further implication can regard economic choices to enlarge new property acquisitions under the expectation to broadened the squarefootage (contruction firms). Some of the choices have an economic sense, some of them have not.
The plot  \@ref(fig:glm_price_sq) has two contionous variables for x (price) and y (sqfootage) axis, the latter is log 10 scaled due to smoothness reasons. Coloration dicretizes points for the each $j$ household rooms totlocali. A sort of overlapping  piece-wise linear regression (log-linear due to transformation) is fitted on each totlocali group, whose response variable is price and whose only predictor is the squarefootage surface(i.e.  price ~ sqfeet). Five different regression models are displayed in the top left. The interesting part regards the models slopes $\hat\beta_{1,j}$. The highest corresponds to "Monolocale" for which the increment of a 10 square meter in surface enriches the apartement of a `r 10^(0.00726*10)`% monthly price addition. Almost the same is witnessed in "Bilocale" for which a 10 square meter enlargement gains `r 10^(0.0049*10)`%. One more major thing to notice is the "ensamble" regression line obtained as the interpolation of the plotted ones. The line suggests a clear descending pattern from Pentalocale and beyond whose assumption is strengthened by looking at the decreasing trend in the $\hat\beta_1$ predictor slopes coefficients.  

```{r glm_price_sq}

my_formula = y ~ x

datiprep_sq = datiprep %>%
  rename_all(tolower)  %>% 
  filter(!is.na(sqfeet),
         sqfeet >= .001) %>%
  arrange(desc(sqfeet)) %>%
  add_count(totlocali, name = "totlocali_count")

datiprep_sq %>%
  dplyr::filter(!is.na(totlocali)) %>%
  mutate(totlocali = fct_lump(totlocali, 5)) %>%
  ggplot(aes(sqfeet, price, color = totlocali)) +
  geom_smooth(method = "lm",se = FALSE, formula = my_formula) +
  stat_poly_eq(formula = my_formula, 
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., sep = "~~~")), 
               parse = TRUE) +
  geom_point() +
  
  # scale_x_log10(labels = unit_format(suffix = expression("m^2"))) +
  scale_y_log10(labels = dollar_format(suffix = "€", prefix = "")) +
  scale_x_continuous(labels = unit_format(suffix = "m^2", prefix = "")) +
  labs(y = "price (EUR)",
       x = "") +
  theme_nicco()
```

In table (...) are displayed the most valuable in absolute terms properties. The first 4 observations are "Bilocale" 

```{r answer_to_quest}

datiprep %>%
  rename_all(tolower)  %>% 
  mutate(abs_price = price / sqfeet) %>%
  arrange(desc(abs_price)) %>% 
  select(location,totlocali, price, sqfeet,floor, totpiani,abs_price, condom) %>%
  head() %>% 
  kable(booktabs = T)
```

Then fitting a linear model can help to understand how covariates interact with response variable through coefficients interpretation. the The linear model fitted is `lm(log2(price_usd) ~ log2(volume_m3) + category + other_colors, data = .)`. 

```{r Coefficient/Tie fighter plot, eval=FALSE}

datiprep_sq %>%
  mutate(category = fct_relevel(category, "Tables & desks")) %>%
  lm(log2(price_usd) ~ log2(volume_m3) + category + other_colors, data = .) %>%
  tidy(conf.int = TRUE) %>%
  filter(term != "(Intercept)") %>%
  mutate(term = ifelse(term == "log2(volume_m3)", "Item volume (doubling)", term),
         term = str_remove(term, "^category")) %>%
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(estimate, term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = .1) +
  geom_vline(xintercept = 0, color = "red", lty = 2) +
  labs(x = "Impact on price (relative to Tables & desks)",
       y = "",
       title = "What objects are unusually expensive/inexpensive relative to volume?")
```

## Missing Assessement and Imputation

As already pointed out some data went missing since immobiliare provides data that in turn is filled by estate agencies or privates through pre compiled standard formats. Some of the missing observations can be reverse engineered by other information in the web pages e.g. given the street address it is possible to trace back the lat and long coordinates, even though this is already handled by the API through scraping functions with the mechanism found in section \@ref(ContentArchitecture). Some of the information lacking in the summary table they might be desumed and then imputed by the estate agency review which is one of the covariates and where the most of the times missing information can be found. The approach followed in this part is to prune redundant information and "over" missing covariates trying to limit the dimensionality of the dataset.

### Missing assessement 

The first problem to assess is why information are missing. As already pointed out in the introduction part and in section \@ref(ContentArchitecture) many of the presumably important covariates (i.e. price lat, long, title ,id ...) undergo to a sequence of forced step inside scraping functions with the aim to avoid to be missed.  If in the end of the sequence the covariates values are still missing, the correspondent observation is not imputed and it is left out of the scraped dataset. The author choice to follow this approach relies on empirical observation that suggest when important information is missing then the rest of the covariates also do, as a consequence the observation is not useful. Moreover when the spatial component can no be reverse engineered then it is also the case of left out observation while scraping. The model needs to have a spatial component in order to be evaluted. To this purpose The missing profile is crucial since it can also suggest problems in the scraping procedure. By identifying pattern in missing observation the maintainer can take advantage of it and then debug the part that causes missingness. In order to identify the pattern a revision of missigness is introduced by @Little .randomnes can be devided into 3 categories:

- *MCAR* (missing completely at random) likelihood of missing is equal for all the infromation, in other words missing data are one idependetn for the other.
- *MAR* (missing at random) likelihood of missin is not equal.
- *NMAR* (not missing at random) data that is missing due to a specific cause, scarping can be the cause.

MNAR is often the case of daily monitoring clinical studies [@Kuhn2019], where patient might drop out the experiment because of death and so as a consequence all the data starting from the death time +1 is missing.
To assess pattern A _heat map_ plot fits the need: 

```{r Heatmap, fig.cap="Missingness Heatmap plot"}
data1 = datiprep %>% select(-which(colMeans(is.na(.)) < 0.001)) ## % of missing
convert_missing = function(x) ifelse(is.na(x), 0, 1)
scat_missing = apply(data1, 2, convert_missing)

Heatmap(
  scat_missing, 
  name = "Missing", #title of legend
  column_title = "Predictors", row_title = "Samples",
  col = c("black","lightgrey"),
  show_heatmap_legend = FALSE,
  row_names_gp = gpar(fontsize = 0) # Text size for row names
)
```

Looking at the top of the heat map plot \@ref(fig:Heatmap) under the "Predictor" label the first tree split divides data into two sub-groups. The left branch considers from *TOTPIANI* to *CATASTINFO* and there are no substantial relevant worrying patterns.  Then missingness can be traced back to the MAR case. Imputation needs to be applied up to *CONDOM* included, the others are discarded due to rarity: i.e. *BUILDAGE*: 14% missing, *CATASTINFO*: 21% and *AC*: 24%. Moreover *CUCINA*,*ALTRO* are generated as separation of the original *LOCALI* variable, so it should not surprise that their missing behavior is similar which is respectively 13% and 14%, as a consequence  they are discarded. 
(capire se fare imputation per bagno cameraletto altro e cucina)
In the far right hand side *ENCLASS* and *DISP* data are completely missing, this can be addressed to a scraping fail to capture data. Further inspection of the API scraping process focused on the two covariates is strongly advised. From *LOWRDPRICE.* covariates class it seems to be witnessing a missing underlining pattern NMAR which is clearer by looking at the co_occurrence plot \@ref(fig:cooccurrence). Co-occurrence analysis might suggest frequency of missing predictor combinations and *LOWRDPRICE.* class covariates are displyaing this tyoe of behaviour. *PAUTO* is missing where lowered price class covariates are missing but this is not true for the contrary leading to the conclusion that *PAUTO* should be treated as a rare covariate, therefore *PAUTO* is discarded.
After a further investigation *LOWRDPRICE.* flags when the *PRICE* covariate is effectively decreased and this is not so common, that is solved by grouping the covariate's information and to encode it as a two levels categorical covariate. Further methods to feature engineer the lowrdprice. class covariates can be treated with profile data, but this is beyond the scope, further references are @Kuhn2019.

```{r cooccurrence, fig.cap="Missingness co-occurrence plot"}
gg_miss_upset(data1, nsets = 7) 
```
```{r resulting_data}
drop = datiprep %>% dplyr::select(-ID:LOCATION, ) %>%  names

datiprep = datiprep %>%  
  dplyr::select(-all_of(drop))

```


### Covariates Imputation

A simple approach to front missing observations is to build a regression model to explain the covariates with the missing observations and plug-in the obtained estimates (e.g. posterior means) from their predictive distributions (Little and Rubin 2002). This approach is fast and easy to implement in most cases, but it ignores the uncertainty about the imputed values. However it has the benefit to be a reasonable choice with respect to the numbr of computation required. That makes it the first choice method to follow.
A further method for imputation has been designed by _Gómez-Rubio, Cameletti, and Blangiardo 2019) miss lit_ by adding a sub-model for the imputations to the final model through the inla functio. This is directly handled inside the predictor formula adding a parameter in the latent field. However the approach makes the model more complex with a further layer of uncertainty to handle. 
At first the additive regression model with all the covariates is called including the covariates with missing values. The response variable *PRICE* displys no missing values.

```{r imputation, eval=FALSE}
data_miss = datiprep %>%  
  rename_all(tolower) %>% 
  select(-id,-lat,-long,-location, -buildage, -totlocali, -cucina, -altro, -ac,-pub_date, -catastinfo, -aptchar, -age,-lowrdprice.originalprice,-lowrdprice.currentprice, -lowrdprice.passeddays,-lowrdprice.date,-enclass,-contr ,-disp, -pauto,-review,  -hasmulti,-title) %>% 
  as.data.frame() 

cov_imputation = inla(price ~ 1 + condom +floor+ascensore+accdisabili+indivsapt+camereletto+bagno+status+heating+photosnum+totpiani+sqfeet+nroom, data = data_miss, verbose = TRUE )
```


When the model is called INLA fit the model on the existing observations ignoring the missing ones. A summary of the model is now proposed. Once the model is trained then the posterior means can be imputed in the place where the respective covariates are missing. Then the model is called a second time to check changes in parameter estimates. The approach followed kindly differ from @Rubio2019 section 12.4 since response var in this case has not missing values, this should anyway give a better estimates since the model is trained on the whole set of the response variable observation. On the other hand, more than one covariates, the ones sentenced to be MAR, have missing values, therefore inla ultimately trains the model on the covariates' missing union set. e.g. missing observations CONDOM `r dati %>%  drop_na(CONDOM) %>%  dim() %>% .[1]`, missing in combination with other covariates.
`r dati %>%  drop_na() %>%  dim() %>% .[1]`




## Spatial Autocorrelation assessement 




Spatial data come packed into point reference 

- tmap 
- leaflet 
- gganimate 

```{r semivariogram}
semivar = variogram(PRICE~1, 
                       locations= ~LONG+LAT,
                       data=dati)
```

## Model Specification



## Mesh building 

*PARAFRASARE*
The SPDE approach approximates the continuous Gaussian field $w_{i}$ as a discrete Gaussian Markov random field by means of a finite basis function defined on a triangulated mesh of the region of study. The spatial surface can be interpolated performing this approximation with the inla.mesh.2d() function of the R-INLA package. This function creates a Constrained Refined Delaunay Triangulation (CRDT) over the study region, that will be simply referred to as the mesh. Mesh should be intended as a trade off between the accuracy of the GMRF surface representation and the computational cost, in other words the more are the vertices, the finer is the GF approximation, leading to a computational funnel. 

![Traingularization intuition, @Krainski-Rubio source](images/triangle.jpg)

Arguments can tune triangularization through inla.mesh.2d() :

* `loc`:location coordinates that are used as initial mesh vertices
* `boundary`:object describing the boundary of the domain,
* `offset`:  argument is a numeric value (or a length two vector) and it is used
to set the automatic extension distance. If positive, it is the extension distance
in the same scale units. If negative, it is interpreted as a factor relative to the
approximate data diameter; i.e., a value of -0.10 (the default) will add a 10%
of the data diameter as outer extension.
* `cutoff`: points at a closer distance than the supplied value are replaced by a single vertex. Hence, it avoids small triangles 
* `max.edge`: A good mesh needs to have triangles as regular as possible in size and shape.
* `min.angle`argument (which can be scalar or length two vector) can be used to specify the minimum internal angles of the triangles in the inner domain and the outer extension

A convex hull is a polygon of triangles out of the domain area, in other words the extension made to avoid the boundary effect. All meshes in Figure 2.12 have been made to have a convex hull boundary. If borders are available are generally preferred, so non convex hull meshes are avoided.

```{r non_convex_hull, eval=FALSE}
bound = inla.nonconvex.hull(coords)
```

### Shinyapp for mesh assessment

INLA includes a Shiny (Chang et al., 2018) application that can be used to tune the mesh params interactively

```{r meshbuilder, eval = FALSE}
INLA::meshbuilder()
```


The mesh builder has a number of options to define the mesh on the left side. These include options to be passed to functions inla.nonconvex.hull() and inla.mesh.2d() and the resulting mesh displayed on the right part.

### BUilding SPDE model on mesh




## Spatial Kriging (Prediction)

QUI INCERTEZZE
```{r variogram}
semivar = variogram(PRICE~1, 
                       locations= ~LONG+LAT,
                       data=dati)
# 
# fit.vgm = fit.variogram(semivar, vgm("Sph"))
# krg = krige(log(PRICE) ~ 1, data, model = fit.vgm)

```








