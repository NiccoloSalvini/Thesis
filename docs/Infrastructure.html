<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Infrastructure | Spatial Machine Learning modelling: End-to-End web app solution</title>
  <meta name="description" content="Chapter 3 Infrastructure | Spatial Machine Learning modelling: End-to-End web app solution by Niccolò Salvini" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Infrastructure | Spatial Machine Learning modelling: End-to-End web app solution" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />
  <meta property="og:description" content="Chapter 3 Infrastructure | Spatial Machine Learning modelling: End-to-End web app solution by Niccolò Salvini" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Infrastructure | Spatial Machine Learning modelling: End-to-End web app solution" />
  
  <meta name="twitter:description" content="Chapter 3 Infrastructure | Spatial Machine Learning modelling: End-to-End web app solution by Niccolò Salvini" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="scraping.html"/>
<link rel="next" href="spatial.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Spatial Modelling: End-to-End web App solution</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#what-is-scraping"><i class="fa fa-check"></i><b>2.1</b> What is Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#scraping-best-practices-and-robot.txt"><i class="fa fa-check"></i><b>2.2</b> Scraping Best Practices and Robot.txt</a></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#user-agents-proxies-handlers"><i class="fa fa-check"></i><b>2.3</b> User agents, Proxies, Handlers</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scraping.html"><a href="scraping.html#user-agents-spoofing"><i class="fa fa-check"></i><b>2.3.1</b> User agents Spoofing</a></li>
<li class="chapter" data-level="2.3.2" data-path="scraping.html"><a href="scraping.html#handlers"><i class="fa fa-check"></i><b>2.3.2</b> Handlers</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#how-they-are-designed-with-rvest"><i class="fa fa-check"></i><b>2.4</b> How they are designed with <code>rvest</code></a><ul>
<li class="chapter" data-level="2.4.1" data-path="scraping.html"><a href="scraping.html#from-generic-and-specific-structure"><i class="fa fa-check"></i><b>2.4.1</b> From Generic and Specific structure</a></li>
<li class="chapter" data-level="2.4.2" data-path="scraping.html"><a href="scraping.html#parallel-computing"><i class="fa fa-check"></i><b>2.4.2</b> Parallel Computing</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#what-are-the-advantages-of-this-workflow"><i class="fa fa-check"></i><b>2.5</b> What are the Advantages of this Workflow</a></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#legal-challenges-ancora-non-validato"><i class="fa fa-check"></i><b>2.6</b> Legal Challenges (ancora non validato)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> Infrastructure</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker-container"><i class="fa fa-check"></i><b>3.2</b> Docker Container</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-docker"><i class="fa fa-check"></i><b>3.2.1</b> What is Docker?</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#what-are-the-main-andvantages-of-using-docker"><i class="fa fa-check"></i><b>3.2.2</b> What are the main andvantages of using Docker</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.3</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api"><i class="fa fa-check"></i><b>3.3</b> API</a></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-an-api"><i class="fa fa-check"></i><b>3.4</b> What is an API</a><ul>
<li class="chapter" data-level="3.4.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-in-practice-an-api-does"><i class="fa fa-check"></i><b>3.4.1</b> What in practice an API does</a></li>
<li class="chapter" data-level="3.4.2" data-path="Infrastructure.html"><a href="Infrastructure.html#plumber-api"><i class="fa fa-check"></i><b>3.4.2</b> Plumber API</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="spatial.html"><a href="spatial.html"><i class="fa fa-check"></i><b>4</b> Spatial Statistics</a><ul>
<li class="chapter" data-level="4.1" data-path="spatial.html"><a href="spatial.html#gentle-introduction"><i class="fa fa-check"></i><b>4.1</b> Gentle Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="spatial.html"><a href="spatial.html#inla-estimation"><i class="fa fa-check"></i><b>4.2</b> INLA estimation</a></li>
<li class="chapter" data-level="4.3" data-path="spatial.html"><a href="spatial.html#presentation-of-data"><i class="fa fa-check"></i><b>4.3</b> Presentation of data</a></li>
<li class="chapter" data-level="4.4" data-path="spatial.html"><a href="spatial.html#point-referenced-data-models"><i class="fa fa-check"></i><b>4.4</b> Point-referenced data models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prd.html"><a href="prd.html"><i class="fa fa-check"></i><b>5</b> Point Referenced Data</a><ul>
<li class="chapter" data-level="5.1" data-path="prd.html"><a href="prd.html#point-referenced-modeling"><i class="fa fa-check"></i><b>5.1</b> Point-Referenced modeling</a><ul>
<li class="chapter" data-level="5.1.1" data-path="prd.html"><a href="prd.html#stationarity"><i class="fa fa-check"></i><b>5.1.1</b> Stationarity</a></li>
<li class="chapter" data-level="5.1.2" data-path="prd.html"><a href="prd.html#variograms"><i class="fa fa-check"></i><b>5.1.2</b> Variograms</a></li>
<li class="chapter" data-level="5.1.3" data-path="prd.html"><a href="prd.html#isotropy"><i class="fa fa-check"></i><b>5.1.3</b> Isotropy</a></li>
<li class="chapter" data-level="5.1.4" data-path="prd.html"><a href="prd.html#variogram-model-fitting"><i class="fa fa-check"></i><b>5.1.4</b> Variogram model fitting</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="prd.html"><a href="prd.html#anisotropy"><i class="fa fa-check"></i><b>5.2</b> Anisotropy</a></li>
<li class="chapter" data-level="5.3" data-path="prd.html"><a href="prd.html#exploratory-analysis"><i class="fa fa-check"></i><b>5.3</b> Exploratory analysis</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesianspat.html"><a href="bayesianspat.html"><i class="fa fa-check"></i><b>6</b> Bayesian Spatial Modelling</a><ul>
<li class="chapter" data-level="6.1" data-path="bayesianspat.html"><a href="bayesianspat.html#inla"><i class="fa fa-check"></i><b>6.1</b> INLA</a></li>
<li class="chapter" data-level="6.2" data-path="bayesianspat.html"><a href="bayesianspat.html#laplace-approximation"><i class="fa fa-check"></i><b>6.2</b> Laplace Approximation</a></li>
<li class="chapter" data-level="6.3" data-path="bayesianspat.html"><a href="bayesianspat.html#the-class-of-latent-gaussian-models"><i class="fa fa-check"></i><b>6.3</b> The Class of Latent Gaussian Models</a><ul>
<li class="chapter" data-level="6.3.1" data-path="bayesianspat.html"><a href="bayesianspat.html#approximate-bayesian-inference-with-inla"><i class="fa fa-check"></i><b>6.3.1</b> Approximate Bayesian inference with INLA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>7</b> Applications</a><ul>
<li class="chapter" data-level="7.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>7.1</b> Example one</a></li>
<li class="chapter" data-level="7.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>7.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>8</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Spatial Machine Learning modelling: End-to-End web app solution</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Infrastructure" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Infrastructure</h1>
<!--  You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).-->
<p>In order to provide a fast, portable and integrated product to the end user It has been designed a quite straightforward software architecture. We have already seen the scraping functions and how they are built around the concept of easy flexibility and debugging. This is due to the fact that they should extract something that is dynamic, it is not sure that it will be as the day before. The data we are trying to grab might have been moved somewhere else throughout the website. Or it might have placed extra expression inside the node we are inspecting (“$” sign following the monthly rental price) . A very often occurring example regards the way information concerning the house are represented in the website. Considering the september 2018 january 2020 time span the design of the website has changed a vast number of times. Since both the design and the scrapping functions relies on the HTML skelethon and CSS queries. As soon as something changes in the website the other files needs to be readjusted to be consistent with the content and so back and forth. The debugging handlers nested in the functions helps the maintainer to grasp what it is not working properly where the error occur.
The same inner philosophy has been applied to the software architecture chosen for this project.
First of all the wide range of open source solutions (back-end and front-end) and documentation on this has made many analyst and data scientist almost full stack developer. This was also due to the fact that RStudio has set very well oriented guidelines spending a lot of effort giving its users an easy, integrated and interconnected environment. By that it is meant that recently the RStudio community has developed, on top of many different others, an entire package dedicated to REST APIs (Plumber <span class="citation">(Trestle Technology, LLC <a href="#ref-plumber" role="doc-biblioref">2018</a>)</span>). MOreover developers in RStudio and its contributors have created an entire new paradigm called Shiny <span class="citation">(Chang et al. <a href="#ref-shiny" role="doc-biblioref">2020</a>)</span>, a popular web app development package, that forces the user to have front-end and back-end technologies tied up in the same IDE (RStudio) and with a unique language to deal with. The front end file (for simplicity named UI.R) contains the UI’s layout and the style and also other javascripts components. On the other hand the server file (named after server.R) absorbs the back-end, under the hood, code and makes the UI intercact and respond to the user.
This comes at a cost of flexibility and customization since Shiny could not easily handle too many embellishments (even though potentially can). Nevertheless a unique environment makes integration with other technologies easier and most of all introduces the analyst to a full stack approach. Many open source projects are gravitating around the Shiny framework with the aim to extend its capabilities. One example is a newly created package called reactR <span class="citation">(Inc et al. <a href="#ref-reactR" role="doc-biblioref">2020</a>)</span> that allows user to implement the power of React.js into the shiny UI front end. All of this is possible, once again, by the R community but a greater contribution come from digging up the right path along which everything by open source comes natural. (parallelo con la vigna e l’albero che la sostiene e indirizza)
The carrier idea for this project is to have parallelized scraping functions called daily by a scheduler producing and subsequently storing a .csv file in a MySQL /cartoDB database. They are all tought to be containerized in a Linux (Ubuntu distr) docker container hosted in a AWS EC2 server. Then in a second container a Shiny app is placed, this one pipes in data from the former infrastructure and apply the statistical model stored (by an API call) in its server.R part.</p>
<p>The main technologies implied are:</p>
<ul>
<li>Scheduler cron job</li>
<li>Docker containers</li>
<li>Shiny</li>
<li>Plumber REST API</li>
<li>AWS (Amazon Web Services) EC2</li>
<li>CartoDB</li>
</ul>
<p>On top of that even each single part of this thesis has been made stand alone and can be easily accessed and modified through this <a href="https://niccolosalvini.github.io/Thesis/">link</a>. The pdf (theis) version of the gitbook can be obtained by clicking the download button that can be seen in figure below. A Latex engine (Xelatex) wrapped into the website compiles a sequence of Markdown documents converting them into .html (the book’s chapters) which are formatted by rules grouped in a .yml file. All the documents are pushed to a Github repository with git. By a simple trick, since all the files are static html, they can be displayed through GH pages as it is a website. All of this has been possible thanks to Bookdown <span class="citation">(Xie <a href="#ref-bookdown1" role="doc-biblioref">2020</a>)</span> once again a R well documented package <span class="citation">(Xie <a href="#ref-bookdown2" role="doc-biblioref">2016</a>)</span> to build interactive books along with RMarkdown <span class="citation">(Allaire et al. <a href="#ref-rmarkdown1" role="doc-biblioref">2020</a>)</span>.</p>
<p>An empirical observation of immobiliare.it has suggested that houses rents advertisement are continuously added and then removed during the day. Fresh data is needed to have updated analysis since the scope in here is to offer realtime considerations. Something should be automated periodically in order to address the issue. Moreover, as rule of thumb, a daily data extraction might be a good option for some reasons. It can intercept price variations with a relatively small time lag, It can also display some sort of pattern in time that would help the reader/user to select the perfect choice. As a consequence a daily .csv file is generated and directly collected into a Db folder arranged by time
The solutiond proposed takes care of the issue by making the scraping script generating the .csv be executed by a scheduler.</p>
<div id="scheduler" class="section level2">
<h2><span class="header-section-number">3.1</span> Scheduler</h2>
<p>A Scheduler in a process is a component on a OS that allows the computer to decide which activity is going to be executed. In the context of multi-programming it is thought as a tool to keep CPU occupied as much as possible. As an example it can trigger a process while some other is still waiting to finish. There are many type of scheduler and they are based on the frequency of times they are executed considering a certain closed time neighbor.</p>
<ul>
<li>Short term scheduler: it can trigger and queue the “ready to go” tasks
<ul>
<li>with pre-emption</li>
<li>without pre-emption</li>
</ul></li>
</ul>
<p>The ST scheduler selects the process and It gains control of the CPU by the dispatcher. OIn this context we can define latency as the time needed to stop a process and to start a new one.</p>
<ul>
<li>Medium term scheduler</li>
<li>Long term scheduler</li>
</ul>
<p>for some other useful but beyond the scope information, such as the scheduling algorithm the reader can refer to <span class="citation">(Wikiversità <a href="#ref-wiki:scheduler" role="doc-biblioref">2020</a>)</span>.</p>
<p>The scheduler in this context cosists in a .sh (shell file, sort of text file) composed by a set of instructions that are being executed by the computer on daily basis. This file has to be in the same WD ( working directory) of the project in order to make it working. Some common issues can occur when new files coming after the execution of the scheduled main script are generated, but the path isnt explicitly specified. This can lead to the partial or incomplete generation of the file since the shell file is executed within the folder but is triggered by some other location on the computer.
Each OS has its own scheduler and syntax to call it. Since we are interested in Ubuntu machines the scheduler is said to be a cron job. Later it will be clear why Ubuntu is the option to pursue.</p>
<p><strong>va parafrasato</strong></p>
<div id="cron-jobs" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Cron Jobs</h3>
<p>The software utility cron also known as cron job is a time-based job scheduler in Unix-like computer operating systems. Users that set up and maintain software environments use cron to schedule jobs (commands or shell scripts) to run periodically at fixed times, dates, or intervals. It typically automates system maintenance or administration—though its general-purpose nature makes it useful for things like downloading files from the Internet and downloading email at regular intervals.</p>
<p>The actions of cron are driven by a crontab (cron table) file, a configuration file that specifies shell commands to run periodically on a given schedule. The crontab files are stored where the lists of jobs and other instructions to the cron daemon are kept. Users can have their own individual crontab files and often there is a system-wide crontab file (usually in /etc or a subdirectory of /etc) that only system administrators can edit.</p>
<p>Each line of a crontab file represents a job, and looks like this:</p>
<div class="figure">
<img src="images/crontab.PNG" alt="" />
<p class="caption">crontab</p>
</div>
<p>Each line of a crontab file represents a job. This example runs a shell program called scheduler.sh at 23:45 (11:45 PM) every Saturday.</p>
<p>45 23 * * 6 /home/oracle/scripts/scheduler.sh</p>
<p>Some rather unusual scheduling definitions and syntax for cronjobs can be found in this reference <span class="citation">(Wikipedia contributors <a href="#ref-wiki:cronjob" role="doc-biblioref">2020</a>)</span></p>
<p>The cron job applied to the script needs to be ran at 11:30 PM everyday. It has that forms:
—&gt; qui immagine</p>
<p><strong>va parafrasato</strong></p>
<p>For now the computational power comes from the machine on which the system is installed. A smarter solution takes into consideration that the former infrastructure has its own limits. Major limits comprehend run time since at the same moment the machine runs locally both the scraping functions and the app computations. This to a certain extent might fit for personal use but as data increases all the system risks to fail. It is also totally local so the analysis can not be shared with anyone. This problem can be addressed with a technology that has seen a huge growth in its usage in the last few years: Docker containers.</p>
</div>
</div>
<div id="docker-container" class="section level2">
<h2><span class="header-section-number">3.2</span> Docker Container</h2>
<p><strong>from docker </strong>
In 2013, Docker introduced what would become the industry standard for containers. A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.</p>
<div id="what-is-docker" class="section level3">
<h3><span class="header-section-number">3.2.1</span> What is Docker?</h3>
<p>Container images become containers at runtime and in the case of Docker containers - images become containers when they run on Docker Engine. Available for both Linux and Windows-based applications, containerized software will always run the same, regardless of the infrastructure. Containers isolate software from its environment and ensure that it works uniformly despite differences for instance between development and staging.</p>
<div class="figure">
<img src="images/docker-ex.png" alt="" />
<p class="caption">docker example</p>
</div>
<p>Docker leveraged existing computing concepts around containers and specifically in the Linux world. Docker’s technology is unique because it focuses on the requirements of developers and systems operators to separate application dependencies from infrastructure.</p>
<p>A question might come up about why a Virtual Machine could not be a preferable container for our specified task. Well, Containers and virtual machines have similar resource isolation and allocation benefits, but function differently because containers virtualize the operating system instead of hardware. Containers are more portable and efficient.</p>
<div class="figure">
<img src="images/container-vm-whatcontainer_2.png" alt="" />
<p class="caption">docker container vs VM</p>
</div>
<p><strong>from docker </strong></p>
</div>
<div id="what-are-the-main-andvantages-of-using-docker" class="section level3">
<h3><span class="header-section-number">3.2.2</span> What are the main andvantages of using Docker</h3>
<p><strong>va parafrasato from Matt Dancho</strong></p>
<p>Indeed, the popular employment-related search engine, released an article this past Tuesday showing changing trends from 2015 to 2019 in “Technology-Related Job Postings”. We can see a number of changes in key technologies - One that we are particularly interested in is the 4000% increase in Docker.</p>
<div class="figure">
<img src="images/Inkedindeed_jobs_LI.jpg" alt="" />
<p class="caption">docker-stats</p>
</div>
<p>The landscape of Data Science is changing <span class="citation">(Economist at the Indeed Hiring Lab <a href="#ref-Skills_Explorer" role="doc-biblioref">2020</a>)</span> from reporting to application building:</p>
<p>In 2015 - Businesses need reports to make better decisions
In 2020 - Businesses need apps to empower better decision making at all levels of the organization
This transition is challenging the Data Scientist to learn new technologies to stay relevant…</p>
<p>As a matter of fact, it is no longer sufficient to just know machine learning algorithms. Future data workers need to know how to put machine learning into production as quickly as possible to meet the business needs. This can be done either integrating existing obsolete/old technologies with the new ones, or build a solid, portable and scalable infrastructure to better the processes.
In order to do so, It is strictly needed to learn from programmers the basics of Software Engineering. The extremely good news is that no data scientist whatsoever needs to be a perfect software engineer, but at least he has to know how to integrate already built techonlogies with its work. This truly can help in the quest to unleash data science at scale and unlock real business value.
The way Docker does that are many <span class="citation">(“7.2. Advantages of Using Docker Red Hat Enterprise Linux 7” <a href="#ref-red_hat_customer_portal" role="doc-biblioref">2020</a>)</span>:</p>
<ul>
<li><em>Rapid application deployment</em> : containers include the minimal run time requirements of the application, reducing their size and allowing them to be deployed quickly.</li>
<li><em>Portability across machines</em> :an application and all its dependencies can be bundled into a single container that is independent from the host version of Linux kernel, platform distribution, or deployment model. This container can be transfered to another machine that runs Docker, and executed there without compatibility issues.</li>
<li><em>Version control and component reuse</em> : you can track successive versions of a container, inspect differences, or roll-back to previous versions. Containers reuse components from the preceding layers, which makes them noticeably lightweight.</li>
<li><em>Sharing</em> : you can use a remote repository to share your container with others. It is also possible to configure a private repository hosted on Docker Hub.</li>
<li><em>Lightweight footprint and minimal overhead</em> : Docker images are typically very small, which facilitates rapid delivery and reduces the time to deploy new application containers.</li>
<li><em>Simplified maintenance</em> :Docker reduces effort and risk of problems with application dependencies.</li>
</ul>
<p>The way all of this is possible is a dockerfile that determines the instruction that docker has to perform to abstract the environment.</p>
</div>
<div id="dockerfile" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Dockerfile</h3>
<p>Docker can build images automatically by interpreting the instructions from a Dockerfile. A Dockerfile can be thought as a written recipe to cook a specific cake, with all the ingredients described in a piece of a paper using a generic oven and adding layer of preparation after layer. A Dockerfile is a text format document that contains all the commands/rules a generic user could call on the CLI to assemble an image. Executing the command <code>docker build</code> from shell the user can trigger the image building. That executes several command-line instructions in chronological succession of steps.
The Dockerfile used to trigger the build of the docker image has this following set of instructions:</p>
<div class="figure">
<img src="images/dockerfile.PNG" alt="" />
<p class="caption">dockerfile</p>
</div>
<ul>
<li><p><code>FROM rocker/r-ver:4.0.0</code> : the command imports an image already written by the rocker team (authored contributors for the R docker project) that contains the base-R version 4.0.0. Recently with the 4.0 version the RStudio team has created a repository management server for its packages that organizes and centralizes R packages (offline access and checkpoints). This will shorten the installation time and secure packages since they all can be freezed into a version that make the whole system works.</p></li>
<li><p><code>RUN R -e "install.packages(c('plumber','tibble','...',dependencies=TRUE)</code> : the command install all the packages required to execute the files (R files) containerized for the scraping. Since all the packages have their dependencies the option <code>dependencies=TRUE</code> is needed.</p></li>
<li><p><code>EXPOSE 8000</code> : the commands instructs Docker that the container listens on the specified network ports 8000 at runtime. It is possible to specify whether the port exposed listens on UDP or TCP, the default is TCP (this part needs a previous set up previous installing, for further online documentation It is reccomended <span class="citation">(“Get Docker” <a href="#ref-docker_documentation_2020" role="doc-biblioref">2020</a>)</span> )</p></li>
<li><p><code>ENTRYPOINT ["Rscript", "main.R"]</code> : the command tells docker to execute the Rscript extension file main.R within the container that triggers the API building/the generation of the .csv file.</p></li>
</ul>
<p><strong>va parafrasato from Matt Dancho</strong></p>
<p>An alternative and very used approach could be wrapping all the scraping function into an API and then send a <code>GET</code> request to the API endpoint needed.</p>
</div>
</div>
<div id="api" class="section level2">
<h2><span class="header-section-number">3.3</span> API</h2>
<p><strong>va parafrasato</strong></p>
<p>The scraping functions, according to how the author as structured them, are able to produce two .csv extension (if the boolean option <code>write</code> is set = TRUE) files. As already clarified in the previous ssection some point should be joined by a primary key. But for the sake of
In order to give the possibility to have a daily updated saptial analysis on data we need to continously have fresh data. In the website data come and go, as products in a marketplace, so the main idea is to have something that catches the new added and deletes what it is already taken.
Nowadays we have many open source, nearly cost free, techonlogies that allow us to have corporate grade applications that can be orizontally scaled at need. Most of them come with great docuemntation and ready to use examples that flatten the learning curve.
The first choice that has to be made is: either to provide a .csv file day by day with all the data to feed the application, or we exploit some portable and fast solutions as API.</p>
<p><strong>va parafrasato</strong></p>
</div>
<div id="what-is-an-api" class="section level2">
<h2><span class="header-section-number">3.4</span> What is an API</h2>
<p>API is a set of definitions and protocols for building and integrating application software. API stands for application programming interface.</p>
<p>APIs let a product or a service communicate with other products and services without having to know how they’re implemented. This can simplify app development, saving time and money. When you’re designing new tools and products—or managing existing ones—APIs give flexibility; simplify design, administration, and use; and provide opportunities for innovation.
APIs are sometimes thought of as contracts, with documentation that represents an agreement between parties: If party 1 sends a remote request structured a particular way, this is how party 2’s software will respond.</p>
<p>API examples:
- Google Maps API: allows developers embed geo-location data using JavaScript. The Google Maps API is designed to work on mobile and desktop.
- YouTube API: allows developers integrate YouTube videos and functionalities into websites or applications.
- Google Analytics API: allows to track website performance in terms of audience, monetization and other important metrics throught the Google Analytics interface. The website the thesis come from has this implementation working.</p>
<div class="figure">
<img src="images/API-page-graphic.png" alt="" />
<p class="caption">API functioning</p>
</div>
<p>Because APIs simplify how developers integrate new application components into an existing architecture, they help business and IT teams collaborate. Business needs often change quickly in response to ever shifting digital markets, where new competitors can change a whole industry with a new app. In order to stay competitive, it’s important to support the rapid development and deployment of innovative services. Cloud-native application development is an identifiable way to increase development speed, and it relies on connecting a microservices application architecture through APIs.</p>
<div id="what-in-practice-an-api-does" class="section level3">
<h3><span class="header-section-number">3.4.1</span> What in practice an API does</h3>
<p>API, strictly talking to this thesis extent, are cloud infrastructure that given a specific URL ( and the most of the times credentials) are outputting data. Data comes in a given format (the most of the times JSON),so that they can be instantly pre-processed and elaborated, in this case by oine other software, the Shiny app.</p>
<p><strong>(va prafrasato)</strong></p>
<p>Since website are continuously changed for many reasons API philosophy results in a smarter choice since it makes easy to access data and flex API endpoints to the business needs.</p>
</div>
<div id="plumber-api" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Plumber API</h3>
<p>Plumber <span class="citation">(“An Api Generator for R” <a href="#ref-api_generator_for_r" role="doc-biblioref">2020</a>)</span> allows the user to create a web API by simply adding decoration comments to the existing R code. Decorations are a special type of comments that suggests the plumber where and when the API parts are. Here below a toy example from the reference:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="Infrastructure.html#cb11-1"></a><span class="co"># plumber.R file</span></span>
<span id="cb11-2"><a href="Infrastructure.html#cb11-2"></a></span>
<span id="cb11-3"><a href="Infrastructure.html#cb11-3"></a><span class="co"># * Echo back the input * @param msg The message to echo * @get /echo</span></span>
<span id="cb11-4"><a href="Infrastructure.html#cb11-4"></a><span class="cf">function</span>(<span class="dt">msg =</span> <span class="st">&quot;&quot;</span>) {</span>
<span id="cb11-5"><a href="Infrastructure.html#cb11-5"></a>    <span class="kw">list</span>(<span class="dt">msg =</span> <span class="kw">paste0</span>(<span class="st">&quot;The message is: &#39;&quot;</span>, msg, <span class="st">&quot;&#39;&quot;</span>))</span>
<span id="cb11-6"><a href="Infrastructure.html#cb11-6"></a>}</span>
<span id="cb11-7"><a href="Infrastructure.html#cb11-7"></a></span>
<span id="cb11-8"><a href="Infrastructure.html#cb11-8"></a><span class="co"># * Plot a histogram * @png * @get /plot</span></span>
<span id="cb11-9"><a href="Infrastructure.html#cb11-9"></a><span class="cf">function</span>() {</span>
<span id="cb11-10"><a href="Infrastructure.html#cb11-10"></a>    rand =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb11-11"><a href="Infrastructure.html#cb11-11"></a>    <span class="kw">hist</span>(rand)</span>
<span id="cb11-12"><a href="Infrastructure.html#cb11-12"></a>}</span>
<span id="cb11-13"><a href="Infrastructure.html#cb11-13"></a></span>
<span id="cb11-14"><a href="Infrastructure.html#cb11-14"></a><span class="co"># * Return the sum of two numbers * @param a The first number to add * @param b</span></span>
<span id="cb11-15"><a href="Infrastructure.html#cb11-15"></a><span class="co"># The second number to add * @post /sum</span></span>
<span id="cb11-16"><a href="Infrastructure.html#cb11-16"></a><span class="cf">function</span>(a, b) {</span>
<span id="cb11-17"><a href="Infrastructure.html#cb11-17"></a>    <span class="kw">as.numeric</span>(a) <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(b)</span>
<span id="cb11-18"><a href="Infrastructure.html#cb11-18"></a>}</span></code></pre></div>
<p>This chunk of code assembled into a .R file has three endpoints where interruption occurs.</p>
<p>Special comments are marked as this <code>#*</code> and they are followed by specific keywords denoted with <code>@</code>.
- the name of the API (unique without the <code>@</code>)
- the <code>@params</code> keyword refers to parameter that has to be inputted to give the result of the function define below. If in the function below defualt parameters are stated then the API response is the elaboration of the functions with those parameters. If the function does not specify any parameter, see the below endpoints below, plumber has to make sure that the function can run without them.
- the <code>#* @png</code> chuck piece specify the extension of the output file.
- the <code>#* @get /plot</code> decorations specify the type of HTTP request we are sending, in this case a GET request. The user in this case is requesting some information to the API, the name is plot, so the expectations are that a plot is given as response</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-red_hat_customer_portal">
<p>“7.2. Advantages of Using Docker Red Hat Enterprise Linux 7.” 2020. <em>Red Hat Customer Portal</em>. <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/7.0_release_notes/sect-red_hat_enterprise_linux-7.0_release_notes-linux_containers_with_docker_format-advantages_of_using_docker">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/7.0_release_notes/sect-red_hat_enterprise_linux-7.0_release_notes-linux_containers_with_docker_format-advantages_of_using_docker</a>.</p>
</div>
<div id="ref-rmarkdown1">
<p>Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2020. <em>Rmarkdown: Dynamic Documents for R</em>. <a href="https://github.com/rstudio/rmarkdown">https://github.com/rstudio/rmarkdown</a>.</p>
</div>
<div id="ref-api_generator_for_r">
<p>“An Api Generator for R.” 2020. <em>An API Generator for R</em>. <a href="https://www.rplumber.io/">https://www.rplumber.io/</a>.</p>
</div>
<div id="ref-shiny">
<p>Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2020. <em>Shiny: Web Application Framework for R</em>. <a href="https://CRAN.R-project.org/package=shiny">https://CRAN.R-project.org/package=shiny</a>.</p>
</div>
<div id="ref-Skills_Explorer">
<p>Economist at the Indeed Hiring Lab, Andrew FlowersAndrew Flowers was previously an. 2020. “Indeed Tech Skills Explorer: Today’s Top Tech Skills.” <a href="https://www.hiringlab.org/2019/11/19/todays-top-tech-skills/">https://www.hiringlab.org/2019/11/19/todays-top-tech-skills/</a>.</p>
</div>
<div id="ref-docker_documentation_2020">
<p>“Get Docker.” 2020. <em>Docker Documentation</em>. <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</p>
</div>
<div id="ref-reactR">
<p>Inc, Facebook, Michel Weststrate, Kent Russell, and Alan Dipert. 2020. <em>ReactR: React Helpers</em>. <a href="https://CRAN.R-project.org/package=reactR">https://CRAN.R-project.org/package=reactR</a>.</p>
</div>
<div id="ref-plumber">
<p>Trestle Technology, LLC. 2018. <em>Plumber: An Api Generator for R</em>. <a href="https://CRAN.R-project.org/package=plumber">https://CRAN.R-project.org/package=plumber</a>.</p>
</div>
<div id="ref-wiki:cronjob">
<p>Wikipedia contributors. 2020. “Cron — Wikipedia, the Free Encyclopedia.” <a href="https://en.wikipedia.org/w/index.php?title=Cron&amp;oldid=978051592">https://en.wikipedia.org/w/index.php?title=Cron&amp;oldid=978051592</a>.</p>
</div>
<div id="ref-wiki:scheduler">
<p>Wikiversità. 2020. “Scheduling — Wikiversità,” <a href="https://it.wikiversity.org/w/index.php?title=Scheduling&amp;oldid=214572">https://it.wikiversity.org/w/index.php?title=Scheduling&amp;oldid=214572</a>.</p>
</div>
<div id="ref-bookdown2">
<p>Xie, Yihui. 2016. <em>Bookdown: Authoring Books and Technical Documents with R Markdown</em>. Boca Raton, Florida: Chapman; Hall/CRC. <a href="https://github.com/rstudio/bookdown">https://github.com/rstudio/bookdown</a>.</p>
</div>
<div id="ref-bookdown1">
<p>Xie, Yihui. 2020. <em>Bookdown: Authoring Books and Technical Documents with R Markdown</em>. <a href="https://github.com/rstudio/bookdown">https://github.com/rstudio/bookdown</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="scraping.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="spatial.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/Thesis/edit/master/03-infrastructure.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf", "Niccolo_Salvini_Thesis.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
