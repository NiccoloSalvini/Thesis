<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Infrastructure | REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA.</title>
  <meta name="description" content="Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Infrastructure | REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA." />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />
  <meta property="og:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Infrastructure | REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA." />
  
  <meta name="twitter:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="scraping.html"/>
<link rel="next" href="exploratory.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"> REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#what-is-web-scraping"><i class="fa fa-check"></i><b>2.1</b> What is Web Scraping</a><ul>
<li class="chapter" data-level="2.1.1" data-path="scraping.html"><a href="scraping.html#webstructure"><i class="fa fa-check"></i><b>2.1.1</b> Immobiliare.it Webscraping website structure</a></li>
<li class="chapter" data-level="2.1.2" data-path="scraping.html"><a href="scraping.html#immobiliare.it-webscraping-content-architecture-with-rvest"><i class="fa fa-check"></i><b>2.1.2</b> Immobiliare.it Webscraping content architecture with <code>rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#best-practices"><i class="fa fa-check"></i><b>2.2</b> Scraping Best Practices and Robottxt</a></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#user-agents-proxies-handlers"><i class="fa fa-check"></i><b>2.3</b> User agents, Proxies, Handlers</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scraping.html"><a href="scraping.html#spoofing"><i class="fa fa-check"></i><b>2.3.1</b> User agents Spoofing</a></li>
<li class="chapter" data-level="2.3.2" data-path="scraping.html"><a href="scraping.html#handlers"><i class="fa fa-check"></i><b>2.3.2</b> Handlers</a></li>
<li class="chapter" data-level="2.3.3" data-path="scraping.html"><a href="scraping.html#parallel-computing"><i class="fa fa-check"></i><b>2.3.3</b> Parallel Computing</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#challenges"><i class="fa fa-check"></i><b>2.4</b> Further Improvements</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#legal-challenges-ancora-non-validato"><i class="fa fa-check"></i><b>2.5</b> Legal Challenges (ancora non validato)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> Infrastructure</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>3.2</b> Docker</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-docker"><i class="fa fa-check"></i><b>3.2.1</b> What is Docker</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#why-docker"><i class="fa fa-check"></i><b>3.2.2</b> Why Docker</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.3</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api"><i class="fa fa-check"></i><b>3.3</b> API</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>3.3.1</b> Plumber API</a></li>
<li class="chapter" data-level="3.3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#immobiliare.it-http-api"><i class="fa fa-check"></i><b>3.3.2</b> Immobiliare.it HTTP API</a></li>
<li class="chapter" data-level="3.3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api-source-code"><i class="fa fa-check"></i><b>3.3.3</b> API source code</a></li>
<li class="chapter" data-level="3.3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#api-documentation"><i class="fa fa-check"></i><b>3.3.4</b> API documentation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>3.4</b> NGINX reverse proxy server</a></li>
<li class="chapter" data-level="3.5" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>3.5</b> AWS EC2 server</a><ul>
<li class="chapter" data-level="3.5.1" data-path="Infrastructure.html"><a href="Infrastructure.html#launch-an-ec2-instance"><i class="fa fa-check"></i><b>3.5.1</b> Launch an EC2 instance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>4</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="exploratory.html"><a href="exploratory.html#what-data-is"><i class="fa fa-check"></i><b>4.1</b> What data is</a></li>
<li class="chapter" data-level="4.2" data-path="exploratory.html"><a href="exploratory.html#data-glimpse"><i class="fa fa-check"></i><b>4.2</b> Data Glimpse</a></li>
<li class="chapter" data-level="4.3" data-path="exploratory.html"><a href="exploratory.html#explorative-analysis"><i class="fa fa-check"></i><b>4.3</b> Explorative Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="exploratory.html"><a href="exploratory.html#semivariogram-covariogram"><i class="fa fa-check"></i><b>4.3.1</b> Semivariogram Covariogram</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inla-spde.html"><a href="inla-spde.html"><i class="fa fa-check"></i><b>5</b> INLA computation</a><ul>
<li class="chapter" data-level="5.1" data-path="inla-spde.html"><a href="inla-spde.html#LGM"><i class="fa fa-check"></i><b>5.1</b> Latent Gaussian Models LGM</a></li>
<li class="chapter" data-level="5.2" data-path="inla-spde.html"><a href="inla-spde.html#approximation-in-inla-setting"><i class="fa fa-check"></i><b>5.2</b> Approximation in INLA setting</a><ul>
<li class="chapter" data-level="5.2.1" data-path="inla-spde.html"><a href="inla-spde.html#further-methods-for-approximations-prolly-do-not-note-include"><i class="fa fa-check"></i><b>5.2.1</b> further methods for approximations (prolly do not note include)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inla-spde.html"><a href="inla-spde.html#r-inla-package-in-a-bayesian-perspective"><i class="fa fa-check"></i><b>5.3</b> R-INLA package in a bayesian perspective</a><ul>
<li class="chapter" data-level="5.3.1" data-path="inla-spde.html"><a href="inla-spde.html#linear-predictor"><i class="fa fa-check"></i><b>5.3.1</b> Linear Predictor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>6</b> Point Referenced Data Modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>6.1</b> Gaussian Process (GP)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="prdm.html"><a href="prdm.html#Matern"><i class="fa fa-check"></i><b>6.1.1</b> Matérn covariance function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="prdm.html"><a href="prdm.html#hedonic-price-models-for-rental-house-market"><i class="fa fa-check"></i><b>6.2</b> Hedonic Price Models for Rental House Market</a></li>
<li class="chapter" data-level="6.3" data-path="prdm.html"><a href="prdm.html#regression-for-univariate-spatial-data"><i class="fa fa-check"></i><b>6.3</b> Regression for univariate spatial data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="prdm.html"><a href="prdm.html#parameter-estimation"><i class="fa fa-check"></i><b>6.3.1</b> Parameter estimation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="prdm.html"><a href="prdm.html#hierreg"><i class="fa fa-check"></i><b>6.4</b> Hierarchical Bayesian Regression</a></li>
<li class="chapter" data-level="6.5" data-path="prdm.html"><a href="prdm.html#inla-as-a-hierarchical-model-va-parafrasato"><i class="fa fa-check"></i><b>6.5</b> INLA as a hierarchical model [va parafrasato]</a></li>
<li class="chapter" data-level="6.6" data-path="prdm.html"><a href="prdm.html#spatial-kriging"><i class="fa fa-check"></i><b>6.6</b> Spatial Kriging</a></li>
<li class="chapter" data-level="6.7" data-path="prdm.html"><a href="prdm.html#model-checking-and-comparison"><i class="fa fa-check"></i><b>6.7</b> Model Checking and Comparison</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="spde.html"><a href="spde.html"><i class="fa fa-check"></i><b>7</b> SPDE approach</a></li>
<li class="chapter" data-level="8" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>8</b> Shiny Web App</a><ul>
<li class="chapter" data-level="8.1" data-path="application.html"><a href="application.html#example-one"><i class="fa fa-check"></i><b>8.1</b> Example one</a></li>
<li class="chapter" data-level="8.2" data-path="application.html"><a href="application.html#example-two"><i class="fa fa-check"></i><b>8.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>9</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA.</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Infrastructure" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Infrastructure</h1>
<!--  You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).-->
<p>In order to provide a fast and easy to use service to the end user many technologies have been involved. Challenges in scraping as pointed out in section <a href="scraping.html#challenges">2.4</a> are many and still some remains unsolved. Challenges regards not only scraping but also the way the service has to respond to the users. Service has to be fast otherwise data become obsolete and so happen to analysis that relied on those data. Service has to be deployed so that in does not only run locally. Service needs to be scaled when needed since when the number of users increases the run time performance should not decrease. Service has to be maintained up to date so that each function can be reshaped with respect to immobiliare.it layout changes. On the other hand code behind the service has to be kept freezed to certain version, so that when packages are updated service still runs. Furthemore service has to be also secured granting access only to the ones authorized. In the end Service has to be run at a certain given times and storing data on cloud, so that it can be tracked back the evolution of the phenomenon.
Open source solutions are available for back-end and front-end to meet the requirements before. Documentations related to technologies served are up to date and offer flexible solutions to embed the R environment. As a general discussion technologies used can be thought as the distance between something running locally on the laptop and something that is actually put into production, seen by company stakeholders, solving business problem. When such technologies are applied data scientist and counterparts gradually close the gap. Insights are better communicated (they can be interactive or automated) and services can be shared over a wider range of subjects. Nonetheless when the infrastructure is made with vision then integrating or substituting existing technologies is not trivial. Anyway technologies can not be always embedded because they might be exclusively designed to work only on certain back ends, therefore some choices are not into discussion. With foresight RStudio by setting future-oriented guidelines has spent a lot of effort giving its users an easy, integrated and interconnected environment. By that it is meant that the RStudio community has tried to either integrate or open the possibility to a number of technologies that fill the blanks in their weaker parts. On top of many, an entire package has been dedicated to democratize REST APIs (Plumber <span class="citation">(Trestle Technology, LLC <a href="#ref-plumber" role="doc-biblioref">2018</a>)</span>). As a further example developers in RStudio have created an entire new paradigm i.e. Shiny <span class="citation">(Chang et al. <a href="#ref-shiny" role="doc-biblioref">2020</a>)</span>, a popular web app development package, that enforces the developer to have front-end and back-end technologies tied up in the same IDE. They also added performance monitoring and optimization packages that are fitted into shiny such as shinytest [metti tag] and shinyloadtest [metti tag] to simulate sessions and verify network traffic congestion.
The actual idea is to provide an API with 4 endpoints which calls parallelized scraping functions. On the other hand (chapter <a href="scraping.html#scraping">2</a>) a daily scheduler, exposing one API endpoint, produces and later stores a .csv file in a NOSQL mongoDB Atlas could database. It is all meant to be containerized in a Linux env (Ubuntu distr) docker container hosted by a AWS EC2 server. API endpoints are going to be secured with https protocols and protected with authentication by nginx reverse proxy. A Shiny app calls an endpoint with specified parameters which returns up to date data from the former infrastructure. It then models data with bayesian spatial methods <a href="prdm.html#prdm">6</a>.</p>
<p>Technologies involved are:</p>
<ul>
<li>GitHub version control</li>
<li>Scheduler cron job, section <a href="Infrastructure.html#scheduler">3.1</a></li>
<li>Docker containers, section <a href="Infrastructure.html#docker">3.2</a></li>
<li>Plumber REST API, section <a href="Infrastructure.html#plumberapi">3.3.1</a></li>
<li>NGINX reverse proxy, section <a href="Infrastructure.html#nginx">3.4</a></li>
<li>AWS (Amazon Web Services) EC2 <a href="Infrastructure.html#aws">3.5</a></li>
<li>MongoDB Atlas</li>
<li>Shiny, see chapter <a href="application.html#application">8</a></li>
</ul>
<div class="figure">
<img src="images/prova.png" alt="" />
<p class="caption">complete infrastructure (Matt Dancho source)</p>
</div>
<p>As a side note even each single part of this thesis has been made stand alone and can be easily accessed and modified through a gitbook deployed at @<a href="https://niccolosalvini.github.io/Thesis/">link</a>. RMarkdown knits the .rmd files extension and coverts them into .html files (the book’s chapters). All the documents are then pushed to a Github repository with git. By a simple trick, since all the files are static html, they can be displayed through GH pages as it is a website whose link is a github subdomain. The pdf output for the thesis can be obtained by clicking the download button, then choosing output pdf in the upper banner. A Latex engine (Xelatex) wrapped into the website compiles the sequence of RMarkdown documents according to a predefined .tex template. Formatting can be tuned by modifying the .yml file, that sets general instructions for the pdf document. All of this has been possible thanks to Bookdown <span class="citation">(Xie <a href="#ref-bookdown1" role="doc-biblioref">2020</a>)</span> once again a R well documented package <span class="citation">(Xie <a href="#ref-bookdown2" role="doc-biblioref">2016</a>)</span> to build interactive books along with RMarkdown <span class="citation">(Allaire et al. <a href="#ref-rmarkdown1" role="doc-biblioref">2020</a>)</span>.</p>
<p>Some of the main technologies implied will be viewed singularly, nonetheless for brevity some of them will be skipped.</p>
<div id="scheduler" class="section level2">
<h2><span class="header-section-number">3.1</span> Scheduler</h2>
<p>A Scheduler in a process is a component on a OS that allows the computer to decide which activity is going to be executed. In the context of multi-programming it is thought as a tool to keep CPU occupied as much as possible. As an example it can trigger a process while some other is still waiting to finish. There are many type of scheduler and they are based on the frequency of times they are executed considering a certain closed time neighbor.</p>
<ul>
<li>Short term scheduler: it can trigger and queue the “ready to go” tasks
<ul>
<li>with pre-emption</li>
<li>without pre-emption</li>
</ul></li>
</ul>
<p>The ST scheduler selects the process and It gains control of the CPU by the dispatcher. In this context we can define latency as the time needed to stop a process and to start a new one.</p>
<ul>
<li>Medium term scheduler</li>
<li>Long term scheduler</li>
</ul>
<p>for some other useful but beyond the scope refereces, such as the scheduling algorithm the reader can refer to <span class="citation">(Wikiversità <a href="#ref-wiki:scheduler" role="doc-biblioref">2020</a>)</span>.</p>
<div id="cron-jobs" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Cron Jobs</h3>
<p>Cron job is a software utility which acts as a time-based job scheduler in Unix-like OS. Linux users that set up and maintain software environments exploit cron to schedule their day-to-day routines to run periodically at fixed times, dates, or intervals. It typically automates system maintenance but its usage is very flexible to whichever needed. It is lightweight and it is widely used since it is a common option for Linux users.
The tasks by cron are driven by a crontab file, which is a configuration file that specifies a set of commands to run periodically on a given schedule. The crontab files are stored where the lists of jobs and other instructions to the cron daemon are kept.</p>
<p>Each line of a crontab file represents a job, and has this structure</p>
<div class="figure">
<img src="images/crontab.PNG" alt="" />
<p class="caption">crontab</p>
</div>
<p>Each line of a crontab file represents a job. This example runs a shell named scheduler.sh at 23:45 (11:45 PM) every Saturday. .sh commands can update mails and other minor routines.</p>
<p>45 23 * * 6 /home/oracle/scripts/scheduler.sh</p>
<p>Some rather unusual scheduling definitions for crontabs can be found in this reference <span class="citation">(Wikipedia contributors <a href="#ref-wiki:cronjob" role="doc-biblioref">2020</a>)</span>. Crontab’s syntax completion can be made easier through <a href="https://crontab.guru/">this</a> GUI.</p>
<p>The cron job needs to be ran on scraping fucntions at 11:30 PM every single day. The get_data.R script first sources an endpoint function, then it applies the function with fixed parameters. Parameters describe the url specification, so that each time the scheduler runs the get_data.R collects data from the same source. Day after day .json files are generated and then stored into a NOSQL <em>mongoDB</em> database whose credentials are public. Data are collected on a daily basis with the explicit aim to track day-by-day changes both in the new entries an goners in rental market, and to investigate the evolution of price differentials over time. Spatio-Temporal modeling is still quite unexplored, data is saved for future used. Crontab configuration for daily 11:30 PM schedules has this appearance:</p>
<p>30 11 * * * /home/oracle/scripts/get_data.R</p>
<p>Since now the computational power comes from the machine on which the system is installed. A smarter solution takes care of it by considering run time limits and the substantial inability to share data. To a certain extent what it has been already done since now might fit for personal use: a scheduler can daily execute the scraping scripts and generate a .csv file. Furthermore an application can rely on those data, but evident reasons suggest that it does not suite any need. What it will do the trick would be an open source dedicated software environment or <em>container</em> that will contains scraping functions and a scheduler on cloud solving a pair of the problems arisen. This problem can be addressed with a technology that has seen a huge growth in its usage in the last few years.</p>
</div>
</div>
<div id="docker" class="section level2">
<h2><span class="header-section-number">3.2</span> Docker</h2>
<div id="what-is-docker" class="section level3">
<h3><span class="header-section-number">3.2.1</span> What is Docker</h3>
<p><em>Docker</em> is a software tool to create and deploy applications using containers.
<em>Docker containers</em> are a standard unit of software (i.e. software boxes) where everything needed for applications, such as libraries or dependencies can be run reliably and quickly. Furthermore they are also portable, in the sense that they can be taken from one computing environment to the following. Docker containers by default run on kernel Linux OS.
Containers can be thought as an abstraction at the app layers that groups code and dependencies together. One major advantage of containers is that multiple containers can run on the same machine with the same OS. Each container can run its own isolated process in the user space, so that each task is complementary to the other. Containers are lightweight and take up less space than Virtual Machines (container images are files which can take up typically tens of MBs in size), can handle more applications and require fewer Virtual Machines and OSs.</p>
<div class="figure">
<img src="images/dockerVSvirtualmachines.PNG" alt="" />
<p class="caption">docker container vs VM</p>
</div>
<p>When containers are built <em>Docker container Images</em> are created and can be open sourced with Docker Hub
<em>Docker Hub</em> is a web service provided by Docker for searching and sharing container images with other teams or developers in the community. Docker Hub behind authentication allows to integrate GitHub in the Docker project repository. Once the connection is authorized on local machine changes are made and then pushed by version control to the remote GH repository. The push command triggers the automatic building (pre-set by the user, branch should be given) of the image in the docker hub repository. The just created docker image can be tagged so that firstly it is recognizable and secondly can be reused in the future. Once the building stage is completed the DH repository can be pulled and then run locally on machine or cloud, see section <a href="Infrastructure.html#aws">3.5</a>.
Docker building and testing images can be very time consumin. R packages can take a long time to install because code has to be compiled, especially if using R on a Linux server or in a Docker container.
Rstudio <a href="https://packagemanager.rstudio.com/client/#/">package manager</a> includes beta support for pre-compiled R packages that can be installed faster. This dramatically reduces packages time installation <span class="citation">(Nolis <a href="#ref-nolis_2020" role="doc-biblioref">2020</a>)</span>.
In addition an open source project named <a href="https://www.rocker-project.org/images/">rocker</a> have already narrowed the path building custom R images for R and docker users. What can be read from their own website about them: “The rocker project provides a collection of containers suited for different needs. find a base image to extend or images with popular software and optimized libraries pre-installed. Get the latest version or a reproducible fixed environment.”
Enabling caching when container images are built heavily shorten the total duration for containerization.</p>
</div>
<div id="why-docker" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Why Docker</h3>
<p>Indeed, an employment-related search engine, released an article on 2019 displaying changing trends from 2015 to 2019 in Technology Job market. Many changes are relevant in key technologies. Two among the others technologies (i.e. docker and Azure) have experienced a huge growth and both refer to the same demand input: <em>containers</em> .
The landscape of Data Science is changing <span class="citation">(Economist at the Indeed Hiring Lab <a href="#ref-Skills_Explorer" role="doc-biblioref">2020</a>)</span> from reporting to application building:
In 2015 - Businesses reports drive better decisions
In 2020 - Businesses need apps to empower better decision making at all levels</p>
<div class="figure">
<img src="images/Inkedindeed_jobs_LI.jpg" alt="" />
<p class="caption">docker-stats</p>
</div>
<p>For all the things said what docker is bringing to business <span class="citation">(R. H. Inc. <a href="#ref-red_hat_customer_portal" role="doc-biblioref">2020</a>)</span>:</p>
<ul>
<li><em>Speed application deployment</em> : containers include the minimal run time requirements of the application, reducing their size and allowing them to be deployed quickly.</li>
<li><em>Portability across machines</em> : an application and all its dependencies can be bundled into a single container that is independent from the host version of Linux kernel, platform distribution, or deployment model. This container can be transfered to another machine that runs Docker, and executed there without compatibility issues.</li>
<li><em>Version control and component reuse</em> : you can track successive versions of a container, inspect differences, or roll-back to previous versions. Containers reuse components from the preceding layers, which makes them noticeably lightweight. In addition due to Docker Hub it is possible to establish a connection between Git and DockerHub. Vesion</li>
<li><em>Sharing</em> : you can use a remote repository to share your container with others. It is also possible to configure a private repository hosted on Docker Hub.</li>
<li><em>Lightweight footprint and minimal overhead</em> : Docker images are typically very small, which facilitates rapid delivery and reduces the time to deploy new application containers.</li>
<li><em>Fault isolation</em> :Docker reduces effort and risk of problems with application dependencies. Docker also freezes the environment to the preferred packages version so that it guarantees continuity in deployment and isolate the container from system fails coming from package version updates.</li>
</ul>
<p>The way to tell docker which system requirements are needed in the newly born software is a <em>Dockerfile</em>.</p>
</div>
<div id="dockerfile" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Dockerfile</h3>
<p>Docker can build images automatically by reading instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands/rules a generic user could call on the CLI to assemble an image. Executing the command <code>docker build</code> from shell the user can trigger the image building. That executes sequentially several command-line instructions. For thesis purposes a dockerfile is written with the specific instructions and then the file is pushed to GitHub repository. Once pushed DockerHub automatically parses the repository looking for a plain text file whose name is “Dockerfile”. When It is matched then it trriggers the building of the image.</p>
<p>The Dockerfile used to trigger the building of the service docker container has the following set of instructions:</p>
<div class="figure">
<img src="images/dockerfile.PNG" alt="" />
<p class="caption">dockerfile</p>
</div>
<ul>
<li><p><code>FROM rocker/tidyverse:latest</code> : the command imports a pre-built image by the rocker team that contains the latest (tag latest) version of base-R along with the tidyverse packages.</p></li>
<li><p><code>MAINTAINER Niccolo Salvini "niccolo.salvini27@gmail.com"</code> : The command tags the maintainer and its e-mail contact information.</p></li>
<li><p><code>RUN apt-get update &amp;&amp; apt-get install -y \ libxml2-dev \ libudunits2-dev</code> :The command update and install Linux dependencies needed for running R packages. <code>rvest</code> requires libxml2-dev and <code>magrittr</code> needs libudunits2-dev. If they are not installed then associated libraries can not be loaded. Linux dependencies needed have been found by trial and error while building containers. Building logs messages print errors and suggest which dependency is mandatory.</p></li>
<li><p><code>RUN R -e "install.packages(c('plumber','tibble','...',dependencies=TRUE)</code> : the command install all the packages required to execute the files (R files) containerized for the scraping. Since all the packages have their direct R dependencies the option <code>dependencies=TRUE</code> is needed.</p></li>
<li><p><code>RUN R -e "install.packages('https://cran.r-project.org/.../iterators, type='source')</code>
<code>RUN R -e "install.packages('https://cran.r-project.org/.../foreach/, type='source')</code>
<code>RUN R -e "install.packages('https://cran.r-project.org/.../doParallel, type='source')</code>
DoParallel was not available in package manager for R version later than 4.0.0. For this reason the choice was to install a previous source version by the online repository, as well as its dependencies.</p></li>
<li><p><code>COPY \\</code> The command tells Docker copies all the files in the container.</p></li>
<li><p><code>EXPOSE 8000</code> : the commands instructs Docker that the container listens on the specified network ports 8000 at runtime. It is possible to specify whether the port exposed listens on UDP or TCP, the default is TCP (this part needs a previous set up previous installing, for further online documentation It is recommended <span class="citation">(D. Inc. <a href="#ref-docker_documentation_2020" role="doc-biblioref">2020</a>)</span> )</p></li>
<li><p><code>ENTRYPOINT ["Rscript", "main.R"]</code> : the command tells docker to execute the file main.R within the container that triggers the API start. In main.R it are pecified both the port and the host where API expects to be exposed (in this case port 8000).</p></li>
</ul>
<p>In order to make the system stand-alone and make the service available to a wider range of subjects a choice has to be made. The service has to have both the characteristics to be run on demand and to specify query parameters.</p>
</div>
</div>
<div id="api" class="section level2">
<h2><span class="header-section-number">3.3</span> API</h2>
<p>API stands for application programming interface and it is a set of definitions and protocols for building and integrating application software. APIs let a product or a service communicate with other products and services without having to know how they’re implemented. This can simplify app development, saving time and impacting positively on the budget.
APIs are sometimes thought of as contracts, with documentation that represents an agreement between parties.
There are many types of API that exploit different medium to communicate with apps or services. HTTP API are a special type of API that accepts http requests as input and elaborate them through end points. An end point identifies the operation through traditional http methods (e.g. /GET /POST) that the API caller wants to perform.
HTTP APIs have now become the predominant medium by which software exchanges information, further documentation and differences between REST and RESTful API can be found <a href="https://docs.aws.amazon.com/it_it/apigateway/latest/developerguide/http-api-vs-rest.html">here</a>.</p>
<p>API examples:
- Google Maps API: allows developers to embed geo-location data using JavaScript. The Google Maps API is designed to work on mobile and desktop.
- YouTube API: allows developers integrate YouTube videos and functionalities into websites or applications.
- Google Analytics API: allows to track website performance in terms of audience, monetization and other important metrics through the Google Analytics interface. The website the thesis come from has this implementation working.</p>
<p>This is obtained by embedding the existing R source code into the Plumber API framework.</p>
<div class="figure">
<img src="images/Rest-API.png" alt="" />
<p class="caption">API functioning</p>
</div>
<div id="plumberapi" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Plumber API</h3>
<p>Plumber <span class="citation">(“An Api Generator for R” <a href="#ref-api_generator_for_r" role="doc-biblioref">2020</a>)</span> allows the user to create a web API by simply adding decoration comments to the existing R source code. Decorations are a special type of comments that suggests to plumber where and when the API parts are. Below the original API source code.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="Infrastructure.html#cb14-1"></a><span class="co"># plumber.R</span></span>
<span id="cb14-2"><a href="Infrastructure.html#cb14-2"></a></span>
<span id="cb14-3"><a href="Infrastructure.html#cb14-3"></a><span class="co"># * Echo back the input * @param msg The message to echo * @get /echo</span></span>
<span id="cb14-4"><a href="Infrastructure.html#cb14-4"></a><span class="cf">function</span>(<span class="dt">msg =</span> <span class="st">&quot;&quot;</span>) {</span>
<span id="cb14-5"><a href="Infrastructure.html#cb14-5"></a>    <span class="kw">list</span>(<span class="dt">msg =</span> <span class="kw">paste0</span>(<span class="st">&quot;The message is: &#39;&quot;</span>, msg, <span class="st">&quot;&#39;&quot;</span>))</span>
<span id="cb14-6"><a href="Infrastructure.html#cb14-6"></a>}</span>
<span id="cb14-7"><a href="Infrastructure.html#cb14-7"></a></span>
<span id="cb14-8"><a href="Infrastructure.html#cb14-8"></a><span class="co"># * Plot a histogram * @serializer png * @get /plot</span></span>
<span id="cb14-9"><a href="Infrastructure.html#cb14-9"></a><span class="cf">function</span>() {</span>
<span id="cb14-10"><a href="Infrastructure.html#cb14-10"></a>    rand &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb14-11"><a href="Infrastructure.html#cb14-11"></a>    <span class="kw">hist</span>(rand)</span>
<span id="cb14-12"><a href="Infrastructure.html#cb14-12"></a>}</span>
<span id="cb14-13"><a href="Infrastructure.html#cb14-13"></a></span>
<span id="cb14-14"><a href="Infrastructure.html#cb14-14"></a><span class="co"># * Return the sum of two numbers * @param a The first number to add * @param b</span></span>
<span id="cb14-15"><a href="Infrastructure.html#cb14-15"></a><span class="co"># The second number to add * @post /sum</span></span>
<span id="cb14-16"><a href="Infrastructure.html#cb14-16"></a><span class="cf">function</span>(a, b) {</span>
<span id="cb14-17"><a href="Infrastructure.html#cb14-17"></a>    <span class="kw">as.numeric</span>(a) <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(b)</span>
<span id="cb14-18"><a href="Infrastructure.html#cb14-18"></a>}</span></code></pre></div>
<p>three endpoints associated to 2 /GET and 1 /POST requests are made available. Functions are made clear without names so that whenever the endpoint is called functions are directly executed.
Decorations are marked as this <code>#*</code> and they are followed by specific keywords denoted with <code>@</code>.
- the <code>@params</code> keyword refers to parameter that specifies the corpus of the HTTP request, i.e. the inputs with respect to the expected output. If default parameters are inputted then the API response is the elaboration of the functions with default parameters. As opposite endpoint function elaborates the provided parameters and returns a response.
- <code>#* @serializer</code> specifies the extension of the output file when needed.
- <code>#* @get</code> specifies the method of HTTP request sent.
- <code>/echo</code> is the end point name.
- <code>@filter</code> decorations activates a filter layer which are used to track logs and to parse request before passing the argbody to the end points.
- many more…</p>
</div>
<div id="immobiliare.it-http-api" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Immobiliare.it HTTP API</h3>
<p>The API service is composed by three endpoints <em>/scrape</em> , <em>/links</em> and <em>/complete</em>:</p>
<ul>
<li><p>*/scrape performs a fast scraping that extracts 5 covariates directly from filtered url. url from which data extraction takes place might be composed through parameters. By default the end point scrape data from Milan real estate rental market. Fast scraping is reached thanks to avoiding to access to single links. It is a superficial scraping and does not contain geospatial, however it might fit for regression settings.</p></li>
<li><p>*/links: extracts the list of single links belonging to each of the page, looking at section <a href="scraping.html#webstructure">2.1.1</a> each 25 single links for each sibling. It displays sufficient performances in terms of run time. It is propaedeutic to apply the following endpoint.</p></li>
<li><p>*/complete: both the function all.links and complete are sourced. The former with the aim to grab each single links and store it into an object. The latter to actually iterate scraping on each of the links.</p></li>
</ul>
<div class="figure">
<img src="images/swagger.PNG" alt="" />
<p class="caption">swagger</p>
</div>
</div>
<div id="api-source-code" class="section level3">
<h3><span class="header-section-number">3.3.3</span> API source code</h3>
<ul>
<li>Get FAST data, it covers 5 covariates: title, price, num of rooms, sqmeter, primarykey</li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="Infrastructure.html#cb15-1"></a><span class="co"># * Get all the links * @param city [chr string] the city you are interested to</span></span>
<span id="cb15-2"><a href="Infrastructure.html#cb15-2"></a><span class="co"># extract data (lowercase without accent) * @param npages [positive integer]</span></span>
<span id="cb15-3"><a href="Infrastructure.html#cb15-3"></a><span class="co"># number of pages to scrape default = 10, min = 2, max = 300 * @param type [chr</span></span>
<span id="cb15-4"><a href="Infrastructure.html#cb15-4"></a><span class="co"># string] affitto = rents, vendita = sell (vendita no available for now) * @get</span></span>
<span id="cb15-5"><a href="Infrastructure.html#cb15-5"></a><span class="co"># /links/&lt;npages:int&gt;/&lt;city:chr&gt;/&lt;type:chr&gt;/&lt;.thesis:bool&gt;</span></span>
<span id="cb15-6"><a href="Infrastructure.html#cb15-6"></a><span class="cf">function</span>(<span class="dt">npages =</span> <span class="dv">10</span>, <span class="dt">city =</span> <span class="st">&quot;milano&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;affitto&quot;</span>, <span class="dt">.thesis =</span> F, req) {</span>
<span id="cb15-7"><a href="Infrastructure.html#cb15-7"></a>    <span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n\n</span><span class="st"> port:&quot;</span>, req<span class="op">$</span>SERVER_PORT, <span class="st">&quot;</span><span class="ch">\n</span><span class="st"> server_name:&quot;</span>, req<span class="op">$</span>SERVER_NAME, <span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">&quot;</span>)</span>
<span id="cb15-8"><a href="Infrastructure.html#cb15-8"></a>    <span class="cf">if</span> (npages <span class="op">&gt;</span><span class="st"> </span><span class="dv">300</span> <span class="op">&amp;</span><span class="st"> </span>npages <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb15-9"><a href="Infrastructure.html#cb15-9"></a>        <span class="kw">stop</span>(<span class="st">&quot;npages must be between 1 and 1,000&quot;</span>)</span>
<span id="cb15-10"><a href="Infrastructure.html#cb15-10"></a>    }</span>
<span id="cb15-11"><a href="Infrastructure.html#cb15-11"></a>    <span class="cf">if</span> (.thesis) {</span>
<span id="cb15-12"><a href="Infrastructure.html#cb15-12"></a>        <span class="kw">list</span>(<span class="kw">all.links</span>(npages, city, type, <span class="dt">.thesis =</span> <span class="ot">TRUE</span>))</span>
<span id="cb15-13"><a href="Infrastructure.html#cb15-13"></a>    } <span class="cf">else</span> {</span>
<span id="cb15-14"><a href="Infrastructure.html#cb15-14"></a>        <span class="kw">list</span>(<span class="kw">all.links</span>(npages, city, type))</span>
<span id="cb15-15"><a href="Infrastructure.html#cb15-15"></a>    }</span>
<span id="cb15-16"><a href="Infrastructure.html#cb15-16"></a>}</span>
<span id="cb15-17"><a href="Infrastructure.html#cb15-17"></a></span>
<span id="cb15-18"><a href="Infrastructure.html#cb15-18"></a></span>
<span id="cb15-19"><a href="Infrastructure.html#cb15-19"></a><span class="co"># * Get the complete data from single links (not the raw) * @param city [chr</span></span>
<span id="cb15-20"><a href="Infrastructure.html#cb15-20"></a><span class="co"># string] the city you are interested to extract data (lowercase without accent)</span></span>
<span id="cb15-21"><a href="Infrastructure.html#cb15-21"></a><span class="co"># * @param npages [positive integer] number of pages to scrape default = 10, min</span></span>
<span id="cb15-22"><a href="Infrastructure.html#cb15-22"></a><span class="co"># = 2, max = 300 * @param type [chr string] affitto = rents, vendita = sell</span></span>
<span id="cb15-23"><a href="Infrastructure.html#cb15-23"></a><span class="co"># (vendita no available for now) * @get</span></span>
<span id="cb15-24"><a href="Infrastructure.html#cb15-24"></a><span class="co"># /complete/&lt;npages:int&gt;/&lt;city:chr&gt;/&lt;type:chr&gt;/&lt;.thesis:bool&gt;</span></span>
<span id="cb15-25"><a href="Infrastructure.html#cb15-25"></a><span class="cf">function</span>(<span class="dt">npages =</span> <span class="dv">10</span>, <span class="dt">city =</span> <span class="st">&quot;milano&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;affitto&quot;</span>, <span class="dt">.thesis =</span> F, req) {</span>
<span id="cb15-26"><a href="Infrastructure.html#cb15-26"></a>    <span class="cf">if</span> (npages <span class="op">&gt;</span><span class="st"> </span><span class="dv">300</span> <span class="op">&amp;</span><span class="st"> </span>npages <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb15-27"><a href="Infrastructure.html#cb15-27"></a>        <span class="kw">stop</span>(<span class="st">&quot;npages must be between 1 and 1,000&quot;</span>)</span>
<span id="cb15-28"><a href="Infrastructure.html#cb15-28"></a>    }</span>
<span id="cb15-29"><a href="Infrastructure.html#cb15-29"></a>    <span class="cf">if</span> (.thesis) {</span>
<span id="cb15-30"><a href="Infrastructure.html#cb15-30"></a>        links =<span class="st"> </span><span class="kw">all.links</span>(npages, city, type, <span class="dt">.thesis =</span> <span class="ot">TRUE</span>)</span>
<span id="cb15-31"><a href="Infrastructure.html#cb15-31"></a>        <span class="kw">list</span>(<span class="kw">complete</span>(links))</span>
<span id="cb15-32"><a href="Infrastructure.html#cb15-32"></a>    } <span class="cf">else</span> {</span>
<span id="cb15-33"><a href="Infrastructure.html#cb15-33"></a>        links =<span class="st"> </span><span class="kw">all.links</span>(npages, city, type)</span>
<span id="cb15-34"><a href="Infrastructure.html#cb15-34"></a>        <span class="kw">list</span>(<span class="kw">complete</span>(links))</span>
<span id="cb15-35"><a href="Infrastructure.html#cb15-35"></a>    }</span>
<span id="cb15-36"><a href="Infrastructure.html#cb15-36"></a>}</span></code></pre></div>
</div>
<div id="api-documentation" class="section level3">
<h3><span class="header-section-number">3.3.4</span> API documentation</h3>
<ul>
<li>Get FAST data, it covers 5 covariates: title, price, num of rooms, sqmeter, primarykey</li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="Infrastructure.html#cb16-1"></a>      GET <span class="op">*</span><span class="er">/</span>scrape</span>
<span id="cb16-2"><a href="Infrastructure.html#cb16-2"></a>      <span class="op">@</span>param city [chr string] the city you are interested <span class="cf">in</span> (e.g. <span class="st">&quot;roma&quot;</span>, <span class="st">&quot;milano&quot;</span>, <span class="st">&quot;firenze&quot;</span><span class="op">-</span>-&gt;<span class="st"> </span>lowercase, without accent)</span>
<span id="cb16-3"><a href="Infrastructure.html#cb16-3"></a>      <span class="op">@</span>param npages [positive integer] number of pages to scrape, default =<span class="st"> </span><span class="dv">10</span>, min  =<span class="st"> </span><span class="dv">2</span>, max =<span class="st"> </span><span class="dv">300</span></span>
<span id="cb16-4"><a href="Infrastructure.html#cb16-4"></a>      <span class="op">@</span>param type [chr string] <span class="st">&quot;affitto&quot;</span> =<span class="st"> </span>rents, <span class="st">&quot;vendita&quot;</span>  =<span class="st"> </span>sell </span>
<span id="cb16-5"><a href="Infrastructure.html#cb16-5"></a>      <span class="op">@</span>param macrozone [chr string] avail<span class="op">:</span><span class="st"> </span>Roma, Firenze, Milano, Torino; e.g. <span class="st">&quot;fiera&quot;</span>, <span class="st">&quot;centro&quot;</span>, <span class="st">&quot;bellariva&quot;</span>, <span class="st">&quot;parioli&quot;</span> </span>
<span id="cb16-6"><a href="Infrastructure.html#cb16-6"></a>      content<span class="op">-</span>type<span class="op">:</span><span class="st"> </span>application<span class="op">/</span>json </span></code></pre></div>
<ul>
<li>Get all the links</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="Infrastructure.html#cb17-1"></a>      GET <span class="op">*</span><span class="er">/</span>link</span>
<span id="cb17-2"><a href="Infrastructure.html#cb17-2"></a>      <span class="op">@</span>param city [chr string] the city you are interested to extract <span class="kw">data</span> (lowercase without accent)</span>
<span id="cb17-3"><a href="Infrastructure.html#cb17-3"></a>      <span class="op">@</span>param npages [positive integer] number of pages to scrape default =<span class="st"> </span><span class="dv">10</span>, min  =<span class="st"> </span><span class="dv">2</span>, max =<span class="st"> </span><span class="dv">300</span></span>
<span id="cb17-4"><a href="Infrastructure.html#cb17-4"></a>      <span class="op">@</span>param type [chr string] <span class="st">&quot;affitto&quot;</span> =<span class="st"> </span>rents, <span class="st">&quot;vendita&quot;</span>  =<span class="st"> </span>sell </span>
<span id="cb17-5"><a href="Infrastructure.html#cb17-5"></a>      <span class="op">@</span>param .thesis [logical] data used <span class="cf">for</span> master thesis</span>
<span id="cb17-6"><a href="Infrastructure.html#cb17-6"></a>      content<span class="op">-</span>type<span class="op">:</span><span class="st"> </span>application<span class="op">/</span>json </span></code></pre></div>
<ul>
<li>Get the complete set of covariates (52) from each single links, takes a while</li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="Infrastructure.html#cb18-1"></a>      GET <span class="op">*</span><span class="er">/</span>complete</span>
<span id="cb18-2"><a href="Infrastructure.html#cb18-2"></a>      <span class="op">@</span>param city [chr string] the city you are interested to extract <span class="kw">data</span> (lowercase without accent)</span>
<span id="cb18-3"><a href="Infrastructure.html#cb18-3"></a>      <span class="op">@</span>param npages [positive integer] number of pages to scrape default =<span class="st"> </span><span class="dv">10</span>, min  =<span class="st"> </span><span class="dv">2</span>, max =<span class="st"> </span><span class="dv">300</span></span>
<span id="cb18-4"><a href="Infrastructure.html#cb18-4"></a>      <span class="op">@</span>param type [chr string] <span class="st">&quot;affitto&quot;</span> =<span class="st"> </span>rents, <span class="st">&quot;vendita&quot;</span>  =<span class="st"> </span>sell </span>
<span id="cb18-5"><a href="Infrastructure.html#cb18-5"></a>      <span class="op">@</span>param .thesis [logical] data used <span class="cf">for</span> master thesis</span>
<span id="cb18-6"><a href="Infrastructure.html#cb18-6"></a>      content<span class="op">-</span>type<span class="op">:</span><span class="st"> </span>application<span class="op">/</span>json</span>
<span id="cb18-7"><a href="Infrastructure.html#cb18-7"></a>            </span></code></pre></div>
</div>
</div>
<div id="nginx" class="section level2">
<h2><span class="header-section-number">3.4</span> NGINX reverse proxy server</h2>
<p>For analysis purposes NGINX is open source software for reverse proxying and load balancing.
Proxying is typically used to distribute the load among several servers, seamlessly show content from different websites, or pass requests for processing to application servers over protocols other than HTTP.
[…]</p>
<p>When NGINX proxies a request, it sends the request to a specified proxied server, fetches the response, and sends it back to the client. It is possible to proxy requests to an HTTP server (another NGINX server or any other server) or a non-HTTP server (which can run an application developed with a specific framework, such as PHP or Python) using a specified protocol. Supported protocols include FastCGI, uwsgi, SCGI, and memcached.
[…]</p>
<p>.conf file and installation on Linux server. Security and Authentication.</p>
</div>
<div id="aws" class="section level2">
<h2><span class="header-section-number">3.5</span> AWS EC2 server</h2>
<p>Executing API on a public server allows to share data with a multitude of subjects. Since it can not be specified a-priori how many users are going to enjoy the service a scalable solution fills the needs. Scalable infrastructure through a flexible cloud provider combined with nginx load balancing can offer a stable and reliable infrastructure for relatively a cheap price.
AWS offers a wide range of services each of which for a wide range of budgets and integration. Free tier servers can be rent up to a certain amount of storage and computation that nearly 0s the total bill. The cloud provider also has a dedicated webpage to configure the service needed with respect to the usage named <a href="https://aws.amazon.com/en/aws-cost-management/">amazon cost manager</a>.
Amazon Elastic Compute Cloud (EC2) is a web service that contributes to a secure, flexible computing capacity in the AWS cloud. EC2 allows to rent as many virtual servers as needed with customized capacity, security and storage.
[few words still on EC2]</p>
<div id="launch-an-ec2-instance" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Launch an EC2 instance</h3>
<p>The preliminary step is to pick up an AMI (Amazon Machine Image). AWS AMI are already-set-up machines with stadardized specification designed to speed up the process of choosing the a customed machine. Since the project is planned to be nearly 0-cost a “Free Tier Eligible” server is chosen. By checking the Free Tier box all the available free tiers are displayed. The machine selected has this specification: t2.micro with 1 CPU and 1GB RAM and runs on a Ubuntu distribution OS. First set up settings needs to be left as-is, networking and VPC can always be updated when needed. In the “add storage” step 30 GB storage are selected, moreover 30 represent the upper limit since the server can be considered free tier. Tags windows are beyond the scope. Secondly configuration needs to account security and a new entry below SSH connection (port 22) has to be set in. New security configuration has to have TCP specification and should be associated to port 8000. Port 8000, as in dockerfile section <a href="Infrastructure.html#dockerfile">3.2.3</a>, has been exposed and needs to be linked to the security port opened.</p>
<div class="figure">
<img src="images/aws.PNG" alt="" />
<p class="caption">aws_dashboard</p>
</div>
<p>At this point instance is prepared to run and in a few minutes is deployed. Key pairs, if never done before, are generated and .pem file is saved and securely stored. Key pairs are mandatory to access to the Ubuntu server via SSH. SSH connection in Windows OS can be handled with <a href="https://www.putty.org/">PuTTY</a>, which is a SSH and telnet client designed for Windows. At first PuTTYgen has to convert the key pair .pem file into a .ppk extension (otherwise Putty can not read it). Once converted .ppk is sourced in the authorization panel. If everything works and authentication is verified then the Ubuntu server CLI appears and an interaction with the server is made available.</p>
<p>posso far vedere:
- come si fa deplyment su AWS quindi
- inizializzazione istanza
- settare paramtri
- mettere spazio macchina
- spiegare perchè conviene tenere un server AWS</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-rmarkdown1">
<p>Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2020. <em>Rmarkdown: Dynamic Documents for R</em>. <a href="https://github.com/rstudio/rmarkdown">https://github.com/rstudio/rmarkdown</a>.</p>
</div>
<div id="ref-api_generator_for_r">
<p>“An Api Generator for R.” 2020. <em>An API Generator for R</em>. <a href="https://www.rplumber.io/">https://www.rplumber.io/</a>.</p>
</div>
<div id="ref-shiny">
<p>Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2020. <em>Shiny: Web Application Framework for R</em>. <a href="https://CRAN.R-project.org/package=shiny">https://CRAN.R-project.org/package=shiny</a>.</p>
</div>
<div id="ref-Skills_Explorer">
<p>Economist at the Indeed Hiring Lab, Andrew FlowersAndrew Flowers was previously an. 2020. “Indeed Tech Skills Explorer: Today’s Top Tech Skills.” <a href="https://www.hiringlab.org/2019/11/19/todays-top-tech-skills/">https://www.hiringlab.org/2019/11/19/todays-top-tech-skills/</a>.</p>
</div>
<div id="ref-docker_documentation_2020">
<p>Inc., Docker. 2020. “Get Docker.” <em>Docker Documentation</em>. <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</p>
</div>
<div id="ref-red_hat_customer_portal">
<p>Inc., Red Hat. 2020. “7.2. Advantages of Using Docker Red Hat Enterprise Linux 7.” <em>Red Hat Customer Portal</em>. <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/7.0_release_notes/sect-red_hat_enterprise_linux-7.0_release_notes-linux_containers_with_docker_format-advantages_of_using_docker">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/7.0_release_notes/sect-red_hat_enterprise_linux-7.0_release_notes-linux_containers_with_docker_format-advantages_of_using_docker</a>.</p>
</div>
<div id="ref-nolis_2020">
<p>Nolis, Jacqueline. 2020. “R Docker Faster.” <em>Medium</em>. Medium. <a href="https://medium.com/@skyetetra/r-docker-faster-28e13a6d241d">https://medium.com/@skyetetra/r-docker-faster-28e13a6d241d</a>.</p>
</div>
<div id="ref-plumber">
<p>Trestle Technology, LLC. 2018. <em>Plumber: An Api Generator for R</em>. <a href="https://CRAN.R-project.org/package=plumber">https://CRAN.R-project.org/package=plumber</a>.</p>
</div>
<div id="ref-wiki:cronjob">
<p>Wikipedia contributors. 2020. “Cron — Wikipedia, the Free Encyclopedia.” <a href="https://en.wikipedia.org/w/index.php?title=Cron&amp;oldid=978051592">https://en.wikipedia.org/w/index.php?title=Cron&amp;oldid=978051592</a>.</p>
</div>
<div id="ref-wiki:scheduler">
<p>Wikiversità. 2020. “Scheduling — Wikiversità,” <a href="https://it.wikiversity.org/w/index.php?title=Scheduling&amp;oldid=214572">https://it.wikiversity.org/w/index.php?title=Scheduling&amp;oldid=214572</a>.</p>
</div>
<div id="ref-bookdown2">
<p>Xie, Yihui. 2016. <em>Bookdown: Authoring Books and Technical Documents with R Markdown</em>. Boca Raton, Florida: Chapman; Hall/CRC. <a href="https://github.com/rstudio/bookdown">https://github.com/rstudio/bookdown</a>.</p>
</div>
<div id="ref-bookdown1">
<p>Xie, Yihui. 2020. <em>Bookdown: Authoring Books and Technical Documents with R Markdown</em>. <a href="https://github.com/rstudio/bookdown">https://github.com/rstudio/bookdown</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="scraping.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exploratory.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/Thesis/edit/master/03-infrastructure.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf", "Niccolo_Salvini_Thesis.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
