<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 API Technology Stack | REST Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</title>
  <meta name="description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 API Technology Stack | REST Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />
  <meta property="og:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 API Technology Stack | REST Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  
  <meta name="twitter:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/logo/spatial_logo.png" />
  <link rel="shortcut icon" href="images/logo/spatial_logo.ico" type="image/x-icon" />
<link rel="prev" href="scraping.html"/>
<link rel="next" href="inla.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/Proj4Leaflet-1.0.1/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.3/leaflet.js"></script>
<link href="libs/leaflet-minimap-3.3.1/Control.MiniMap.min.css" rel="stylesheet" />
<script src="libs/leaflet-minimap-3.3.1/Control.MiniMap.min.js"></script>
<script src="libs/leaflet-minimap-3.3.1/Minimap-binding.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/logo/spatial_logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Web Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#a-gentle-introduction-on-web-scraping"><i class="fa fa-check"></i><b>2.1</b> A Gentle Introduction on Web Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#inverse-url-compostion"><i class="fa fa-check"></i><b>2.2</b> Inverse url compostion</a></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#graph-representation-of-html"><i class="fa fa-check"></i><b>2.3</b> Graph Representation of HTML</a></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#crawling"><i class="fa fa-check"></i><b>2.4</b> Crawling</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#proper-scraping"><i class="fa fa-check"></i><b>2.5</b> Proper Scraping</a><ul>
<li class="chapter" data-level="2.5.1" data-path="scraping.html"><a href="scraping.html#webstructure"><i class="fa fa-check"></i><b>2.5.1</b> Immobiliare.it website structure</a></li>
<li class="chapter" data-level="2.5.2" data-path="scraping.html"><a href="scraping.html#immobiliare.it-content-architecture-with-rvest"><i class="fa fa-check"></i><b>2.5.2</b> Immobiliare.it content architecture with <code id="ContentArchitecture">rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#ProperScraping"><i class="fa fa-check"></i><b>2.6</b> Proper Scarping</a></li>
<li class="chapter" data-level="2.7" data-path="scraping.html"><a href="scraping.html#best-practices"><i class="fa fa-check"></i><b>2.7</b> Scraping Best Practices and Security provisions</a></li>
<li class="chapter" data-level="2.8" data-path="scraping.html"><a href="scraping.html#web-client-security-provisions-user-agents-proxies-and-fail-dealers"><i class="fa fa-check"></i><b>2.8</b> Web Client Security provisions: User Agents, Proxies and Fail Dealers</a><ul>
<li class="chapter" data-level="2.8.1" data-path="scraping.html"><a href="scraping.html#spoofing"><i class="fa fa-check"></i><b>2.8.1</b> HTTP User Agent and Mail Spoofing</a></li>
<li class="chapter" data-level="2.8.2" data-path="scraping.html"><a href="scraping.html#possibly"><i class="fa fa-check"></i><b>2.8.2</b> Dealing with failure</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="scraping.html"><a href="scraping.html#parallelscraping"><i class="fa fa-check"></i><b>2.9</b> Parallel Scraping</a><ul>
<li class="chapter" data-level="2.9.1" data-path="scraping.html"><a href="scraping.html#parallel-furrrfuture"><i class="fa fa-check"></i><b>2.9.1</b> Parallel furrr+future</a></li>
<li class="chapter" data-level="2.9.2" data-path="scraping.html"><a href="scraping.html#parallel-foreachdofuture"><i class="fa fa-check"></i><b>2.9.2</b> Parallel foreach+doFuture</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="scraping.html"><a href="scraping.html#challenges"><i class="fa fa-check"></i><b>2.10</b> Open Challenges and Further Improvemements</a></li>
<li class="chapter" data-level="2.11" data-path="scraping.html"><a href="scraping.html#legal-profiles"><i class="fa fa-check"></i><b>2.11</b> Legal Profiles</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> API Technology Stack</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#rest-api"><i class="fa fa-check"></i><b>3.2</b> REST API</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>3.2.1</b> Plumber REST API</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#immobiliare.it-parallel-rest-api"><i class="fa fa-check"></i><b>3.2.2</b> Immobiliare.it <em>Parallel</em> REST API</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#APIdocs"><i class="fa fa-check"></i><b>3.2.3</b> REST API documentation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>3.3</b> Docker</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#why-docker"><i class="fa fa-check"></i><b>3.3.1</b> Why Docker</a></li>
<li class="chapter" data-level="3.3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.3.2</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>3.4</b> AWS EC2 instance</a><ul>
<li class="chapter" data-level="3.4.1" data-path="Infrastructure.html"><a href="Infrastructure.html#launch-an-ec2-instance"><i class="fa fa-check"></i><b>3.4.1</b> Launch an EC2 instance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>3.5</b> NGINX reverse proxy server</a></li>
<li class="chapter" data-level="3.6" data-path="Infrastructure.html"><a href="Infrastructure.html#software-development-workflow"><i class="fa fa-check"></i><b>3.6</b> Software development Workflow</a></li>
<li class="chapter" data-level="3.7" data-path="Infrastructure.html"><a href="Infrastructure.html#further-integrations"><i class="fa fa-check"></i><b>3.7</b> Further Integrations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inla.html"><a href="inla.html"><i class="fa fa-check"></i><b>4</b> INLA computation</a><ul>
<li class="chapter" data-level="4.1" data-path="inla.html"><a href="inla.html#LGM"><i class="fa fa-check"></i><b>4.1</b> Latent Gaussian Models LGM</a></li>
<li class="chapter" data-level="4.2" data-path="inla.html"><a href="inla.html#approx"><i class="fa fa-check"></i><b>4.2</b> Approximation in INLA setting</a><ul>
<li class="chapter" data-level="4.2.1" data-path="inla.html"><a href="inla.html#further-approximations-prolly-do-not-note-include"><i class="fa fa-check"></i><b>4.2.1</b> further approximations (prolly do not note include)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inla.html"><a href="inla.html#rinla"><i class="fa fa-check"></i><b>4.3</b> R-INLA package in a bayesian hierarchical regression perspective</a><ul>
<li class="chapter" data-level="4.3.1" data-path="inla.html"><a href="inla.html#overview"><i class="fa fa-check"></i><b>4.3.1</b> Overview</a></li>
<li class="chapter" data-level="4.3.2" data-path="inla.html"><a href="inla.html#example"><i class="fa fa-check"></i><b>4.3.2</b> Linear Predictor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>5</b> Point Referenced Data Modeling</a><ul>
<li class="chapter" data-level="5.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>5.1</b> Gaussian Process (GP)</a></li>
<li class="chapter" data-level="5.2" data-path="prdm.html"><a href="prdm.html#spatial-covariance-function"><i class="fa fa-check"></i><b>5.2</b> Spatial Covariance Function</a><ul>
<li class="chapter" data-level="5.2.1" data-path="prdm.html"><a href="prdm.html#Matern"><i class="fa fa-check"></i><b>5.2.1</b> Matérn Covariance Function</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="prdm.html"><a href="prdm.html#hedonic-models-literature-review-and-spatial-hedonic-price-models"><i class="fa fa-check"></i><b>5.3</b> Hedonic models Literature Review and Spatial Hedonic Price Models</a></li>
<li class="chapter" data-level="5.4" data-path="prdm.html"><a href="prdm.html#univariateregr"><i class="fa fa-check"></i><b>5.4</b> Point Referenced Regression for univariate spatial data</a></li>
<li class="chapter" data-level="5.5" data-path="prdm.html"><a href="prdm.html#hiermod"><i class="fa fa-check"></i><b>5.5</b> Hierarchical Bayesian models</a></li>
<li class="chapter" data-level="5.6" data-path="prdm.html"><a href="prdm.html#finalregr"><i class="fa fa-check"></i><b>5.6</b> INLA model through spatial hierarchical regression</a></li>
<li class="chapter" data-level="5.7" data-path="prdm.html"><a href="prdm.html#spatial-kriging"><i class="fa fa-check"></i><b>5.7</b> Spatial Kriging</a></li>
<li class="chapter" data-level="5.8" data-path="prdm.html"><a href="prdm.html#model-checking"><i class="fa fa-check"></i><b>5.8</b> Model Checking</a></li>
<li class="chapter" data-level="5.9" data-path="prdm.html"><a href="prdm.html#prior-specification"><i class="fa fa-check"></i><b>5.9</b> Prior Specification</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="spde.html"><a href="spde.html"><i class="fa fa-check"></i><b>6</b> SPDE approach</a><ul>
<li class="chapter" data-level="6.1" data-path="spde.html"><a href="spde.html#set-spde-problem"><i class="fa fa-check"></i><b>6.1</b> Set SPDE Problem</a></li>
<li class="chapter" data-level="6.2" data-path="spde.html"><a href="spde.html#spde-within-r-inla"><i class="fa fa-check"></i><b>6.2</b> SPDE within R-INLA</a></li>
<li class="chapter" data-level="6.3" data-path="spde.html"><a href="spde.html#first-point-krainsky-rubio-too-technical"><i class="fa fa-check"></i><b>6.3</b> First Point Krainsky Rubio TOO TECHNICAL</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>7</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="exploratory.html"><a href="exploratory.html#prep"><i class="fa fa-check"></i><b>7.1</b> Data preparation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="exploratory.html"><a href="exploratory.html#maps-and-geo-visualisations"><i class="fa fa-check"></i><b>7.1.1</b> Maps and Geo-Visualisations</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="exploratory.html"><a href="exploratory.html#counts-and-first-orientations"><i class="fa fa-check"></i><b>7.2</b> Counts and First Orientations</a></li>
<li class="chapter" data-level="7.3" data-path="exploratory.html"><a href="exploratory.html#text-mining-in-estate-review"><i class="fa fa-check"></i><b>7.3</b> Text Mining in estate Review</a></li>
<li class="chapter" data-level="7.4" data-path="exploratory.html"><a href="exploratory.html#missing-assessement-and-imputation"><i class="fa fa-check"></i><b>7.4</b> Missing Assessement and Imputation</a><ul>
<li class="chapter" data-level="7.4.1" data-path="exploratory.html"><a href="exploratory.html#missing-assessement"><i class="fa fa-check"></i><b>7.4.1</b> Missing assessement</a></li>
<li class="chapter" data-level="7.4.2" data-path="exploratory.html"><a href="exploratory.html#covariates-imputation"><i class="fa fa-check"></i><b>7.4.2</b> Covariates Imputation</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="exploratory.html"><a href="exploratory.html#model-specification"><i class="fa fa-check"></i><b>7.5</b> Model Specification</a></li>
<li class="chapter" data-level="7.6" data-path="exploratory.html"><a href="exploratory.html#mesh-building"><i class="fa fa-check"></i><b>7.6</b> Mesh building</a><ul>
<li class="chapter" data-level="7.6.1" data-path="exploratory.html"><a href="exploratory.html#shinyapp-for-mesh-assessment"><i class="fa fa-check"></i><b>7.6.1</b> Shinyapp for mesh assessment</a></li>
<li class="chapter" data-level="7.6.2" data-path="exploratory.html"><a href="exploratory.html#building-spde-model-on-mesh"><i class="fa fa-check"></i><b>7.6.2</b> BUilding SPDE model on mesh</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="exploratory.html"><a href="exploratory.html#spatial-kriging-prediction"><i class="fa fa-check"></i><b>7.7</b> Spatial Kriging (Prediction)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modelspec.html"><a href="modelspec.html"><i class="fa fa-check"></i><b>8</b> Model Selection &amp; Fitting</a><ul>
<li class="chapter" data-level="8.1" data-path="modelspec.html"><a href="modelspec.html#model-criticism"><i class="fa fa-check"></i><b>8.1</b> Model Criticism</a></li>
<li class="chapter" data-level="8.2" data-path="modelspec.html"><a href="modelspec.html#spatial-kriging-1"><i class="fa fa-check"></i><b>8.2</b> Spatial Kriging</a></li>
<li class="chapter" data-level="8.3" data-path="modelspec.html"><a href="modelspec.html#model-checking-1"><i class="fa fa-check"></i><b>8.3</b> Model Checking</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>9</b> Shiny Application</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="9.1" data-path="appendix.html"><a href="appendix.html#gp-basics"><i class="fa fa-check"></i><b>9.1</b> GP basics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">REST Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Infrastructure" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> API Technology Stack</h1>
<!--  You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).-->
<!-- Scraping function in the way they are designed are not portable and can only be executed locally. One way to scale up code is to make it available as API. -->
<p>In order to provide a fast and secure API service to the end user many technologies needs to be considered. Challenges in scraping as pointed out in section <a href="scraping.html#challenges">2.10</a> are many and still some unfortunately remains unsolved. Challenges not only regard scraping per se, but also the way and how many times the service has to interact with users. Some of the main obstacle to tackle in dealing with users are:</p>
<ul>
<li>The API has to be executed <span class="math inline">\(n\)</span> given times at fixed date-time daily and it has to store resulting data on a cloud database. This is done with the explicit goal of tracking the evolution of the phenomenon in time.</li>
<li>The API has to be very fast otherwise data can not be consumed.</li>
<li>The API has to be deployed so that it can be shared over different stakeholders, without having them to know what it takes.</li>
<li>The API maintainer needs to take action be to due to sudden unpredictable immobiliare.it changes, thus it needs to be continuously maintained and updated.</li>
<li>On the other code behind the service has to be version controlled and freezed, so that the service can guarantee continuity and prevent failures.</li>
<li>service has to be scalable at need since, due to deployment, when the number of users increases the run time performances should not decrease.</li>
<li>In addition API inbound traffic has to be managed both in terms traffic and security by securing access only to the ones authorized.</li>
</ul>
<p>The long list of requirements is met by many DevOps technologies, some of them offer a direct R embedding. As a result the technologies stack is the intersection between a comprehensive documentation available and the requirements listed.
Fo these reasons the recipe is to provide a REST Plumber API with 4 endpoints each of which calls scraping functions in Parallel built in section <a href="scraping.html#scraping">2</a>. On top of that a daily Cron Job scheduler exposes one precise API endpoint, which produces and later stores a .csv file in a NOSQL mongoDB Atlas could database. Containerization happens through a Linux OS (Ubuntu distr) Docker container hosted by a free tier AWS EC2 server machine equipped with 30GB max capacity. API endpoints are secured with https protocols, load balanced and protected with authentication by NGINX reverse proxy server. On a second server a Shiny App calls one endpoint gievn specified parameters that returns data from the former infrastructure. A sketch of the infrastructure is represented in figure <a href="Infrastructure.html#fig:CompleteStructure">3.1</a>.</p>
<p>Technology stack:</p>
<ul>
<li>GitHub version control</li>
<li>Scheduler cron job, section <a href="Infrastructure.html#scheduler">3.1</a></li>
<li>Plumber REST API, section <a href="Infrastructure.html#plumberapi">3.2.1</a></li>
<li>Docker containers, section <a href="Infrastructure.html#docker">3.3</a></li>
<li>AWS (Amazon Web Services) EC2 <a href="Infrastructure.html#aws">3.4</a></li>
<li>NGINX reverse proxy, section <a href="Infrastructure.html#nginx">3.5</a></li>
<li>MongoDB Atlas</li>
<li>Shiny</li>
</ul>
<div class="figure"><span id="fig:CompleteStructure"></span>
<img src="images/tot_infra.jpg" alt="" />
<p class="caption">Figure 3.1: complete infrastructure, author’s source</p>
</div>
<p>As a side note each single part of this thesis has been made according to some of the API inspiring criteria of sharing and self containerization. RMarkdown <span class="citation">(Allaire et al. <a href="#ref-rmarkdown1" role="doc-biblioref">2020</a>)</span> documents (book’s chapters) are compiled and then converted into .html files. Through Bookdown <span class="citation">(Xie <a href="#ref-bookdown2" role="doc-biblioref">2016</a>)</span> the resulting documents are put together according to general .yml instruction file and are readble as gitbook.
Files are then pushed to a <a href="https://github.com/NiccoloSalvini/thesis">Github repository</a>. By a simple trick with GH pages, .html files are dispalyed into a Github subdomain hosted at <a href="https://niccolosalvini.github.io/thesis/">link</a>. The resulting deployed gitbook can also produce a .pdf version output through a Xelatex engine. Xelatex compiles .Rmd documents according to a .tex template which formatting rules are contained in a further .yml file. The pdf version of the thesis can be obtained by clicking the download button, then choosing pdf output version in the upper banner. For further references on the topic <span class="citation">Xie (<a href="#ref-bookdown2" role="doc-biblioref">2016</a>)</span></p>
<p>Some of the main technologies implied will be viewed singularly, nonetheless for brevity reasons the rest needs to be skipped.</p>
<div id="scheduler" class="section level2">
<h2><span class="header-section-number">3.1</span> Scheduler</h2>

<div class="definition">
<span id="def:scheduler" class="definition"><strong>Definition 3.1  (Scheduler)  </strong></span>A Scheduler in a process is a component on a OS that allows the computer to decide which activity is going to be executed. In the context of multi-programming it is thought as a tool to keep CPU occupied as much as possible.
</div>

<p>As an example it can trigger a process while some other is still waiting to finish. There are many type of scheduler and they are based on the frequency of times they are executed considering a certain closed time neighbor.</p>
<ul>
<li>Short term scheduler: it can trigger and queue the “ready to go” tasks
<ul>
<li>with pre-emption</li>
<li>without pre-emption</li>
</ul></li>
</ul>
<p>The ST scheduler selects the process and It gains control of the CPU by the dispatcher. In this context we can define latency as the time needed to stop a process and to start a new one.</p>
<ul>
<li>Medium term scheduler</li>
<li>Long term scheduler</li>
</ul>
<p>for some other useful but beyond the scope references, such as the scheduling algorithm the reader can refer to <span class="citation">(Wikiversità <a href="#ref-wiki:scheduler" role="doc-biblioref">2020</a>)</span>.</p>
<div id="cron-jobs" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Cron Jobs</h3>

<div class="definition">
<span id="def:cronjob" class="definition"><strong>Definition 3.2  (Cronjob)  </strong></span>Cron job is a software utility which acts as a time-based job scheduler in Unix-like OS. Linux users that set up and maintain software environments exploit cron to schedule their day-to-day routines to run periodically at fixed times, dates, or intervals. It typically automates system maintenance but its usage is very flexible to whichever needed. It is lightweight and it is widely used since it is a common option for Linux users.
</div>

<p>The tasks by cron are driven by a crontab file, which is a configuration file that specifies a set of commands to run periodically on a given schedule. The crontab files are stored where the lists of jobs and other instructions to the cron daemon are kept.
Each line of a crontab file represents a job, and the composition follows the syntax in figure <a href="Infrastructure.html#fig:crontab">3.2</a></p>
<div class="figure"><span id="fig:crontab"></span>
<img src="images/crontab.PNG" alt="" />
<p class="caption">Figure 3.2: Crontab Scheduling Syntax</p>
</div>
<p>Each line of a crontab file represents a job. This example runs a shell named scheduler.sh at 23:45 (11:45 PM) every Saturday. .sh commands can update mails and other minor routines.</p>
<p>45 23 * * 6 /home/oracle/scripts/scheduler.sh</p>
<p>Some rather unusual scheduling definitions for crontabs can be found in this reference <span class="citation">(Wikipedia contributors <a href="#ref-wiki:cronjob" role="doc-biblioref">2020</a>)</span>. Crontab’s syntax completion can be made easier through <a href="https://crontab.guru/">this</a> GUI.</p>
<p>The cron job needs to be ran on scraping fucntions at 11:30 PM every single day. The get_data.R script first sources an endpoint function, then it applies the function with fixed parameters. Parameters describe the url specification, so that each time the scheduler runs the get_data.R collects data from the same source. Day after day .json files are generated and then stored into a NOSQL <em>mongoDB</em> database whose credentials are public. Data are collected on a daily basis with the explicit aim to track day-by-day changes both in the new entries an goners in rental market, and to investigate the evolution of price differentials over time. Spatio-Temporal modeling is still quite unexplored, data is saved for future used. Crontab configuration for daily 11:30 PM schedules has this appearance:</p>
<p>30 11 * * * /home/oracle/scripts/get_data.R</p>
<p>To a certain extent what it has been already presented since now might fit for personal use. A scheduler can daily execute the scraping script and can generate a .csv file. Later the same .csv file can be parsed into an application and analysis can be locally reported. The solution proposed is totally <em>not feasible</em> in a production environment, since in order to be executed a vast number files has to be sourced and a number of functions should be routinely called. For these reasons the present architecture can not be shared. The solution adopted tries to minimize the analyst/scientist involvement into scraping procedures by offering a compact and fast (due to Parallel execution) service that manages all the processes without having to know how scraping under the hood is working.</p>
</div>
</div>
<div id="rest-api" class="section level2">
<h2><span class="header-section-number">3.2</span> REST API</h2>

<div class="definition">
<span id="def:api" class="definition"><strong>Definition 3.3  (API)  </strong></span>API stands for application programming interface and it is a set of definitions and protocols for building and integrating application software. Most importantly APIs let a product or a service communicate with other products and services without having to know how they’re implemented.
</div>

APIs are thought of as contracts, with documentation that represents an general agreement between parties.
There are many types of APIs that exploit different media and architectures to communicate with apps or services.

<div class="definition">
<span id="def:rest" class="definition"><strong>Definition 3.4  (REST)  </strong></span>The specification REST stands for <strong>RE</strong>presentational <strong>S</strong>tate <strong>T</strong>ransfer and is a set of <em>architectural principles</em>.
</div>

<p>When a request is made through a REST API it transfers a representation of the state to the requester. This representation, is submitted in one out of the many available formats via HTTP: JSON (Javascript Object Notation), HTML, XLT, TXT. JSON is the most popular because it is language agnostic <span class="citation">(<a href="#ref-what_is_a_rest_api" role="doc-biblioref">2018</a>)</span>, as well as it is more comfortable to be read and parsed.
In order for an API to be considered REST, it has to conform to these criteria:</p>
<ul>
<li>A client-server architecture made up of clients, servers, and resources, with requests managed through HTTP.</li>
<li>Stateless client-server communication, meaning no client information is stored between requests and each request is separate and unconnected.</li>
<li>Cacheable data that streamlines client-server interactions.</li>
<li>A uniform interface between components so that information is transferred in a standard form. This requires that:
<ul>
<li>resources requested are identifiable and separate from the representations sent to the client.</li>
<li>resources can be manipulated by the client via the representation they receive because the representation contains enough information to do so.</li>
<li>self-descriptive messages returned to the client have enough information to describe how the client should process it.</li>
<li>hypermedia, meaning that after accessing a resource the client should be able to use hyperlinks to find all other currently available actions they can take.</li>
</ul></li>
<li>A layered system that organizes each type of server (those responsible for security, load-balancing, etc.) involved the retrieval of requested information into hierarchies, invisible to the client.</li>
</ul>
<p>REST API accepts http requests as input and elaborates them through end points. An end point identifies the operation through traditional http methods (e.g. /GET /POST) that the API caller wants to perform. Further documentation and differences between HTTP and REST API can be found to this <a href="https://docs.aws.amazon.com/it_it/apigateway/latest/developerguide/http-api-vs-rest.html">reference</a>.</p>
<p>open REST API examples:
- BigQuery API: A data platform for customers to create, manage, share and query data.
- YouTube Data API v3: The YouTube Data API v3 is an API that provides access to YouTube data, such as videos, playlists, and channels.
- Cloud Natural Language API: Provides natural language understanding technologies, such as sentiment analysis, entity recognition, entity sentiment analysis, and other text annotations, to developers.
- Skyscanner Flight Search API: The Skyscanner API lets you search for flights &amp; get flight prices from Skyscanner’s database of prices, as well as get live quotes directly from ticketing agencies.
- Openweathermap API: current weather data for any location on Earth including over 200,000 cities.</p>
<div class="figure">
<img src="images/Rest-API.png" alt="" />
<p class="caption">API general functioning functioning</p>
</div>
<div id="plumberapi" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Plumber REST API</h3>
<p>Plumber allows the user to create a REST API by adding decoration comments to the existing R code, in this case to scraping code. Decorations are a special type of comments that suggests to Plumber where and when the API specifications parts are. Below a simple example extracted by the documentation:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="Infrastructure.html#cb10-1"></a><span class="co"># plumber.R</span></span>
<span id="cb10-2"><a href="Infrastructure.html#cb10-2"></a></span>
<span id="cb10-3"><a href="Infrastructure.html#cb10-3"></a><span class="co">#* Echo back the input</span></span>
<span id="cb10-4"><a href="Infrastructure.html#cb10-4"></a><span class="co">#* @param msg The message to echo</span></span>
<span id="cb10-5"><a href="Infrastructure.html#cb10-5"></a><span class="co">#* @get /echo</span></span>
<span id="cb10-6"><a href="Infrastructure.html#cb10-6"></a><span class="cf">function</span>(<span class="dt">msg=</span><span class="st">&quot;&quot;</span>) {</span>
<span id="cb10-7"><a href="Infrastructure.html#cb10-7"></a>  <span class="kw">list</span>(<span class="dt">msg =</span> <span class="kw">paste0</span>(<span class="st">&quot;The message is: &#39;&quot;</span>, msg, <span class="st">&quot;&#39;&quot;</span>))</span>
<span id="cb10-8"><a href="Infrastructure.html#cb10-8"></a>}</span>
<span id="cb10-9"><a href="Infrastructure.html#cb10-9"></a></span>
<span id="cb10-10"><a href="Infrastructure.html#cb10-10"></a><span class="co">#* Plot a histogram</span></span>
<span id="cb10-11"><a href="Infrastructure.html#cb10-11"></a><span class="co">#* @serializer png</span></span>
<span id="cb10-12"><a href="Infrastructure.html#cb10-12"></a><span class="co">#* @get /plot</span></span>
<span id="cb10-13"><a href="Infrastructure.html#cb10-13"></a><span class="cf">function</span>() {</span>
<span id="cb10-14"><a href="Infrastructure.html#cb10-14"></a>  rand =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb10-15"><a href="Infrastructure.html#cb10-15"></a>  <span class="kw">hist</span>(rand)</span>
<span id="cb10-16"><a href="Infrastructure.html#cb10-16"></a>}</span>
<span id="cb10-17"><a href="Infrastructure.html#cb10-17"></a></span>
<span id="cb10-18"><a href="Infrastructure.html#cb10-18"></a><span class="co">#* Return the sum of two numbers</span></span>
<span id="cb10-19"><a href="Infrastructure.html#cb10-19"></a><span class="co">#* @param a The first number to add</span></span>
<span id="cb10-20"><a href="Infrastructure.html#cb10-20"></a><span class="co">#* @param b The second number to add</span></span>
<span id="cb10-21"><a href="Infrastructure.html#cb10-21"></a><span class="co">#* @post /sum</span></span>
<span id="cb10-22"><a href="Infrastructure.html#cb10-22"></a><span class="cf">function</span>(a, b) {</span>
<span id="cb10-23"><a href="Infrastructure.html#cb10-23"></a>  <span class="kw">as.numeric</span>(a) <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(b)</span>
<span id="cb10-24"><a href="Infrastructure.html#cb10-24"></a>}</span></code></pre></div>
<p>three endpoints associated to 2 /GET and 1 /POST requests are made available. Functions are made clear without names so that whenever the endpoint is called functions are directly executed.
Decorations are marked as this <code>#*</code> and they are followed by specific keywords denoted with <code>@</code>.
- the <code>@params</code> keyword refers to parameter that specifies the corpus of the HTTP request, i.e. the inputs with respect to the expected output. If default parameters are inputted then the API response is the elaboration of the functions with default parameters. As opposite endpoint function elaborates the provided parameters and returns a response.
- <code>#* @serializer</code> specifies the extension of the output file when needed.
- <code>#* @get</code> specifies the method of HTTP request sent.
- <code>/echo</code> is the end point name.
- <code>@filter</code> decorations activates a filter layer which are used to track logs and to parse request before passing the argbody to the end points.</p>
<p>Many more options are available to customize plumber API but are beyond the scope, a valuable resource for further insights can be found in the dedicated package website <span class="citation">(<span class="citeproc-not-found" data-reference-id="an_api_generator_for_r"><strong>???</strong></span>)</span></p>
</div>
<div id="immobiliare.it-parallel-rest-api" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Immobiliare.it <em>Parallel</em> REST API</h3>
<p>(Sanitization, antidossing, logs tracking)</p>
<p>The API service is composed by 4 endpoints <em>/scrape</em> , <em>/links</em>, <em>/complete</em> and <em>/get_data</em>:</p>
<ul>
<li><p>*/scrape performs a fast Parallel scraping of the website that leverages a rooted tree shortest path to get to data (250 X 5 predictors in <span class="math inline">\(\approx 10.91^{&#39;&#39;}\)</span>). This comes at the cost of the number of available covariates to scrape which are: title, price, number of rooms, sqmeter, primarykey. By default the end point scrape data from Milan real estate rents. It is a superficial and does not contain geospatial, however it might fit for some basic regression settings. The macrozone parameter allows to specify the NIL (Nucleo Identità Locale), targeting very detailed zones in some of the cities for which is available (Roma, Firenze, Milano, Torino).</p></li>
<li><p>*/links: extracts the list of each single advertisement link belonging to each of the npages parameter specified, recall section <a href="scraping.html#webstructure">2.5.1</a>. It displays sufficient performances in terms of run time. It is strictly needed to apply the following endpoint. .thesis options secures a pre combined url with the data wanted for thesis analysis. The option takes care to decompose the website structure of the url supplied with the aim to apply scraping function in the /complete endpoint.</p></li>
<li><p>*/complete: both the function all.links and complete are sourced. The former with the aim to grab each single links and store it into an object. The latter to actually iterate Parallel scraping on each of the extracted link.</p></li>
<li><p>*/get_data: it triggers the data extraction by sourcing the /complete endpoint and then storing .json file into the NOSQL mongoDB ATLAS</p></li>
</ul>
<div class="figure">
<img src="images/swagger.PNG" alt="" />
<p class="caption">Swagger UI screenshot, author’s source</p>
</div>
</div>
<div id="APIdocs" class="section level3">
<h3><span class="header-section-number">3.2.3</span> REST API documentation</h3>
<ul>
<li>Get FAST data, it covers 5 covariates:</li>
</ul>
<pre><code>      GET */scrape
      @param city [chr string] the city you are interested in (e.g. &quot;roma&quot;, &quot;milano&quot;, &quot;firenze&quot;--&gt; lowercase, without accent)
      @param npages [positive integer] number of pages to scrape, default = 10, min  = 2, max = 300
      @param type [chr string] &quot;affitto&quot; = rents, &quot;vendita&quot;  = sell 
      @param macrozone [chr string] avail: Roma, Firenze, Milano, Torino; e.g. &quot;fiera&quot;, &quot;centro&quot;, &quot;bellariva&quot;, &quot;parioli&quot; 
      content-type: application/json </code></pre>
<ul>
<li>Get all the links</li>
</ul>
<pre><code>      GET */link
      @param city [chr string] the city you are interested to extract data (lowercase without accent)
      @param npages [positive integer] number of pages to scrape default = 10, min  = 2, max = 300
      @param type [chr string] &quot;affitto&quot; = rents, &quot;vendita&quot;  = sell 
      @param .thesis [logical] data used for master thesis
      content-type: application/json </code></pre>
<ul>
<li>Get the complete set of covariates (52) from each single links, takes a while</li>
</ul>
<pre><code>      GET */complete
      @param city [chr string] the city you are interested to extract data (lowercase without accent)
      @param npages [positive integer] number of pages to scrape default = 10, min  = 2, max = 300
      @param type [chr string] &quot;affitto&quot; = rents, &quot;vendita&quot;  = sell 
      @param .thesis [logical] data used for master thesis
      content-type: application/json</code></pre>
<p>Up to this point the API can smoothly run in local and potentially can be deployed to share results without requesting scraping process knowledge. However the API software for now is not portable and is very heavy. In addition it can also run into failures for many reasons, one among the others is package versione incompatibility due to updates. In the end it also fully relies on the laptop computational power that can be heavily stressed when a number of API calls are executed, especially for single threaded programming languages as R.
The approach followed proposes a dedicated lightweight software environment that minimizes dependencies both improving performances and enabling the <em>cloud computing</em> coverage. A fast growing techonlogy is what fits the need.</p>
</div>
</div>
<div id="docker" class="section level2">
<h2><span class="header-section-number">3.3</span> Docker</h2>

<div class="definition">
<span id="def:docker" class="definition"><strong>Definition 3.5  (Docker)  </strong></span><em>Docker</em> <span class="citation">(Merkel <a href="#ref-docker" role="doc-biblioref">2014</a>)</span> is a software tool to create and deploy applications using containers.
<em>Docker containers</em> are a standard unit of software (i.e. software boxes) where everything needed for applications, such as libraries or dependencies can be run reliably and quickly. Containers are also portable, in the sense that they can be taken from one computing environment to the following without further adaptations.
</div>

<p>Containers can be thought as an abstraction that groups code and dependencies together. One major advantage of containers is that multiple containers can run on the same machine with the same OS with their specific dependencies. Each container can run its own isolated process in the user space, so that each task/application is complementary to the other. The fact that containers are treated singularly enables a collaborative framework that it also simplifies bugs isolation.</p>
<!-- Containers are lightweight and take up less space than Virtual Machines (container images are files which can take up typically tens of MBs in size), can handle more applications and require fewer Virtual Machines and OS. The structure differences are figrued in \@ref(fig:). -->
<!-- ![Docker containers versus Virtual Machines, ](images/dockerVSvirtualmachines.PNG) -->
<p>When images are built <em>Docker container</em> are created and can be open sourced through Docker Hub.
<em>Docker Hub</em> is a web service provided by Docker for searching and sharing container images with other teams or developers in the community. Docker Hub can connect with GitHub behind authorization entailing an image version control tool. Once the connection is established changes that are pushed with git to the GitHub repository are passed to Docker Hub. The push command automatically triggers the image building. Then docker image can be tagged (salvini/api-immobiliare:latest)so that on one hand it is recognizable and on the other can be reused in the future. Once the building stage is completed the DH repository can be pulled and then run locally on machine or cloud, see section <a href="Infrastructure.html#aws">3.4</a>.
Docker building and testing images can be very time consuming. R packages can take a long time to install because code has to be compiled, especially if using R on a Linux server or in a Docker container.
Rstudio <a href="https://packagemanager.rstudio.com/client/#/">package manager</a> includes beta support for pre-compiled R packages that can be installed faster. This dramatically reduces packages time installation <span class="citation">(Nolis <a href="#ref-nolis_2020" role="doc-biblioref">2020</a>)</span>.
In addition to that an open source project <a href="https://www.rocker-project.org/images/">rocker</a> has narrowed the path for developers by building custom R docker images for a wide range of usages. What can be read from their own website about the project is: “The rocker project provides a collection of containers suited for different needs. find a base image to extend or images with popular software and optimized libraries pre-installed. Get the latest version or a reproducible fixed environment”.</p>
<div id="why-docker" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Why Docker</h3>
<p><a href="https://it.indeed.com/">Indeed</a>, an employment-related search engine, released an article on 2019 displaying changing trends from 2015 to 2019 in Technology Job market, a summary of those changes is in figure <a href="Infrastructure.html#fig:indeedstats">3.4</a>. Many changes are relevant in key technologies. Two among the others technologies (i.e. docker and Azure, arrow pointed) have experienced a huge growth and both refer to the certain demand input: <em>containers</em> and <em>cloud computing</em>.
The landscape of Data Science is changing from reporting to application building:
In 2015 - Businesses reports drive better decisions.
In 2020 - Businesses need apps to empower better decision making at all levels.</p>
<div class="figure"><span id="fig:indeedstats"></span>
<img src="images/Inkedindeed_jobs_LI.jpg" alt="" />
<p class="caption">Figure 3.4: Indeed top skills for 2019 in percent changes, <span class="citation">Flowers (<a href="#ref-top_tech_skills" role="doc-biblioref">2020</a>)</span> source</p>
</div>
<p>For all the things said what docker is bringing to business are <span class="citation">(R. H. Inc. <a href="#ref-red_hat_customer_portal" role="doc-biblioref">2020</a>)</span>:</p>
<ul>
<li><em>Speed application deployment</em> : containers include the minimal run time requirements of the application, reducing their size and allowing them to be deployed quickly.</li>
<li><em>Portability across machines</em> : an application and all its dependencies can be bundled into a single container that is independent from the host version of Linux kernel, platform distribution, or deployment model. This container can be transferred to another machine that runs Docker, and executed there without compatibility issues.</li>
<li><em>Version control and component reuse</em> : you can track successive versions of a container, inspect differences, or roll-back to previous versions. Containers reuse components from the preceding layers, which makes them noticeably lightweight. In addition due to Docker Hub it is possible to establish a connection between Git and DockerHub. Vesion</li>
<li><em>Sharing</em> : you can use a remote repository to share your container with others. It is also possible to configure a private repository hosted on Docker Hub.</li>
<li><em>Lightweight footprint and minimal overhead</em> : Docker images are typically very small, which facilitates rapid delivery and reduces the time to deploy new application containers.</li>
<li><em>Fault isolation</em> :Docker reduces effort and risk of problems with application dependencies. Docker also freezes the environment to the preferred packages version so that it guarantees continuity in deployment and isolate the container from system fails coming from package version updates.</li>
</ul>
<p>The way to tell docker which system requirements are needed in the newly born software is a <em>Dockerfile</em>.</p>
</div>
<div id="dockerfile" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Dockerfile</h3>
<p>Docker can build images automatically by reading instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands/rules a generic user could call on the CLI to assemble an image. Executing the command <code>docker build</code> from shell the user can trigger the image building. That executes sequentially several command-line instructions. For thesis purposes a Dockerfile is written with the specific instructions and then the file is pushed to GitHub repository. Once pushed DockerHub automatically parses the repository looking for a plain text file whose name is “Dockerfile”. When It is matched then it triggers the building of the image.</p>
<p>The Dockerfile used to trigger the building of the docker container has the following sequential set of instructions in figure <a href="Infrastructure.html#fig:dockerfile">3.5</a>) :</p>
<div class="figure"><span id="fig:dockerfile"></span>
<img src="images/dockerfile.PNG" alt="" />
<p class="caption">Figure 3.5: Example of a Dockerfile from Docker Hub, author’s source</p>
</div>
<p>where the instructions are:</p>
<ul>
<li><p><code>FROM rocker/tidyverse:latest</code> : The command imports a pre-built image by the rocker team that contains the latest (tag latest) version of base-R along with the tidyverse packages.</p></li>
<li><p><code>MAINTAINER Niccolo Salvini "niccolo.salvini27@gmail.com"</code> : The command tags the maintainer and its e-mail contact information.</p></li>
<li><p><code>RUN apt-get update &amp;&amp; apt-get install -y \ libxml2-dev \ libudunits2-dev</code> :The command update and install Linux dependencies needed for running R packages. <code>rvest</code> requires libxml2-dev and <code>magrittr</code> needs libudunits2-dev. If they are not installed then associated libraries can not be loaded. Linux dependencies needed have been found by trial and error while building containers. Building logs messages print errors and suggest which dependency is mandatory.</p></li>
<li><p><code>RUN R -e "install.packages(c('plumber','tibble','...',dependencies=TRUE)</code> : the command install all the packages required to execute the files (R files) containerized for the scraping. Since all the packages have their direct R dependencies the option <code>dependencies=TRUE</code> is needed.</p></li>
<li><p><code>RUN R -e "install.packages('https://cran.r-project.org/.../iterators, type='source')</code>
<code>RUN R -e "install.packages('https://cran.r-project.org/.../foreach/, type='source')</code>
<code>RUN R -e "install.packages('https://cran.r-project.org/.../doParallel, type='source')</code>
DoParallel was not available in package manager for R version later than 4.0.0. For this reason the choice was to install a previous source version by the online repository, as well as its dependencies.</p></li>
<li><p><code>COPY \\</code> The command tells Docker copies all the files in the container.</p></li>
<li><p><code>EXPOSE 8000</code> : the commands instructs Docker that the container listens on the specified network ports 8000 at runtime. It is possible to specify whether the port exposed listens on UDP or TCP, the default is TCP (this part needs a previous set up previous installing, for further online documentation It is recommended <span class="citation">(D. Inc. <a href="#ref-docker_documentation_2020" role="doc-biblioref">2020</a>)</span> )</p></li>
<li><p><code>ENTRYPOINT ["Rscript", "main.R"]</code> : the command tells docker to execute the file main.R within the container that triggers the API start. In main.R it are pecified both the port and the host where API expects to be exposed (in this case port 8000).</p></li>
</ul>
<p>In order to make the system stand-alone and make the service available to a wider range of subjects a choice has to be made. The service has to have both the characteristics to be run on demand and to specify query parameters.</p>
</div>
</div>
<div id="aws" class="section level2">
<h2><span class="header-section-number">3.4</span> AWS EC2 instance</h2>
<p>Exporting the API on a server allows to make scraping available to a various number of services thorough multitude of subjects. Since it can not be specified a-priori how many times and users are going to enjoy the service a scalable solutio might fill the needs. Scalable infrastructure through a flexible cloud provider combined with nginx load balancing can offer a stable and reliable infrastructure for a relatively cheap price.
AWS offers a wide range of services each of which for a wide range of budgets and integration. Free tier servers can be rent up to a certain amount of storage and computation that nearly 0s the total bill. The cloud provider also has a dedicated webpage to configure the service needed with respect to the usage named <a href="https://aws.amazon.com/en/aws-cost-management/">amazon cost manager</a>.</p>

<div class="definition">
<p><span id="def:aws" class="definition"><strong>Definition 3.6  (AWS EC2)  </strong></span>Amazon Elastic Compute Cloud (EC2) is a web service that contributes to a secure, flexible computing capacity in the AWS cloud. EC2 allows to rent as many virtual servers as needed with customized capacity, security and storage.</p>
</div>

<p>[few words still on EC2]</p>
<div id="launch-an-ec2-instance" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Launch an EC2 instance</h3>
<p>The preliminary step is to pick up an AMI (Amazon Machine Image). AWS AMI are already pre set-up machines with standardized specifications built with the purpose to speed up choosing the a customed machine. Since the project is planned to be nearly 0-cost a “Free Tier Eligible” Linux server is chosen. By checking the Free Tier box all the available free tiers machines are displayed. The machine selected has this specification: t2.micro with 1 CPU and 1GB RAM and runs on a Ubuntu distribution OS. First set up settings needs to be left as-is, networking and VPC can always be updated when needed. In the “add storage” step 30 GB storage are selected, moreover 30 represent the upper limit since the server can be considered free tier. Tags windows are beyond the scope. Secondly configuration needs to account security and a new entry below SSH connection (port 22) has to be set in. New security configuration has to have TCP specification and should be associated to port 8000. Port 8000, as in dockerfile section <a href="Infrastructure.html#dockerfile">3.3.2</a>, has been exposed and needs to be linked to the security port opened.</p>
<div class="figure">
<img src="images/aws.PNG" alt="" />
<p class="caption">aws_dashboard</p>
</div>
<p>At this point instance is prepared to run and in a few minutes is deployed. Key pairs, if never done before, are generated and a .pem file is saved and securely stored. Key pairs are a mandatory step to log into the Ubuntu server via SSH. SSH connection in Windows OS can be handled with <a href="https://www.putty.org/">PuTTY</a>, which is a SSH and telnet client designed for Windows. At first PuTTYgen, a PuTTY extensions, has to convert the key pair .pem file into a .ppk extension (otherwise PuTTY can not read it). Once .ppk is converted is immediately sourced in the authorization panel. If everything works and authentication is verified then the Ubuntu server CLI appears and interaction with the server is made possible.
Once the CLI pops out some Linux libraries to check file structure (“tree”) and Docker are installed. Then a connection with Docker hub is established providing user login credentials. From the Hub repository the container image is pulled on the machine and is then executed with the docker RUN command.
AWS automatically assign to the server a unique Public DNS address which is going to be the REST API url to call.
the Public DNS has the following form:</p>
<p><code>ec2-15-161-94-121.eu-south-1.compute.amazonaws.com</code></p>
</div>
</div>
<div id="nginx" class="section level2">
<h2><span class="header-section-number">3.5</span> NGINX reverse proxy server</h2>
<p>For analysis purposes NGINX is open source software for reverse proxying and load balancing.
Proxying is typically used to distribute the load among several servers, seamlessly show content from different websites, or pass requests for processing to application servers over protocols other than HTTP.
[…]</p>
<p>When NGINX proxies a request, it sends the request to a specified proxied server, fetches the response, and sends it back to the client. It is possible to proxy requests to an HTTP server (another NGINX server or any other server) or a non-HTTP server (which can run an application developed with a specific framework, such as PHP or Python) using a specified protocol. Supported protocols include FastCGI, uwsgi, SCGI, and memcached.
[…]</p>
<p>.conf file and installation on Linux server. Security and Authentication.</p>
</div>
<div id="software-development-workflow" class="section level2">
<h2><span class="header-section-number">3.6</span> Software development Workflow</h2>
<div class="figure"><span id="fig:sfmap"></span>
<img src="images/SoftwareDevWF.jpg" alt="" />
<p class="caption">Figure 3.6: Sofwtare development workflow, author’s source</p>
</div>
</div>
<div id="further-integrations" class="section level2">
<h2><span class="header-section-number">3.7</span> Further Integrations</h2>
<p>From a software point of view a more robust code can be obtained embedding recent R software development frameworks as <code>Golem</code> <span class="citation">Colin Fay (<a href="#ref-colin_fay_2020" role="doc-biblioref">2020</a>)</span> into the existing code. The framework stimulates the usage of modules According to the latest literature APIs (as well as Shiny) should be treated as R packages (<span class="citation">Santiago (<a href="#ref-plungr" role="doc-biblioref">2020</a>)</span> aligns with that) as argued in section 4.2 <span class="citation">Colin Fay (<a href="#ref-colin_fay_2020" role="doc-biblioref">2020</a>)</span> and in the <a href="https://deanattali.com/2015/04/21/r-package-shiny-app/">comment</a> by Dean Attali. As a consequence of that <em>TDD</em> (i.e.Test-Driven Development <span class="citation">(“Test Driven Development” <a href="#ref-TDD_2004" role="doc-biblioref">2004</a>)</span>) through the tools of <span class="citation">Wickham and Bryan (<a href="#ref-usethis" role="doc-biblioref">2020</a>)</span> and <span class="citation">Wickham (<a href="#ref-testthat" role="doc-biblioref">2011</a>)</span> during package building can make software more robust, organized and production graded. Loadtest <span class="citation">(<span class="citeproc-not-found" data-reference-id="loadtest"><strong>???</strong></span>)</span> can help figure out
CI/CT
It can be missed to cite a popular API development integration service and automate testing tool <a href="https://www.postman.com/">Postman</a>, which does the best when POST requests endpoints are served, since for the moment they are not required it is not used.</p>
<p>Pins is an r packages <a href="https://rstudio.com/resources/rstudioconf-2020/deploying-end-to-end-data-science-with-shiny-plumber-and-pins/?mkt_tok=eyJpIjoiTmprNU1USXhPVEprWXpNMSIsInQiOiJtTUhKVzlvSjVIV2hKc0NRNVU1NTRQYSsrRGd5MWMyemlTazQ5b1lHRGJXNVBLcnpScjZRaWVcL2JGUjBPNGIwV3pwY1dKTW45cnhcL2JzZUlGWndtSFNJZVNaOUcyc1ZXcEJOcnppSVJXSGZRSVU1ZUY1YUU2NWdDamoxZG5VMHZcLyJ9">this link</a>
software development framework and tools for testing <a href="https://github.com/isteves/plumbplumb">this work</a></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-rmarkdown1">
<p>Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2020. <em>Rmarkdown: Dynamic Documents for R</em>. <a href="https://github.com/rstudio/rmarkdown">https://github.com/rstudio/rmarkdown</a>.</p>
</div>
<div id="ref-colin_fay_2020">
<p>Colin Fay, Sébastien Rochette. 2020. <em>Engineering Production-Grade Shiny Apps</em>. <a href="https://engineering-shiny.org/">https://engineering-shiny.org/</a>.</p>
</div>
<div id="ref-top_tech_skills">
<p>Flowers, Andrew. 2020. “Indeed Tech Skills Explorer: Today’s Top Tech Skills.” <em>Indeed Hiring Lab</em>. <a href="https://www.hiringlab.org/2019/11/19/todays-top-tech-skills/">https://www.hiringlab.org/2019/11/19/todays-top-tech-skills/</a>.</p>
</div>
<div id="ref-docker_documentation_2020">
<p>Inc., Docker. 2020. “Get Docker.” <em>Docker Documentation</em>. <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</p>
</div>
<div id="ref-red_hat_customer_portal">
<p>Inc., Red Hat. 2020. “7.2. Advantages of Using Docker Red Hat Enterprise Linux 7.” <em>Red Hat Customer Portal</em>. <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/7.0_release_notes/sect-red_hat_enterprise_linux-7.0_release_notes-linux_containers_with_docker_format-advantages_of_using_docker">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/7.0_release_notes/sect-red_hat_enterprise_linux-7.0_release_notes-linux_containers_with_docker_format-advantages_of_using_docker</a>.</p>
</div>
<div id="ref-docker">
<p>Merkel, Dirk. 2014. “Docker: Lightweight Linux Containers for Consistent Development and Deployment.” <em>Linux Journal</em> 2014 (239): 2.</p>
</div>
<div id="ref-nolis_2020">
<p>Nolis, Jacqueline. 2020. “R Docker Faster.” <em>Medium</em>. Medium. <a href="https://medium.com/@skyetetra/r-docker-faster-28e13a6d241d">https://medium.com/@skyetetra/r-docker-faster-28e13a6d241d</a>.</p>
</div>
<div id="ref-plungr">
<p>Santiago, Joao. 2020. <em>Plungr: Opinionated Framework for Developing Plumber Apis</em>. <a href="https://github.com/ozean12/plungr">https://github.com/ozean12/plungr</a>.</p>
</div>
<div id="ref-TDD_2004">
<p>“Test Driven Development.” 2004. <em>Wikipedia</em>. Wikimedia Foundation. <a href="https://en.wikipedia.org/wiki/Test_driven_development">https://en.wikipedia.org/wiki/Test_driven_development</a>.</p>
</div>
<div id="ref-testthat">
<p>Wickham, Hadley. 2011. “Testthat: Get Started with Testing.” <em>The R Journal</em> 3: 5–10. <a href="https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf">https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf</a>.</p>
</div>
<div id="ref-usethis">
<p>Wickham, Hadley, and Jennifer Bryan. 2020. <em>Usethis: Automate Package and Project Setup</em>. <a href="https://CRAN.R-project.org/package=usethis">https://CRAN.R-project.org/package=usethis</a>.</p>
</div>
<div id="ref-wiki:cronjob">
<p>Wikipedia contributors. 2020. “Cron — Wikipedia, the Free Encyclopedia.” <a href="https://en.wikipedia.org/w/index.php?title=Cron&amp;oldid=978051592">https://en.wikipedia.org/w/index.php?title=Cron&amp;oldid=978051592</a>.</p>
</div>
<div id="ref-wiki:scheduler">
<p>Wikiversità. 2020. “Scheduling — Wikiversità,” <a href="https://it.wikiversity.org/w/index.php?title=Scheduling&amp;oldid=214572">https://it.wikiversity.org/w/index.php?title=Scheduling&amp;oldid=214572</a>.</p>
</div>
<div id="ref-bookdown2">
<p>Xie, Yihui. 2016. <em>Bookdown: Authoring Books and Technical Documents with R Markdown</em>. Boca Raton, Florida: Chapman; Hall/CRC. <a href="https://github.com/rstudio/bookdown">https://github.com/rstudio/bookdown</a>.</p>
</div>
<div id="ref-what_is_a_rest_api">
<p>2018. <em>What Is a REST API?</em> Red Hat. <a href="https://www.redhat.com/en/topics/api/what-is-a-rest-api">https://www.redhat.com/en/topics/api/what-is-a-rest-api</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="scraping.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inla.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/thesis/edit/master/03-apiinfrastructure.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
