<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 INLA bayesian computation | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</title>
  <meta name="description" content="Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 INLA bayesian computation | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />
  <meta property="og:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 INLA bayesian computation | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA" />
  
  <meta name="twitter:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="exploratory.html"/>
<link rel="next" href="prdm.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"> End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#what-is-web-scraping"><i class="fa fa-check"></i><b>2.1</b> What is Web Scraping</a><ul>
<li class="chapter" data-level="2.1.1" data-path="scraping.html"><a href="scraping.html#immobiliare.it-webscraping-website-structurewebstructure"><i class="fa fa-check"></i><b>2.1.1</b> Immobiliare.it Webscraping website <span>structure{@webstructure</span>}</a></li>
<li class="chapter" data-level="2.1.2" data-path="scraping.html"><a href="scraping.html#immobiliare.it-webscraping-content-architecture-with-rvest"><i class="fa fa-check"></i><b>2.1.2</b> Immobiliare.it Webscraping content architecture with <code>rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#best-practices"><i class="fa fa-check"></i><b>2.2</b> Scraping Best Practices and Robottxt</a></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#user-agents-proxies-handlers"><i class="fa fa-check"></i><b>2.3</b> User agents, Proxies, Handlers</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scraping.html"><a href="scraping.html#spoofing"><i class="fa fa-check"></i><b>2.3.1</b> User agents Spoofing</a></li>
<li class="chapter" data-level="2.3.2" data-path="scraping.html"><a href="scraping.html#handlers"><i class="fa fa-check"></i><b>2.3.2</b> Handlers</a></li>
<li class="chapter" data-level="2.3.3" data-path="scraping.html"><a href="scraping.html#parallel-computing"><i class="fa fa-check"></i><b>2.3.3</b> Parallel Computing</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#challenges"><i class="fa fa-check"></i><b>2.4</b> Further Improvements</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#legal-challenges-ancora-non-validato"><i class="fa fa-check"></i><b>2.5</b> Legal Challenges (ancora non validato)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> Infrastructure</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>3.2</b> Docker</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-docker"><i class="fa fa-check"></i><b>3.2.1</b> What is Docker</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#why-docker"><i class="fa fa-check"></i><b>3.2.2</b> Why Docker</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.3</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api"><i class="fa fa-check"></i><b>3.3</b> API</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>3.3.1</b> Plumber API</a></li>
<li class="chapter" data-level="3.3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#immobiliare.it-http-api"><i class="fa fa-check"></i><b>3.3.2</b> Immobiliare.it HTTP API</a></li>
<li class="chapter" data-level="3.3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api-source-code"><i class="fa fa-check"></i><b>3.3.3</b> API source code</a></li>
<li class="chapter" data-level="3.3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#api-documentation"><i class="fa fa-check"></i><b>3.3.4</b> API documentation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>3.4</b> NGINX reverse proxy server</a></li>
<li class="chapter" data-level="3.5" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>3.5</b> AWS EC2 server</a><ul>
<li class="chapter" data-level="3.5.1" data-path="Infrastructure.html"><a href="Infrastructure.html#launch-an-ec2-instance"><i class="fa fa-check"></i><b>3.5.1</b> Launch an EC2 instance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>4</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="exploratory.html"><a href="exploratory.html#what-data-is"><i class="fa fa-check"></i><b>4.1</b> What data is</a></li>
<li class="chapter" data-level="4.2" data-path="exploratory.html"><a href="exploratory.html#data-glimpse"><i class="fa fa-check"></i><b>4.2</b> Data Glimpse</a></li>
<li class="chapter" data-level="4.3" data-path="exploratory.html"><a href="exploratory.html#explorative-analysis"><i class="fa fa-check"></i><b>4.3</b> Explorative Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="exploratory.html"><a href="exploratory.html#semivariogram-covariogram"><i class="fa fa-check"></i><b>4.3.1</b> Semivariogram Covariogram</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="exploratory.html"><a href="exploratory.html#gaussian-random-fields"><i class="fa fa-check"></i><b>4.4</b> Gaussian random fields</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inla-spde.html"><a href="inla-spde.html"><i class="fa fa-check"></i><b>5</b> INLA bayesian computation</a><ul>
<li class="chapter" data-level="5.1" data-path="inla-spde.html"><a href="inla-spde.html#lgm-latent-gaussian-models"><i class="fa fa-check"></i><b>5.1</b> LGM Latent Gaussian Models:</a></li>
<li class="chapter" data-level="5.2" data-path="inla-spde.html"><a href="inla-spde.html#latent-gaussian-models-paula-moraga"><i class="fa fa-check"></i><b>5.2</b> Latent Gaussian Models Paula Moraga</a></li>
<li class="chapter" data-level="5.3" data-path="inla-spde.html"><a href="inla-spde.html#lgm-blangiardo"><i class="fa fa-check"></i><b>5.3</b> LGM Blangiardo</a></li>
<li class="chapter" data-level="5.4" data-path="inla-spde.html"><a href="inla-spde.html#laplace-approximation"><i class="fa fa-check"></i><b>5.4</b> Laplace Approximation</a></li>
<li class="chapter" data-level="5.5" data-path="inla-spde.html"><a href="inla-spde.html#latentGaussian"><i class="fa fa-check"></i><b>5.5</b> Latent Gaussian Models</a></li>
<li class="chapter" data-level="5.6" data-path="inla-spde.html"><a href="inla-spde.html#approximate-bayesian-inference-with-inla"><i class="fa fa-check"></i><b>5.6</b> Approximate Bayesian inference with INLA</a></li>
<li class="chapter" data-level="5.7" data-path="inla-spde.html"><a href="inla-spde.html#the-projector-matrix"><i class="fa fa-check"></i><b>5.7</b> The Projector Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>6</b> Point Referenced Data Modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>6.1</b> Gaussian Process (GP)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="prdm.html"><a href="prdm.html#stationarity-in-gp"><i class="fa fa-check"></i><b>6.1.1</b> Stationarity in GP</a></li>
<li class="chapter" data-level="6.1.2" data-path="prdm.html"><a href="prdm.html#isotropy-in-gp"><i class="fa fa-check"></i><b>6.1.2</b> Isotropy in GP</a></li>
<li class="chapter" data-level="6.1.3" data-path="prdm.html"><a href="prdm.html#Matern"><i class="fa fa-check"></i><b>6.1.3</b> Matérn covariance function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="prdm.html"><a href="prdm.html#hedonic-class-models-for-rental-house-market"><i class="fa fa-check"></i><b>6.2</b> Hedonic class models for Rental House Market</a></li>
<li class="chapter" data-level="6.3" data-path="prdm.html"><a href="prdm.html#regression-for-univariate-spatial-data"><i class="fa fa-check"></i><b>6.3</b> Regression for univariate spatial data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="prdm.html"><a href="prdm.html#parameter-estimation"><i class="fa fa-check"></i><b>6.3.1</b> Parameter estimation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="prdm.html"><a href="prdm.html#hierarchical-bayesian-regression"><i class="fa fa-check"></i><b>6.4</b> Hierarchical Bayesian Regression</a></li>
<li class="chapter" data-level="6.5" data-path="prdm.html"><a href="prdm.html#inla-as-a-hierarchical-model-va-parafrasato"><i class="fa fa-check"></i><b>6.5</b> INLA as a hierarchical model [va parafrasato]</a></li>
<li class="chapter" data-level="6.6" data-path="prdm.html"><a href="prdm.html#spatial-kriging"><i class="fa fa-check"></i><b>6.6</b> Spatial Kriging</a></li>
<li class="chapter" data-level="6.7" data-path="prdm.html"><a href="prdm.html#model-checking-and-comparison"><i class="fa fa-check"></i><b>6.7</b> Model Checking and Comparison</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>7</b> Shiny Web App</a><ul>
<li class="chapter" data-level="7.1" data-path="application.html"><a href="application.html#example-one"><i class="fa fa-check"></i><b>7.1</b> Example one</a></li>
<li class="chapter" data-level="7.2" data-path="application.html"><a href="application.html#example-two"><i class="fa fa-check"></i><b>7.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>8</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inla-spde" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> INLA bayesian computation</h1>
<p>INLA <span class="citation">(<span class="citeproc-not-found" data-reference-id="Rue2009"><strong>???</strong></span>)</span> stands for Integrated Nested Laplace approximation and constitutes an computational alternative to traditional MCMC method. INLA does approximate bayesian inference on special type of models called LGM latent gaussian models due to the fact that they are <em>computationally</em> convenient. The benefits are many:
- low computational costs, even for large models-
- it provides high accuracy
- can define very comolex models within that framework
- most important statistical models are LGM (latent gaussian models)
- very good support for spatial models
[ puoi parlare del big-O problem, puoi pa]</p>
<p>INLA uses a combiation onf analyticsa approximations and nuemrical integration to obtain an approximated prosetrior distibution of the parameters faster.
the combiantion of INLA and SPDE (stochatsic partial diferential equations) allwos to analyze point level data.</p>
<div id="lgm-latent-gaussian-models" class="section level2">
<h2><span class="header-section-number">5.1</span> LGM Latent Gaussian Models:</h2>
<p>Given some observations <span class="math inline">\(y_{i}\)</span> that follows <em>exponential famoily</em> (gaussian poisson edxponential) this observation follows a distribution that depends on some parameters and hyperparameteers. Additionally to that there exists some non gaussian hyper paramters (even though they could also be gaussian too) for which it is possibile to specify a prior. Essentially this is the definition a LGM whose general expression can be simplyfied as:</p>
<p><span class="math display">\[
\underbrace{\pi(\theta)}_{\text {prior }} \times \underbrace{\pi(\boldsymbol{x} \mid \boldsymbol{\theta})}_{\text {GMRF }} \times \underbrace{\prod_{i} \pi\left(y_{i} \mid x_{i}, \boldsymbol{\theta}\right)}_{\text {likelihood }}
\]</span></p>
<p>There is a lot of structure, there are some parameters that are non gaussian but ae very low in dimension, the gaussian part generally is high in dimension and the likelihood fits data.
the main part at this point is to compute margianl distribution.
<span class="math inline">\(\left\{\pi\left(x_{j} \mid \boldsymbol{y}\right)\right\} \quad\)</span> and <span class="math inline">\(\quad\left\{\pi\left(\theta_{k} \mid \boldsymbol{y}\right)\right\}\)</span>
This is going to be solved by using Integrated nested laplace approxiamtion.
Many models are essentially in the cluster of LGM. An important remark regards the fact that LGM models are a way to computes inference, not a way to model data.</p>
</div>
<div id="latent-gaussian-models-paula-moraga" class="section level2">
<h2><span class="header-section-number">5.2</span> Latent Gaussian Models Paula Moraga</h2>
<p>Given some observations <span class="math inline">\(y_{i}\)</span> that follow an <em>exponential family</em> (Gaussian, Poisson, Exponential..) whose probability distribution depends on some parameters, hyper-parameters <span class="math inline">\(y_{i} \mid \boldsymbol{x}, \boldsymbol{\theta} \sim \pi\left(y_{i} \mid x_{i}, \boldsymbol{\theta}\right) i=1, \ldots, n\)</span> and have a mean moment as such:<span class="math inline">\(\mu_{i}=g^{-1}\left(\eta_{i}\right)\)</span>. And then given some parameters <span class="math inline">\(\boldsymbol{x}\)</span> that follow a gaussian distribution with mean <span class="math inline">\(\boldsymbol{\mu}\)</span> and precision matrix <span class="math inline">\(\boldsymbol{Q}(\theta)^{-1}\)</span> that depends only on hyperparameter. <span class="math inline">\(\boldsymbol{x} \mid \boldsymbol{\theta} \sim N\left(\boldsymbol{\mu}(\theta), \boldsymbol{Q}(\theta)^{-1}\right)\)</span>. And then ultimately also given the hyper-parameters that follow a non-gaussian (but it can be also gaussian) probability distribution for which it is possible to specify a prior. Thus if the specified model can be rewritten under these terms, the model take the name of Latent Gaussian Model.
As a side note Some people choose to approach the precision matrix as a covariance function this is not encpuraged since complciates notation. However This can not be done later in thre anlyssy since Some special covariance fucntion is goint ti be itnroduced.
The lienar predictor that sums up the information to include the model can be written as: <span class="math inline">\(\eta_{i}=\alpha+\sum_{k=1}^{n_{\beta}} \beta_{k} z_{k i}+\sum_{j=1}^{n_{f}} f^{(j)}\left(u_{j i}\right)\)</span>
The linear prdictor can be expressed as a sum of <span class="math inline">\(\aplha\)</span> is gen intercept <span class="math inline">\(\beta\)</span> are the linar effect on covariates <span class="math inline">\(z_{ki}\)</span> on the response nad <span class="math inline">\(f\)</span> are a set of random effecct defined in terms os some covariates <span class="math inline">\(u_{ji}\)</span> (e.g. rw, ar1)</p>
<p>INla comoute the posterdior marignals for the LGF x the and the hyper param</p>
<p><span class="math display">\[
\pi\left(x_{i} \mid \boldsymbol{y}\right)=\int \pi\left(x_{i} \mid \boldsymbol{\theta}, \boldsymbol{y}\right) \pi(\boldsymbol{\theta} \mid \boldsymbol{y}) d \boldsymbol{\theta}, \pi\left(\theta_{j} \mid \boldsymbol{y}\right)=\int \pi(\boldsymbol{\theta} \mid \boldsymbol{y}) d \boldsymbol{\theta}_{-j}
\]</span></p>
<p>posetriors have some integrals and ther terms insiede the i tegrals are approxiamted using la opace approximation, thye can be integrate wrt to latent parametnes. you do not need to use thsi,
once you hqve the posteriro distribution you can</p>
</div>
<div id="lgm-blangiardo" class="section level2">
<h2><span class="header-section-number">5.3</span> LGM Blangiardo</h2>
<p>Following the notation imposed in <span class="citation">(Marta Blangiardo <a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span> and in <span class="citation">(Moraga <a href="#ref-Moraga2019" role="doc-biblioref">2019</a>)</span> a reversed approach might better offer the intuition. In order to define a Latent Gaussain Model Within the bayesian framework it is convenient to specify at first, given some observations <span class="math inline">\(y_{i \ldots n}\)</span>, an <em>exponential family</em> (Gaussian Poisson Exponential) distribution function characterized by some parameters <span class="math inline">\(\phi_{i}\)</span> (usually expressed by the mean <span class="math inline">\(\left.E\left(y_{i}\right)\right\)</span>) and some other hyper-parameters <span class="math inline">\(\psi_{k}\)</span>. The parameter <span class="math inline">\(\phi_{i}\)</span> can be defined as an additive linear predictor <span class="math inline">\(\eta_{i}\)</span> by a link function <span class="math inline">\(g(\cdot)\)</span>, i.e. <span class="math inline">\(g\left(\phi_{i}\right)=\eta_{i}\)</span>. A comprehensive expression of the linear predictor take into account all the possible effects:</p>
<p><span class="math display">\[
\eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)
\]</span></p>
<p>where <span class="math inline">\(\beta_{0}\)</span> is the intercept, <span class="math inline">\(\boldsymbol{\beta}=\left\{\beta_{1}, \ldots, \beta_{M}\right\}\)</span> are the coefficient that quantifies the linear effects on covariates <span class="math inline">\(\boldsymbol{x}=\left({x}_{1}, \ldots, {x}_{M}\right)\)</span> and <span class="math inline">\(f\)</span> are a set of random effects defined in terms of a set of covariates <span class="math inline">\(\boldsymbol{z}=\left(z_{1}, \ldots, z_{L}\right)\)</span> (e.g. rw, ar1). [estendibile a tanti modelli] . All the latent components can be conveniently grouped into a varibale denoted with <span class="math inline">\(\boldsymbol{\theta}\)</span> such that. <span class="math inline">\(\boldsymbol{\theta}=\left\{\beta_{0}, \beta, f\right\}\)</span> and the same can be done for hyper parameters: . Then the distribution probability conditioned to parameters and hyper parameters <span class="math inline">\(\boldsymbol{\psi}=\left\{\psi_{1}, \ldots, \psi_{K}\right\}\)</span>:</p>
<p><span class="math display">\[y_{i} \mid \boldsymbol{\theta}, \boldsymbol{\psi} \sim \pi\left(y_{i} \mid \boldsymbol{\theta},\boldsymbol{\psi}\right)\]</span>
and Since they are conditionally independent the joint distribution is given by the likelihood:
<span class="math display">\[p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i=1}^{n} p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\]</span>
Each data point is connected to a linear combination of each element in <span class="math inline">\(\boldsymbol{\theta}\)</span> <em>latent field</em>. A multivariate Normal distribution on <span class="math inline">\(\boldsymbol{\theta}\)</span> is imposed centered in 0 with a precision matrix <span class="math inline">\(Q(\psi),\)</span> i.e., <span class="math inline">\(\theta \sim \operatorname{Normal}\left(\mathbf{0}, Q^{-1}(\psi)\right)\)</span> depending on the hyper parameter vector <span class="math inline">\(\boldsymbol{\psi}\)</span>. Therefore the density function is:</p>
<p><span class="math display">\[
p(\theta \mid \psi)=(2 \pi)^{-n / 2}|Q(\psi)|^{1 / 2} \exp \left(-\frac{1}{2} \theta^{\prime} Q(\psi) \theta\right)
\]</span></p>
</div>
<div id="laplace-approximation" class="section level2">
<h2><span class="header-section-number">5.4</span> Laplace Approximation</h2>
<p>An alternative approach to the simulation- based MC integration is analytic approximation with the Laplace method. Suppose we are interested in computing the
following integral:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x=\int \exp (\log f(x)) \mathrm{d} x\]</span></p>
<p>where <span class="math inline">\(f(x)\)</span> is the density function of a random variable X. We represent <span class="math inline">\(log f(x)\)</span> by
means of a Taylor series expansion evaluated in x = x0:</p>
<p><span class="math display">\[\log f(x) \approx \log f\left(x_{0}\right)+\left.\left(x-x_{0}\right) \frac{\partial \log f(x)}{\partial x}\right|_{x=x_{0}}+\left.\frac{\left(x-x_{0}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x_{0}}\]</span></p>
<p>If x0 is set equal to the mode <span class="math inline">\(x∗ = argmax\)</span>, log f(x) then log f(x)<span class="math inline">\(\left.\frac{\partial \log f(x)}{\partial x}\right|_{x=x^{*}}=0\)</span> and the approximation becomes</p>
<p><span class="math display">\[\log f(x) \approx \log f\left(x^{*}\right)+\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\]</span></p>
<p>The integral of interest is then approximated as follows:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x \approx \int \exp \left(\log f\left(x^{*}\right)+\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\right) \mathrm{d} x\]</span></p>
<p><span class="math display">\[=\exp \left(\log f\left(x^{*}\right)\right) \int \exp \left(\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\right) \mathrm{d} x\]</span></p>
<p>where the integrand can be associated with the density of a Normal distribution. In
fact, by setting <span class="math display">\[\sigma^{2 *}=-1 /\left.\frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\]</span> we obtain:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x \approx \exp \left(\log f\left(x^{*}\right)\right) \int \exp \left(-\frac{\left(x-x^{*}\right)^{2}}{2 \sigma^{2 *}}\right) \mathrm{d} x\]</span></p>
<p>where the integrand is the kernel of a Normal distribution with mean equal to x∗
and variance <span class="math inline">\(\sigma^{2*}\)</span>. More precisely, the integral evaluated in the interval <span class="math inline">\((\alpha, \beta)\)</span> is
approximated by:</p>
<p><span class="math display">\[\int_{\alpha}^{\beta} f(x) \mathrm{d} x \approx f\left(x^{*}\right) \sqrt{2 \pi \sigma^{2 *}}(\Phi(\beta)-\Phi(\alpha))\]</span></p>
<p>where <span class="math inline">\(\Phi(⋅)\)</span> denotes the cumulative density function of th <span class="math inline">\(Normal(x_i, \sigma^{2*})\)</span> distribution.</p>
<p>as an example this a logistic regression xample bivariate noramle we have wo obsservation we just wna ot compute the marginal essentially whe ahv to approxiamte this integral, so what the cpmputaion twe need to do we chose a value for x 1 sand the we approxaimte thsi inegral this is how w e itntegratem to do we have to comoute the mode ant the curvature and then use laplace spproximation</p>
<p>[couple figures]</p>
<p>the point now is if you want ot have you compatiin to be smooth not any gaussian</p>
</div>
<div id="latentGaussian" class="section level2">
<h2><span class="header-section-number">5.5</span> Latent Gaussian Models</h2>
<p>The first step in defining a latent Gaussian model within the Bayesian framework
is to identify a distribution for the observed data y = (y1, … , yn). A very general
approach consists in specifying a distribution for yi characterized by a parameter
𝜙i (usually the mean E(yi)) defined as a function of a structured additive predictor
𝜂i through a link function g(⋅), such that g(𝜙i) = 𝜂i. The additive linear predictor 𝜂i
is defined as follows:</p>
<p><span class="math display">\[\eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)\]</span></p>
<p>Here 𝛽0 is a scalar representing the intercept; the coefficients <span class="math inline">\(\boldsymbol{\beta}=\left\{\beta_{1}, \ldots, \beta_{M}\right\}\)</span>
quantify the (linear) effect of some covariates <span class="math inline">\(\boldsymbol{x}=\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{M}\right)\)</span> on the response; and
<span class="math inline">\(f=\left\{f_{1}(\cdot), \ldots, f_{L}(\cdot)\right\}\)</span> is a collection of fmunctions defined in terms of a set of covariates <span class="math inline">\(z = (z_1, … , z_L)\)</span>. The terms <span class="math inline">\(f_l(⋅)\)</span> can assume different forms such as smooth and</p>
<p>nonlinear effects of covariates, time trends and seasonal effects, random intercept
and slopes as well as temporal or spatial random effects. For this reason, the class of
latent Gaussian models is very flexible and can accomodate a wide range of models
ranging from generalized and dynamic linear models to spatial and spatio-temporal
models (see Martins et al., 2013 for a review).
We collect all the latent (nonobservable) components of interest for the inference
in a set of parameters named 𝜽 defined as <span class="math inline">\(\boldsymbol{\theta}=\left\{\beta_{0}, \boldsymbol{\beta}, \boldsymbol{f}\right\}\)</span>. Moreover, we denote with
<span class="math inline">\(\psi=\left\{\psi_{1}, \ldots, \psi_{K}\right\}\)</span> the vector of the K hyperparameters. By assuming conditional
independence, the distribution of the n observations (all coming from the same
distribution family) is given by the likelihood</p>
<p><span class="math display">\[p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i=1}^{n} p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\]</span></p>
<p>where each data point yi is connected to only one element 𝜃i in the latent field <span class="math inline">\(\theta\)</span>.
Martins et al. (2013) discuss the possibility of relaxing this assumption assuming
that each observation may be connected with a linear combination of elements in
<span class="math inline">\(\theta\)</span>; moreover, they take into account the case when the data belong to several distri-
butions, i.e., the multiple likelihoods case.</p>
<p>We assume a multivariate Normal prior on 𝜽 with mean 𝟎 and precision matrix
<span class="math inline">\(Q(\psi)\)</span>, i.e., <span class="math inline">\(\boldsymbol{\theta} \sim \operatorname{Normal}\left(\mathbf{0}, \boldsymbol{Q}^{-1}(\boldsymbol{\psi})\right)\)</span> with density function given by</p>
<p><span class="math display">\[p(\theta \mid \psi)=(2 \pi)^{-n / 2}|Q(\psi)|^{1 / 2} \exp \left(-\frac{1}{2} \theta^{\prime} Q(\psi) \theta\right)\]</span></p>
<p>where | ⋅ | denotes the matrix determinant and ′ is used for the transpose operation.
The components of the latent Gaussian field 𝜽 are supposed to be conditionally
independent with the consequence that <span class="math inline">\(Q(\psi)\)</span> is a sparse precision matrix.8 This
specification is known as Gaussian Markov random field (GMRF, Rue and Held,
2005). Note that the sparsity of the precision matrix gives rise to computational ben-
efits when making inference with GMRFs. In fact, linear algebra operations can be
performed using numerical methods for sparse matrices, resulting in a considerable
computational gain (see Rue and Held, 2005 for algorithms).
The joint posterior distribution of 𝜽 and <span class="math inline">\(\psi\)</span> is given by the product of the likelihood
(4.13), of the GMRF density (4.14) and of the hyperparameter prior distribution
<span class="math inline">\(p(\psi)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned} p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y}) &amp; \propto p(\boldsymbol{\psi}) \times p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) \\ &amp; \propto p(\boldsymbol{\psi}) \times p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \prod_{i=1}^{n} p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right) \\ &amp; \propto p(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}\right) \times \prod_{i=1}^{n} \exp \left(\log \left(p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right) \\ &amp; \propto p(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}+\sum_{i=1}^{n} \log \left(p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right) \end{aligned}
\]</span></p>
</div>
<div id="approximate-bayesian-inference-with-inla" class="section level2">
<h2><span class="header-section-number">5.6</span> Approximate Bayesian inference with INLA</h2>
<p>The objectives of Bayesian inference are the marginal posterior distributions for
each element of the parameter vector</p>
<p><span class="math display">\[p\left(\theta_{i} \mid \boldsymbol{y}\right)=\int p\left(\theta_{i}, \boldsymbol{\psi} \mid \boldsymbol{y}\right) \mathrm{d} \boldsymbol{\psi}=\int p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) p(\boldsymbol{\psi} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\psi}\]</span></p>
<p>and for each element of the hyperparameter vector</p>
<p><span class="math display">\[p\left(\psi_{k} \mid \boldsymbol{y}\right)=\int p(\boldsymbol{\psi} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\psi}_{-k}\]</span></p>
<p>Thus, we need to perform the following tasks:
(i) compute <span class="math inline">\(p(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span>, from which also all the relevant marginals p(𝜓k|y) can be
obtained;
(ii) compute <span class="math inline">\(p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> which is needed to compute the parameter marginal posteriors <span class="math inline">\(p\left(\theta_{i} \mid \boldsymbol{y}\right)\)</span></p>
<p>The INLA approach exploits the assumptions of the model to produce a numerical
approximation to the posteriors of interest based on the Laplace approximation
method introduced in Section 4.7 (Tierney and Kadane, 1986).
The first task (i) consists of the computation of an approximation to the joint
posterior of the hyperparameters as:</p>
<p><span class="math display">\[\begin{aligned} p(\boldsymbol{\psi} \mid \boldsymbol{y}) &amp;=\frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;=\frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta}, \boldsymbol{\psi})}{p(\boldsymbol{y})} \frac{1}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;=\frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{p(\boldsymbol{y})} \frac{1}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp; \propto \frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;\left.\approx \frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{\tilde{p}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{*} \boldsymbol{\psi}}=: \tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y}) \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tilde{p}(\theta \mid \psi, y)\)</span> is the Gaussian approximation – given by the Laplace method – of <span class="math inline">\(p(\theta \mid \psi, y)\)</span> and <span class="math inline">\(\theta^{*}(\psi)\)</span> is the mode for a given <span class="math inline">\(\psi\)</span> the Gaussian approximation turns out to be accurate since <span class="math inline">\(p(\theta \mid \psi, y)\)</span>appears to be almost Gaussian as it is a priori dis-
tributed like a GMRF, y is generally not informative and the observation distribution
is usually well-behaved.
The second task (ii) is slightly more complex, because in general there will
be more elements in 𝜽 than in 𝝍, and thus this computation is more expensive.
A first easy possibility is to approximate the posterior conditional distributions
<span class="math inline">\(p(\theta \mid \psi, y)\)</span> directly as the marginals from <span class="math inline">\(\tilde{p}(\theta \mid \psi, y)\)</span> i.e. using a Normal distribution, where the Cholesky decomposition is used for the precision matrix (Rue
and Martino, 2007). While this is very fast, the approximation is generally
not very good. The second possibility is to rewrite the vector of parameters as
<span class="math inline">\(\theta=\left(\theta_{i}, \theta_{-i}\right)\)</span> and use again Laplace approximation to obtain</p>
<p><span class="math display">\[\begin{aligned} p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) &amp;=\frac{p\left(\left(\theta_{i}, \boldsymbol{\theta}_{-i}\right) \mid \boldsymbol{\psi}, \boldsymbol{y}\right)}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp;=\frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p(\boldsymbol{\psi} \mid \boldsymbol{y})} \frac{1}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp; \propto \frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp;\left.\approx \frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{\tilde{p}\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)}\right|_{\boldsymbol{\theta}_{-i}=\boldsymbol{\theta}_{-i}^{*}\left(\theta_{i}, \boldsymbol{\psi}\right)}=: \tilde{p}\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tilde{p}\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> is the Laplace Gaussian approximation to <span class="math inline">\(p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> and <span class="math inline">\(\boldsymbol{\theta}_{-i}^{*}\left(\theta_{i}, \boldsymbol{\psi}\right)\)</span> is its mode. Because the random variables <span class="math inline">\(\theta_{-i} \mid \theta_{i}, \psi, y\)</span> re in general
reasonably Normal, the approximation provided by (4.20) typically works very
well. This strategy, however, can be very expensive in computational terms as <span class="math inline">\(\tilde{p}\left(\theta_{-i} \mid \theta_{i}, \psi, y\right)\)</span> must be recomputed for each value of 𝜽 and 𝝍 (some modifications
to the Laplace approximation in order to reduce the computational costs are
described in Rue et al., 2009).</p>
<p>Operationally, INLA proceeds as follows:
(i) first it explores the hyperparameter joint posterior distribution <span class="math inline">\(\tilde{p}(\psi \mid \boldsymbol{y})\)</span> of Eq. (4.18) in a nonparametric way, in order to detect good points <span class="math inline">\(\left\{\psi^{(j)}\right\}\)</span> for
the numerical integration required in Eq. (4.22). Rue et al. (2009) propose
two different exploration schemes, both requiring a reparameterization of
the <span class="math inline">\(\psi\)</span>-space – in order to deal with more regular densities – through the
following steps:</p>
<ol style="list-style-type: lower-alpha">
<li>Locate the mode <span class="math inline">\(\psi^{*}\)</span> of <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> by optimizing log <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> with respect to
𝝍 (e.g., through the Newton–Raphson method).</li>
<li>Compute the negative Hessian <span class="math inline">\(H\)</span> at the modal configuration.</li>
<li>Compute the eigen-decomposition <span class="math inline">\(\mathbf{\Sigma}=\boldsymbol{V} \Lambda^{1 / 2} \boldsymbol{V}^{\prime}\)</span>, with <span class="math inline">\(\Sigma=H^{-1}\)</span>.</li>
<li>Define the new variable z, with standardized and mutually orthogonal
components, such that:</li>
</ol>
<p><span class="math display">\[\boldsymbol{\psi}(z)=\boldsymbol{\psi}^{*}+\boldsymbol{V} \Lambda^{1 / 2} z\]</span></p>
<p>The first exploration scheme (named grid strategy) builds, using the
z-parameterization, a grid of points associated with the bulk of the mass of <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> . This approach has a computational cost which grows exponentially
with the number of hyperparameters; therefore the advice is to adopt it
when K, the dimension of <span class="math inline">\(\psi\)</span>, is lower than 4. Otherwise, the second explo-
ration scheme, named central composite design (CCD) strategy, should be
used as it reduces the computational costs. With the CCD approach, the
integration problem is seen as a design problem; using the mode <span class="math inline">\(\psi^{*}\)</span> and the
Hessian H, some relevant points in the 𝝍-space are selected for performing
a second-order approximation to a response variable (see Section 6.5 of
Rue et al., 2009 for details). In general, the CCD strategy uses much less
points, but still is able to capture the variability of the hyperparameter
distribution. For this reason it is the default option in R-INLA.</p>
</div>
<div id="the-projector-matrix" class="section level2">
<h2><span class="header-section-number">5.7</span> The Projector Matrix</h2>
<p>We need to construct a projection matrix <strong>A</strong> to project the GRF from the observations to thetriangulation vertices. The matrix <strong>A</strong> as the number of rows equal to the number of observations, and the number of columns equal to the number of vertices of the triangulation. Row <span class="math inline">\(i\)</span> of <strong>A</strong> corresponding to an observation at location <span class="math inline">\(s_{i}\)</span> ossibly has three non-zero values at the columns that correspond to the vertices of the triangle that contains the location. If <span class="math inline">\(s_{i}\)</span> within the triangle, these values are equal to the barycentric coordinates. That is, they are proportional to the areas of each of the three subtriangles defined by the location <span class="math inline">\(s_{i}\)</span>
and the triangle’s vertices, and sum to 1. If <span class="math inline">\(s_{i}\)</span> s equal to a vertex of the triangle, row<br />
<span class="math inline">\(i\)</span> has just one non-zero value equal to 1 at the column that corresponds to the vertex. Intuitively, the value <span class="math inline">\(Z(\boldsymbol{s})\)</span> at a location that lies within one triangle is the projection of the plane formed by the triangle vertices weights at location <span class="math inline">\(s\)</span> .</p>
<p>An example of a projection matrix is given below. This projection matrix projects <span class="math inline">\(n\)</span> observations to <span class="math inline">\(G\)</span> triangulation vertices. The first row of the matrix corresponds to an observation with location that coincides with vertex number 3. The second and last rows correspond to observations with locations lying within triangles.</p>
<p><span class="math inline">\(4=\left[\begin{array}{ccccc}A_{11} &amp; A_{12} &amp; A_{13} &amp; \ldots &amp; A_{1 G} \\ A_{21} &amp; A_{22} &amp; A_{23} &amp; \ldots &amp; A_{2 G} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ A_{n 1} &amp; A_{n 2} &amp; A_{n 3} &amp; \ldots &amp; A_{n G}\end{array}\right]=\left[\begin{array}{ccccc}0 &amp; 0 &amp; 1 &amp; \ldots &amp; 0 \\ A_{21} &amp; A_{22} &amp; 0 &amp; \ldots &amp; A_{2 G} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ A_{n 1} &amp; A_{n 2} &amp; A_{n 3} &amp; \ldots &amp; 0\end{array}\right]\)</span></p>
<p>igure 8.6 shows a location <span class="math inline">\(s\)</span> that lies within one of the triangles of a triangulated mesh. The value of the process <span class="math inline">\(Z(\cdot)\)</span> at <span class="math inline">\(s\)</span> is expressed as a weighted average of the values of the process at the vertices of the triangle (<span class="math inline">\(Z_{1}\)</span>, <span class="math inline">\(Z_{2}\)</span> and <span class="math inline">\(Z_{3}\)</span>)and with weights equal to
<span class="math inline">\(T_{1} / T\)</span>, <span class="math inline">\(T_{2} / T\)</span> and <span class="math inline">\(T_{3} / T\)</span> where <span class="math inline">\(T\)</span>denotes the area of the big triangle that contains <span class="math inline">\(s\)</span> and <span class="math inline">\(T_{1}\)</span>, <span class="math inline">\(T_{2}\)</span> and <span class="math inline">\(T_{3}\)</span> are the areas of the subtriangles</p>
<p><span class="math inline">\(Z(\boldsymbol{s}) \approx \frac{T_{1}}{T} Z_{1}+\frac{T_{2}}{T} Z_{2}+\frac{T_{3}}{T} Z_{3}\)</span></p>
<p><img src="images/triangle-mesh-1.png" alt="triangle-mesh-1" />
<code>R-INLA</code> provides the <code>inla.spde.make.A()</code> function to easily construct a projection matrix <strong>A</strong>
We create the projection matrix of our example by using <code>inla.spde.make.A()</code> passing the triangulated mesh <code>mesh</code> and the coordinates <code>coo</code>.</p>
<pre><code>A &lt;- inla.spde.make.A(mesh = mesh, loc = coo)</code></pre>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Blangiardo-Cameletti">
<p>Marta Blangiardo, Michela Cameletti. 2015. <em>Spatial and Spatio-Temporal Bayesian Models with R-Inla</em>. Wiley.</p>
</div>
<div id="ref-Moraga2019">
<p>Moraga, Paula. 2019. <em>Geospatial Health Data</em>. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9780429341823">https://doi.org/10.1201/9780429341823</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exploratory.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="prdm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/Thesis/edit/master/05-inla_spde.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf", "Niccolo_Salvini_Thesis.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
