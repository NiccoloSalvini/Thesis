<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 INLA bayesian computation | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</title>
  <meta name="description" content="Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 INLA bayesian computation | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />
  <meta property="og:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 INLA bayesian computation | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA" />
  
  <meta name="twitter:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="exploratory.html"/>
<link rel="next" href="prdm.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"> End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#what-is-web-scraping"><i class="fa fa-check"></i><b>2.1</b> What is Web Scraping</a><ul>
<li class="chapter" data-level="2.1.1" data-path="scraping.html"><a href="scraping.html#immobiliare.it-webscraping-website-structurewebstructure"><i class="fa fa-check"></i><b>2.1.1</b> Immobiliare.it Webscraping website <span>structure{@webstructure</span>}</a></li>
<li class="chapter" data-level="2.1.2" data-path="scraping.html"><a href="scraping.html#immobiliare.it-webscraping-content-architecture-with-rvest"><i class="fa fa-check"></i><b>2.1.2</b> Immobiliare.it Webscraping content architecture with <code>rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#best-practices"><i class="fa fa-check"></i><b>2.2</b> Scraping Best Practices and Robottxt</a></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#user-agents-proxies-handlers"><i class="fa fa-check"></i><b>2.3</b> User agents, Proxies, Handlers</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scraping.html"><a href="scraping.html#spoofing"><i class="fa fa-check"></i><b>2.3.1</b> User agents Spoofing</a></li>
<li class="chapter" data-level="2.3.2" data-path="scraping.html"><a href="scraping.html#handlers"><i class="fa fa-check"></i><b>2.3.2</b> Handlers</a></li>
<li class="chapter" data-level="2.3.3" data-path="scraping.html"><a href="scraping.html#parallel-computing"><i class="fa fa-check"></i><b>2.3.3</b> Parallel Computing</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#challenges"><i class="fa fa-check"></i><b>2.4</b> Further Improvements</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#legal-challenges-ancora-non-validato"><i class="fa fa-check"></i><b>2.5</b> Legal Challenges (ancora non validato)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> Infrastructure</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>3.2</b> Docker</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-docker"><i class="fa fa-check"></i><b>3.2.1</b> What is Docker</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#why-docker"><i class="fa fa-check"></i><b>3.2.2</b> Why Docker</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.3</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api"><i class="fa fa-check"></i><b>3.3</b> API</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>3.3.1</b> Plumber API</a></li>
<li class="chapter" data-level="3.3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#immobiliare.it-http-api"><i class="fa fa-check"></i><b>3.3.2</b> Immobiliare.it HTTP API</a></li>
<li class="chapter" data-level="3.3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api-source-code"><i class="fa fa-check"></i><b>3.3.3</b> API source code</a></li>
<li class="chapter" data-level="3.3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#api-documentation"><i class="fa fa-check"></i><b>3.3.4</b> API documentation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>3.4</b> NGINX reverse proxy server</a></li>
<li class="chapter" data-level="3.5" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>3.5</b> AWS EC2 server</a><ul>
<li class="chapter" data-level="3.5.1" data-path="Infrastructure.html"><a href="Infrastructure.html#launch-an-ec2-instance"><i class="fa fa-check"></i><b>3.5.1</b> Launch an EC2 instance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>4</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="exploratory.html"><a href="exploratory.html#what-data-is"><i class="fa fa-check"></i><b>4.1</b> What data is</a></li>
<li class="chapter" data-level="4.2" data-path="exploratory.html"><a href="exploratory.html#data-glimpse"><i class="fa fa-check"></i><b>4.2</b> Data Glimpse</a></li>
<li class="chapter" data-level="4.3" data-path="exploratory.html"><a href="exploratory.html#explorative-analysis"><i class="fa fa-check"></i><b>4.3</b> Explorative Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="exploratory.html"><a href="exploratory.html#semivariogram-covariogram"><i class="fa fa-check"></i><b>4.3.1</b> Semivariogram Covariogram</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="exploratory.html"><a href="exploratory.html#gaussian-random-fields"><i class="fa fa-check"></i><b>4.4</b> Gaussian random fields</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inla-spde.html"><a href="inla-spde.html"><i class="fa fa-check"></i><b>5</b> INLA bayesian computation</a><ul>
<li class="chapter" data-level="5.1" data-path="inla-spde.html"><a href="inla-spde.html#latent-gaussian-models-lgm"><i class="fa fa-check"></i><b>5.1</b> Latent Gaussian Models LGM</a></li>
<li class="chapter" data-level="5.2" data-path="inla-spde.html"><a href="inla-spde.html#approximation-in-inla-setting"><i class="fa fa-check"></i><b>5.2</b> Approximation in INLA setting</a></li>
<li class="chapter" data-level="5.3" data-path="inla-spde.html"><a href="inla-spde.html#laplace-approximation"><i class="fa fa-check"></i><b>5.3</b> Laplace Approximation</a></li>
<li class="chapter" data-level="5.4" data-path="inla-spde.html"><a href="inla-spde.html#approximate-bayesian-inference-with-inla"><i class="fa fa-check"></i><b>5.4</b> Approximate Bayesian inference with INLA</a></li>
<li class="chapter" data-level="5.5" data-path="inla-spde.html"><a href="inla-spde.html#the-projector-matrix"><i class="fa fa-check"></i><b>5.5</b> The Projector Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>6</b> Point Referenced Data Modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>6.1</b> Gaussian Process (GP)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="prdm.html"><a href="prdm.html#stationarity-in-gp"><i class="fa fa-check"></i><b>6.1.1</b> Stationarity in GP</a></li>
<li class="chapter" data-level="6.1.2" data-path="prdm.html"><a href="prdm.html#isotropy-in-gp"><i class="fa fa-check"></i><b>6.1.2</b> Isotropy in GP</a></li>
<li class="chapter" data-level="6.1.3" data-path="prdm.html"><a href="prdm.html#Matern"><i class="fa fa-check"></i><b>6.1.3</b> Matérn covariance function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="prdm.html"><a href="prdm.html#hedonic-class-models-for-rental-house-market"><i class="fa fa-check"></i><b>6.2</b> Hedonic class models for Rental House Market</a></li>
<li class="chapter" data-level="6.3" data-path="prdm.html"><a href="prdm.html#regression-for-univariate-spatial-data"><i class="fa fa-check"></i><b>6.3</b> Regression for univariate spatial data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="prdm.html"><a href="prdm.html#parameter-estimation"><i class="fa fa-check"></i><b>6.3.1</b> Parameter estimation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="prdm.html"><a href="prdm.html#hierarchical-bayesian-regression"><i class="fa fa-check"></i><b>6.4</b> Hierarchical Bayesian Regression</a></li>
<li class="chapter" data-level="6.5" data-path="prdm.html"><a href="prdm.html#inla-as-a-hierarchical-model-va-parafrasato"><i class="fa fa-check"></i><b>6.5</b> INLA as a hierarchical model [va parafrasato]</a></li>
<li class="chapter" data-level="6.6" data-path="prdm.html"><a href="prdm.html#spatial-kriging"><i class="fa fa-check"></i><b>6.6</b> Spatial Kriging</a></li>
<li class="chapter" data-level="6.7" data-path="prdm.html"><a href="prdm.html#model-checking-and-comparison"><i class="fa fa-check"></i><b>6.7</b> Model Checking and Comparison</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>7</b> Shiny Web App</a><ul>
<li class="chapter" data-level="7.1" data-path="application.html"><a href="application.html#example-one"><i class="fa fa-check"></i><b>7.1</b> Example one</a></li>
<li class="chapter" data-level="7.2" data-path="application.html"><a href="application.html#example-two"><i class="fa fa-check"></i><b>7.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>8</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inla-spde" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> INLA bayesian computation</h1>
<p>INLA <span class="citation">(Rue, Martino, and Chopin <a href="#ref-Rue2009" role="doc-biblioref">2009</a>)</span> stands for Integrated Nested Laplace approximation and constitutes an computational alternative to traditional MCMC method. INLA does approximate bayesian inference on special type of models called LGM latent gaussian models due to the fact that they are <em>computationally</em> convenient. The benefits are many:
- low computational costs, even for large models-
- it provides high accuracy
- can define very comolex models within that framework
- most important statistical models are LGM (latent gaussian models)
- very good support for spatial models
[ puoi parlare del big-O problem, puoi pa]</p>
<p>INLA uses a combination of analytics approximations and numerical integration to obtain an approximated posterior distribution of the parameters much faster.
The combination of INLA and SPDE (Stochastic Partial Differential Equations) allows to analyze point level data.</p>
<div id="latent-gaussian-models-lgm" class="section level2">
<h2><span class="header-section-number">5.1</span> Latent Gaussian Models LGM</h2>
<p>Following the notation imposed in <span class="citation">(Marta Blangiardo <a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span> and in <span class="citation">(Moraga <a href="#ref-Moraga2019" role="doc-biblioref">2019</a>)</span> a reversed approach might better offer the intuition. In order to define a Latent Gaussain Model Within the bayesian framework it is convenient to specify at first, given some observations <span class="math inline">\(y_{i \ldots n}\)</span>, an <em>exponential family</em> (Gaussian Poisson Exponential) distribution function characterized by some parameters <span class="math inline">\(\phi_{i}\)</span> (usually expressed by the mean <span class="math inline">\(\left.E\left(y_{i}\right)\right)\)</span>) and some other hyper-parameters <span class="math inline">\(\psi_{k}\)</span>. The parameter <span class="math inline">\(\phi_{i}\)</span> can be defined as an additive latent linear predictor <span class="math inline">\(\eta_{i}\)</span> <span class="citation">(Krainski <a href="#ref-Krainski-Rubio" role="doc-biblioref">2019</a>)</span> by a link function <span class="math inline">\(g(\cdot)\)</span>, i.e. <span class="math inline">\(g\left(\phi_{i}\right)=\eta_{i}\)</span>. A comprehensive expression of the linear predictor take into account all the possible effects:</p>
<p><span class="math display">\[
\eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)
\]</span></p>
<p>where <span class="math inline">\(\beta_{0}\)</span> is the intercept, <span class="math inline">\(\boldsymbol{\beta}=\left\{\beta_{1}, \ldots, \beta_{M}\right\}\)</span> are the coefficient that quantifies the linear effects on covariates <span class="math inline">\(\boldsymbol{x}=\left({x}_{1}, \ldots, {x}_{M}\right)\)</span> and <span class="math inline">\(f\)</span> are a set of random effects defined in terms of a set of covariates <span class="math inline">\(\boldsymbol{z}=\left(z_{1}, \ldots, z_{L}\right)\)</span> (e.g. rw, ar1). [estendibile a tanti modelli] . All the latent components can be conveniently grouped into a varibale denoted with <span class="math inline">\(\boldsymbol{\theta}\)</span> such that. <span class="math inline">\(\boldsymbol{\theta}=\left\{\beta_{0}, \beta, f\right\}\)</span> and the same can be done for hyper parameters: . Then the distribution probability conditioned to parameters and hyper parameters <span class="math inline">\(\boldsymbol{\psi}=\left\{\psi_{1}, \ldots, \psi_{K}\right\}\)</span>:</p>
<p><span class="math display">\[y_{i} \mid \boldsymbol{\theta}, \boldsymbol{\psi} \sim \pi\left(y_{i} \mid \boldsymbol{\theta},\boldsymbol{\psi}\right)\]</span>
Since they are conditionally independent the joint distribution is given by the likelihood:
<span class="math display">\[\pi(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\]</span>
Each data point is connected to a linear combination of each element in <span class="math inline">\(\boldsymbol{\theta}\)</span> <em>latent field</em> . Hyper parameters are going to be ex-ante independent, therefore <span class="math inline">\(\boldsymbol{\theta}\)</span> will be the product of many univariate priors <span class="citation">(Gómez Rubio <a href="#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span>. A multivariate Normal distribution on <span class="math inline">\(\boldsymbol{\theta}\)</span> is imposed centered in 0 with a precision matrix <span class="math inline">\(Q(\psi),\)</span> i.e., <span class="math inline">\(\theta \sim \operatorname{Normal}\left(\mathbf{0}, Q^{-1}(\psi)\right)\)</span> depending on the hyper parameter vector <span class="math inline">\(\boldsymbol{\psi}\)</span>. As side note some authors choose to approach the precision matrix as a covariance function in the setting seen before. This is strongly not encouraged since it compicates notation. However This has to be forcly done in next chpater since a special covariance function is used. The density function is expressed through:</p>
<p><span class="math display">\[
\pi(\theta \mid \psi)=(2 \pi)^{-n / 2}|Q(\psi)|^{1 / 2} \exp \left(-\frac{1}{2} \theta^{\prime} Q(\psi) \theta\right)
\]</span>
Each component of the latent files is supposed to be conditionally independent, this leads to <span class="math inline">\(\boldsymbol{Q(\psi)}\)</span> being a sparse precision matrix, that is called <em>Gaussian Markov random field</em> (<strong>GMRF</strong>). From here comes the computational power inherited usgn latent models, matrices are sparse so numerical methods can be exploited <span class="citation">(Marta Blangiardo <a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span>.
Once priors are specified for <span class="math inline">\(\boldsymbol{\psi}\)</span> then the following holds:</p>
<p><span class="math display">\[
\underbrace{\pi(\boldsymbol{\psi})}_{\text {prior }} \times \underbrace{\pi(\theta \mid \psi)}_{\text {GMRF }} \times \underbrace{\prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)}_{\text {likelihood }}
\]</span>
Then the joint posterior distribution plugging in the likelihood and GMRF:</p>
<p><span class="math display">\[
\begin{aligned}
\pi(\theta, \psi \mid y) &amp; \propto \pi(\psi) \times \pi(\theta \mid \psi) \times \pi(y \mid \theta, \psi) \\
&amp; \propto     \pi(\psi) \times \pi(\theta \mid \psi) \times \prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right) \\
&amp; \propto \pi(\psi) \times|Q(\psi)|^{1 / 2} \exp \left(-\frac{1}{2} \theta^{\prime} Q(\psi) \theta\right) \times \prod_{i=1}^{n} \exp \left(\log \left(\pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right) \\
&amp; \propto \pi(\psi) \times|Q(\psi)|^{1 / 2} \exp \left(-\frac{1}{2} \theta^{\prime} Q(\psi) \theta+\sum_{i=1}^{n} \log \left(\pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right)
\end{aligned}
\]</span></p>
</div>
<div id="approximation-in-inla-setting" class="section level2">
<h2><span class="header-section-number">5.2</span> Approximation in INLA setting</h2>
<p>INLA is not going to try to estimate posterior distribution solving the integral, instead marginals effects and hyper parameters. For latent parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> :</p>
<p><span class="math display">\[
\pi\left( \boldsymbol{\theta}\mid y \right)=\int \pi\left(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \mathbf{y}\right) \pi(\boldsymbol{\psi} \mid \mathbf{y}) d \boldsymbol{\psi}
\]</span></p>
<p>The posterior marginal for the hyoerparameter is <span class="math inline">\(\boldsymbol{\psi}\)</span> since <span class="math inline">\(\boldsymbol{\psi}=\left\{\psi_{1}, \ldots, \psi_{K}\right\}\)</span> :</p>
<p><span class="math display">\[
\pi\left(\psi_{k} \mid y\right)=\int \pi(\boldsymbol{\psi} \mid y) d \psi_{-k}
\]</span>
where <span class="math inline">\(\psi_{-k}\)</span> is a vector of hyper parameters <span class="math inline">\(\psi\)</span> without the element <span class="math inline">\(\psi_{k}\)</span>.</p>
<p>[ DA RIMETTERE DA QUI ]</p>
</div>
<div id="laplace-approximation" class="section level2">
<h2><span class="header-section-number">5.3</span> Laplace Approximation</h2>
<p>An alternative approach to the simulation- based MC integration is analytic approximation with the Laplace method. Suppose we are interested in computing the
following integral:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x=\int \exp (\log f(x)) \mathrm{d} x\]</span></p>
<p>where <span class="math inline">\(f(x)\)</span> is the density function of a random variable X. We represent <span class="math inline">\(log f(x)\)</span> by
means of a Taylor series expansion evaluated in x = x0:</p>
<p><span class="math display">\[\log f(x) \approx \log f\left(x_{0}\right)+\left.\left(x-x_{0}\right) \frac{\partial \log f(x)}{\partial x}\right|_{x=x_{0}}+\left.\frac{\left(x-x_{0}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x_{0}}\]</span></p>
<p>If x0 is set equal to the mode <span class="math inline">\(x∗ = argmax\)</span>, log f(x) then log f(x)<span class="math inline">\(\left.\frac{\partial \log f(x)}{\partial x}\right|_{x=x^{*}}=0\)</span> and the approximation becomes</p>
<p><span class="math display">\[\log f(x) \approx \log f\left(x^{*}\right)+\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\]</span></p>
<p>The integral of interest is then approximated as follows:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x \approx \int \exp \left(\log f\left(x^{*}\right)+\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\right) \mathrm{d} x\]</span></p>
<p><span class="math display">\[=\exp \left(\log f\left(x^{*}\right)\right) \int \exp \left(\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\right) \mathrm{d} x\]</span></p>
<p>where the integrand can be associated with the density of a Normal distribution. In
fact, by setting <span class="math display">\[\sigma^{2 *}=-1 /\left.\frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\]</span> we obtain:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x \approx \exp \left(\log f\left(x^{*}\right)\right) \int \exp \left(-\frac{\left(x-x^{*}\right)^{2}}{2 \sigma^{2 *}}\right) \mathrm{d} x\]</span></p>
<p>where the integrand is the kernel of a Normal distribution with mean equal to x∗
and variance <span class="math inline">\(\sigma^{2*}\)</span>. More precisely, the integral evaluated in the interval <span class="math inline">\((\alpha, \beta)\)</span> is
approximated by:</p>
<p><span class="math display">\[\int_{\alpha}^{\beta} f(x) \mathrm{d} x \approx f\left(x^{*}\right) \sqrt{2 \pi \sigma^{2 *}}(\Phi(\beta)-\Phi(\alpha))\]</span></p>
<p>where <span class="math inline">\(\Phi(⋅)\)</span> denotes the cumulative density function of th <span class="math inline">\(Normal(x_i, \sigma^{2*})\)</span> distribution.</p>
<p>as an example this a logistic regression xample bivariate noramle we have wo obsservation we just wna ot compute the marginal essentially whe ahv to approxiamte this integral, so what the cpmputaion twe need to do we chose a value for x 1 sand the we approxaimte thsi inegral this is how w e itntegratem to do we have to comoute the mode ant the curvature and then use laplace spproximation</p>
<p>[couple figures]</p>
<p>INLA compute the posterior marignals for the LGM:</p>
<p><span class="math display">\[
\pi\left(x_{i} \mid \boldsymbol{y}\right)=\int \pi\left(x_{i} \mid \boldsymbol{\theta}, \boldsymbol{y}\right) \pi(\boldsymbol{\theta} \mid \boldsymbol{y}) d \boldsymbol{\theta}, \pi\left(\theta_{j} \mid \boldsymbol{y}\right)=\int \pi(\boldsymbol{\theta} \mid \boldsymbol{y}) d \boldsymbol{\theta}_{-j}
\]</span></p>
</div>
<div id="approximate-bayesian-inference-with-inla" class="section level2">
<h2><span class="header-section-number">5.4</span> Approximate Bayesian inference with INLA</h2>
<p>The objectives of Bayesian inference are the marginal posterior distributions for
each element of the parameter vector</p>
<p><span class="math display">\[p\left(\theta_{i} \mid \boldsymbol{y}\right)=\int p\left(\theta_{i}, \boldsymbol{\psi} \mid \boldsymbol{y}\right) \mathrm{d} \boldsymbol{\psi}=\int p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) p(\boldsymbol{\psi} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\psi}\]</span></p>
<p>and for each element of the hyperparameter vector</p>
<p><span class="math display">\[p\left(\psi_{k} \mid \boldsymbol{y}\right)=\int p(\boldsymbol{\psi} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\psi}_{-k}\]</span></p>
<p>Thus, we need to perform the following tasks:
(i) compute <span class="math inline">\(p(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span>, from which also all the relevant marginals p(𝜓k|y) can be
obtained;
(ii) compute <span class="math inline">\(p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> which is needed to compute the parameter marginal posteriors <span class="math inline">\(p\left(\theta_{i} \mid \boldsymbol{y}\right)\)</span></p>
<p>The INLA approach exploits the assumptions of the model to produce a numerical
approximation to the posteriors of interest based on the Laplace approximation
method introduced in Section 4.7 (Tierney and Kadane, 1986).
The first task (i) consists of the computation of an approximation to the joint
posterior of the hyperparameters as:</p>
<p><span class="math display">\[\begin{aligned} p(\boldsymbol{\psi} \mid \boldsymbol{y}) &amp;=\frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;=\frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta}, \boldsymbol{\psi})}{p(\boldsymbol{y})} \frac{1}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;=\frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{p(\boldsymbol{y})} \frac{1}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp; \propto \frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;\left.\approx \frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{\tilde{p}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{*} \boldsymbol{\psi}}=: \tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y}) \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tilde{p}(\theta \mid \psi, y)\)</span> is the Gaussian approximation – given by the Laplace method – of <span class="math inline">\(p(\theta \mid \psi, y)\)</span> and <span class="math inline">\(\theta^{*}(\psi)\)</span> is the mode for a given <span class="math inline">\(\psi\)</span> the Gaussian approximation turns out to be accurate since <span class="math inline">\(p(\theta \mid \psi, y)\)</span>appears to be almost Gaussian as it is a priori dis-
tributed like a GMRF, y is generally not informative and the observation distribution
is usually well-behaved.
The second task (ii) is slightly more complex, because in general there will
be more elements in 𝜽 than in 𝝍, and thus this computation is more expensive.
A first easy possibility is to approximate the posterior conditional distributions
<span class="math inline">\(p(\theta \mid \psi, y)\)</span> directly as the marginals from <span class="math inline">\(\tilde{p}(\theta \mid \psi, y)\)</span> i.e. using a Normal distribution, where the Cholesky decomposition is used for the precision matrix (Rue
and Martino, 2007). While this is very fast, the approximation is generally
not very good. The second possibility is to rewrite the vector of parameters as
<span class="math inline">\(\theta=\left(\theta_{i}, \theta_{-i}\right)\)</span> and use again Laplace approximation to obtain</p>
<p><span class="math display">\[\begin{aligned} p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) &amp;=\frac{p\left(\left(\theta_{i}, \boldsymbol{\theta}_{-i}\right) \mid \boldsymbol{\psi}, \boldsymbol{y}\right)}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp;=\frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p(\boldsymbol{\psi} \mid \boldsymbol{y})} \frac{1}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp; \propto \frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp;\left.\approx \frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{\tilde{p}\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)}\right|_{\boldsymbol{\theta}_{-i}=\boldsymbol{\theta}_{-i}^{*}\left(\theta_{i}, \boldsymbol{\psi}\right)}=: \tilde{p}\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tilde{p}\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> is the Laplace Gaussian approximation to <span class="math inline">\(p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> and <span class="math inline">\(\boldsymbol{\theta}_{-i}^{*}\left(\theta_{i}, \boldsymbol{\psi}\right)\)</span> is its mode. Because the random variables <span class="math inline">\(\theta_{-i} \mid \theta_{i}, \psi, y\)</span> re in general
reasonably Normal, the approximation provided by (4.20) typically works very
well. This strategy, however, can be very expensive in computational terms as <span class="math inline">\(\tilde{p}\left(\theta_{-i} \mid \theta_{i}, \psi, y\right)\)</span> must be recomputed for each value of 𝜽 and 𝝍 (some modifications
to the Laplace approximation in order to reduce the computational costs are
described in Rue et al., 2009).</p>
<p>Operationally, INLA proceeds as follows:
(i) first it explores the hyperparameter joint posterior distribution <span class="math inline">\(\tilde{p}(\psi \mid \boldsymbol{y})\)</span> of Eq. (4.18) in a nonparametric way, in order to detect good points <span class="math inline">\(\left\{\psi^{(j)}\right\}\)</span> for
the numerical integration required in Eq. (4.22). Rue et al. (2009) propose
two different exploration schemes, both requiring a reparameterization of
the <span class="math inline">\(\psi\)</span>-space – in order to deal with more regular densities – through the
following steps:</p>
<ol style="list-style-type: lower-alpha">
<li>Locate the mode <span class="math inline">\(\psi^{*}\)</span> of <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> by optimizing log <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> with respect to
𝝍 (e.g., through the Newton–Raphson method).</li>
<li>Compute the negative Hessian <span class="math inline">\(H\)</span> at the modal configuration.</li>
<li>Compute the eigen-decomposition <span class="math inline">\(\mathbf{\Sigma}=\boldsymbol{V} \Lambda^{1 / 2} \boldsymbol{V}^{\prime}\)</span>, with <span class="math inline">\(\Sigma=H^{-1}\)</span>.</li>
<li>Define the new variable z, with standardized and mutually orthogonal
components, such that:</li>
</ol>
<p><span class="math display">\[\boldsymbol{\psi}(z)=\boldsymbol{\psi}^{*}+\boldsymbol{V} \Lambda^{1 / 2} z\]</span></p>
<p>The first exploration scheme (named grid strategy) builds, using the
z-parameterization, a grid of points associated with the bulk of the mass of <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> . This approach has a computational cost which grows exponentially
with the number of hyperparameters; therefore the advice is to adopt it
when K, the dimension of <span class="math inline">\(\psi\)</span>, is lower than 4. Otherwise, the second explo-
ration scheme, named central composite design (CCD) strategy, should be
used as it reduces the computational costs. With the CCD approach, the
integration problem is seen as a design problem; using the mode <span class="math inline">\(\psi^{*}\)</span> and the
Hessian H, some relevant points in the 𝝍-space are selected for performing
a second-order approximation to a response variable (see Section 6.5 of
Rue et al., 2009 for details). In general, the CCD strategy uses much less
points, but still is able to capture the variability of the hyperparameter
distribution. For this reason it is the default option in R-INLA.</p>
</div>
<div id="the-projector-matrix" class="section level2">
<h2><span class="header-section-number">5.5</span> The Projector Matrix</h2>
<p>We need to construct a projection matrix <strong>A</strong> to project the GRF from the observations to thetriangulation vertices. The matrix <strong>A</strong> as the number of rows equal to the number of observations, and the number of columns equal to the number of vertices of the triangulation. Row <span class="math inline">\(i\)</span> of <strong>A</strong> corresponding to an observation at location <span class="math inline">\(s_{i}\)</span> ossibly has three non-zero values at the columns that correspond to the vertices of the triangle that contains the location. If <span class="math inline">\(s_{i}\)</span> within the triangle, these values are equal to the barycentric coordinates. That is, they are proportional to the areas of each of the three subtriangles defined by the location <span class="math inline">\(s_{i}\)</span>
and the triangle’s vertices, and sum to 1. If <span class="math inline">\(s_{i}\)</span> s equal to a vertex of the triangle, row<br />
<span class="math inline">\(i\)</span> has just one non-zero value equal to 1 at the column that corresponds to the vertex. Intuitively, the value <span class="math inline">\(Z(\boldsymbol{s})\)</span> at a location that lies within one triangle is the projection of the plane formed by the triangle vertices weights at location <span class="math inline">\(s\)</span> .</p>
<p>An example of a projection matrix is given below. This projection matrix projects <span class="math inline">\(n\)</span> observations to <span class="math inline">\(G\)</span> triangulation vertices. The first row of the matrix corresponds to an observation with location that coincides with vertex number 3. The second and last rows correspond to observations with locations lying within triangles.</p>
<p><span class="math inline">\(4=\left[\begin{array}{ccccc}A_{11} &amp; A_{12} &amp; A_{13} &amp; \ldots &amp; A_{1 G} \\ A_{21} &amp; A_{22} &amp; A_{23} &amp; \ldots &amp; A_{2 G} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ A_{n 1} &amp; A_{n 2} &amp; A_{n 3} &amp; \ldots &amp; A_{n G}\end{array}\right]=\left[\begin{array}{ccccc}0 &amp; 0 &amp; 1 &amp; \ldots &amp; 0 \\ A_{21} &amp; A_{22} &amp; 0 &amp; \ldots &amp; A_{2 G} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ A_{n 1} &amp; A_{n 2} &amp; A_{n 3} &amp; \ldots &amp; 0\end{array}\right]\)</span></p>
<p>igure 8.6 shows a location <span class="math inline">\(s\)</span> that lies within one of the triangles of a triangulated mesh. The value of the process <span class="math inline">\(Z(\cdot)\)</span> at <span class="math inline">\(s\)</span> is expressed as a weighted average of the values of the process at the vertices of the triangle (<span class="math inline">\(Z_{1}\)</span>, <span class="math inline">\(Z_{2}\)</span> and <span class="math inline">\(Z_{3}\)</span>)and with weights equal to
<span class="math inline">\(T_{1} / T\)</span>, <span class="math inline">\(T_{2} / T\)</span> and <span class="math inline">\(T_{3} / T\)</span> where <span class="math inline">\(T\)</span>denotes the area of the big triangle that contains <span class="math inline">\(s\)</span> and <span class="math inline">\(T_{1}\)</span>, <span class="math inline">\(T_{2}\)</span> and <span class="math inline">\(T_{3}\)</span> are the areas of the subtriangles</p>
<p><span class="math inline">\(Z(\boldsymbol{s}) \approx \frac{T_{1}}{T} Z_{1}+\frac{T_{2}}{T} Z_{2}+\frac{T_{3}}{T} Z_{3}\)</span></p>
<p><img src="images/triangle-mesh-1.png" alt="triangle-mesh-1" />
<code>R-INLA</code> provides the <code>inla.spde.make.A()</code> function to easily construct a projection matrix <strong>A</strong>
We create the projection matrix of our example by using <code>inla.spde.make.A()</code> passing the triangulated mesh <code>mesh</code> and the coordinates <code>coo</code>.</p>
<pre><code>A &lt;- inla.spde.make.A(mesh = mesh, loc = coo)</code></pre>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bayesian_INLA_Rubio">
<p>Gómez Rubio, Virgilio. 2020. <em>Bayesian Inference with Inla</em>. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9781315175584">https://doi.org/10.1201/9781315175584</a>.</p>
</div>
<div id="ref-Krainski-Rubio">
<p>Krainski, Elias T. 2019. <em>Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and Inla</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-Blangiardo-Cameletti">
<p>Marta Blangiardo, Michela Cameletti. 2015. <em>Spatial and Spatio-Temporal Bayesian Models with R-Inla</em>. Wiley.</p>
</div>
<div id="ref-Moraga2019">
<p>Moraga, Paula. 2019. <em>Geospatial Health Data</em>. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9780429341823">https://doi.org/10.1201/9780429341823</a>.</p>
</div>
<div id="ref-Rue2009">
<p>Rue, Håvard, Sara Martino, and Nicolas Chopin. 2009. “Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 71 (2): 319–92. <a href="https://doi.org/10.1111/j.1467-9868.2008.00700.x">https://doi.org/10.1111/j.1467-9868.2008.00700.x</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exploratory.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="prdm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/Thesis/edit/master/05-inla_spde.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf", "Niccolo_Salvini_Thesis.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
