[["inla-spde.html", "Chapter 5 INLA computation 5.1 Latent Gaussian Models LGM 5.2 Approximation in INLA setting 5.3 R-INLA package in a bayesian perspective", " Chapter 5 INLA computation INLA (Rue, Martino, and Chopin 2009) stands for Integrated Nested Laplace approximation and constitutes a integral computational alternative to traditional MCMC methods. INLA does approximate bayesian inference on special type of models called LGM (Latent Gaussian Models) due to the fact that they are computationally convenient. The benefits are many, some among the other are: Low computational costs, even for large models. It provides high accuracy. Can define very complex models within that framework. Most important statistical models are LGM. Very good support for spatial models. Implementatio of spatio-temporal model enabled. INLA uses a combination of analytics approximations and numerical integration to obtain an approximated posterior distribution of the parameters in a shorter time period. The combination of INLA and SPDE (Stochastic Partial Differential Equations) allows to analyze point level data at a faster pace, SPDE is approached in chapter 7. Notation is imported from Marta Blangiardo (2015) and GÃ³mez Rubio (2020), and quite differ from the one in the original paper by Rue, Chopin and Martino (2009). The chrono-logic steps in the methodology presentation follows Moraga (2019) and author choices. As a notation remark, bold symbols are considered as vectors, so each time they occur they have to be considered like the ensamble of their values. 5.1 Latent Gaussian Models LGM INLA can fit only LG type of models so the following work tries to encapsulate the main properties of a LGM, so that a problem can be shaped into the LGM framework with the purpose to exploit its benefits. Given some observations \\(y_{i \\ldots n}\\) in order to define a Latent Gaussain Model within the bayesian framework it is convenient to specify at first an exponential family (Gaussian, Poisson, Exponential) distribution function characterized by some parameters \\(\\phi_{i}\\) (usually expressed by the mean \\(\\left.E\\left(y_{i}\\right)\\right)\\)) and some other hyper-parameters \\(\\psi_{k} ,\\forall k \\in \\ 1\\ldots K\\). The parameter \\(\\phi_{i}\\) can be defined as an additive latent linear predictor \\(\\eta_{i}\\), as pointed out by Krainski and Rubio ((2019)) through a link function \\(g(\\cdot)\\), i.e. \\(g\\left(\\phi_{i}\\right)=\\eta_{i}\\). A comprehensive expression of the linear predictor takes into account all the possible effects on covariates \\[ \\eta_{i}=\\beta_{0}+\\sum_{m=1}^{M} \\beta_{m} x_{m i}+\\sum_{l=1}^{L} f_{l}\\left(z_{l i}\\right) \\] where \\(\\beta_{0}\\) is the intercept, \\(\\boldsymbol{\\beta}=\\left\\{\\beta_{1}, \\ldots, \\beta_{M}\\right\\}\\) are the coefficient that quantifies the linear effects on covariates \\(\\boldsymbol{x}=\\left({x}_{1}, \\ldots, {x}_{M}\\right)\\) and \\(f_{l}(\\cdot), \\forall l \\in 1 \\ldots L\\) are a set of random effects defined in terms of a set of covariates \\(\\boldsymbol{z}=\\left(z_{1}, \\ldots, z_{L}\\right)\\) (e.g. rw, ar1). Because of the last assumption LGM models can receive a wide range of models e.g. GLM, GAM, GLMM, linear models and spatio-temporal models. All the latent components can be conveniently grouped into a variable denoted with \\(\\boldsymbol{\\theta}\\) such that: \\(\\boldsymbol{\\theta}=\\left\\{\\beta_{0}, \\beta, f\\right\\}\\) and the same can de done for hyper parameters \\(\\boldsymbol{\\psi} = \\left\\{\\psi_{1}, \\ldots, \\psi_{K}\\right\\}\\). Then the probability distribution conditioned to parameters and hyper parameters is then: \\[y_{i} \\mid \\boldsymbol{\\theta}, \\boldsymbol{\\psi} \\sim \\pi\\left(y_{i} \\mid \\boldsymbol{\\theta},\\boldsymbol{\\psi}\\right)\\] Since data \\(\\left(y_{1}, \\ldots, y_{n}\\right)\\) is drawn by the same distribution family but it is conditioned to parameters and hyper parameters (i.e. conditional independence) then joint distribution is given by the likelihood. The Product operator index \\(i\\) goes from 1 to \\(n\\), i.e. \\(\\mathbf{I} = \\left\\{1 \\ldots n \\right\\}\\). When a is missing so the correspoding i is not in \\(\\mathbf{I}\\) INLA authomatically will not include the index in the model avoiding errors. (2020). As a consequence the likelihood expression is: \\[\\pi(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}, \\boldsymbol{\\psi})=\\prod_{i \\in \\mathbf{I}} \\pi\\left(y_{i} \\mid \\theta_{i}, \\boldsymbol{\\psi}\\right)\\] Each data point is connected to a linear combination of each element in \\(\\boldsymbol{\\theta}\\) said latent field. Each component of the latent field is supposed to be conditionally independent (i.e. \\(p\\left(\\theta_{i}, \\theta_{j} \\mid \\theta_{-i, j}\\right)=p\\left(\\theta_{i} \\mid \\theta_{-i, j}\\right) p\\left(\\theta_{j} \\mid \\theta_{-i, j}\\right)\\)) which constitutes a very important assumption. Hyper parameters are by definition independent, therefore \\(\\boldsymbol{\\psi}\\) will be the product of many univariate priors (2020). A Multivariate Normal distribution is imposed on the latent field \\(\\boldsymbol{\\theta}\\) such that it is centered in 0 with precision matrix \\(\\boldsymbol{Q(\\psi)}\\) depending on \\(\\boldsymbol{\\psi}\\) hyper parameter vector i.e., \\(\\boldsymbol{\\theta} \\sim \\operatorname{Normal}\\left(\\mathbf{0}, \\boldsymbol{Q}^{-1}(\\boldsymbol{\\psi})\\right)\\). As side note some authors choose to approach the precision matrix as a covariance function in the setting before. This is strongly not encouraged since it complicates notation. However This has to be forcibly done in next chapter since a special covariance function is used. The exponential family density function is then expressed through: \\[ \\pi(\\theta \\mid \\psi)=(2 \\pi)^{-n / 2}|Q(\\psi)|^{1 / 2} \\exp \\left(-\\frac{1}{2} \\theta^{\\prime} Q(\\psi) \\theta\\right) \\] The conditional independence assumption on the latent field \\(\\boldsymbol{\\theta}\\) leads \\(\\boldsymbol{Q(\\psi)}\\) to be a sparse precision matrix, and the probability distribution function takes the name of Gaussian Markov random field (GMRF). From here comes the computational power inherited using GMRF for inference, matrices are sparse so numerical methods can be exploited (Marta Blangiardo 2015). This last assumptio will be discussed and verifies in capter 7. Once priors are specified for \\(\\boldsymbol{\\psi}\\) then the following holds: \\[ \\pi(\\boldsymbol{\\theta}, \\boldsymbol{\\psi} \\mid y)\\propto \\underbrace{\\pi(\\boldsymbol{\\psi})}_{\\text {prior }} \\times \\underbrace{\\pi(\\theta \\mid \\psi)}_{\\text {GMRF }} \\times \\underbrace{\\prod_{i=1}^{n} \\pi\\left(y_{i} \\mid \\theta_{i}, \\boldsymbol{\\psi}\\right)}_{\\text {likelihood }} \\] Last expression is said a LGM if the whole set of assumption imposed since now are met. At this point it is possible to write the joint posterior distribution plugging in the corresponding likelihood and GMRF expression: \\[ \\begin{aligned} \\pi(\\boldsymbol{\\theta}, \\boldsymbol{\\psi} \\mid y) &amp; \\propto \\pi(\\boldsymbol{\\psi}) \\times \\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\psi}) \\times \\pi(y \\mid \\boldsymbol{\\theta}, \\boldsymbol{\\psi}) \\\\ &amp; \\propto \\pi(\\boldsymbol{\\psi}) \\times \\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\psi}) \\times \\prod_{i=1}^{n} \\pi\\left(y_{i} \\mid \\theta_{i}, \\boldsymbol{\\psi}\\right) \\\\ &amp; \\propto \\pi(\\boldsymbol{\\psi}) \\times|\\boldsymbol{Q}(\\boldsymbol{\\psi})|^{1 / 2} \\exp \\left(-\\frac{1}{2} \\boldsymbol{\\theta}^{\\prime} \\boldsymbol{Q}(\\boldsymbol{\\psi}) \\boldsymbol{\\theta}\\right) \\times \\prod_{i}^{n} \\exp \\left(\\log \\left(\\pi\\left(y_{i} \\mid \\theta_{i}, \\boldsymbol{\\psi}\\right)\\right)\\right) \\end{aligned} \\] Joining the exponentials because of their multiplicative property: \\[\\begin{equation} \\pi(\\boldsymbol{\\theta}, \\boldsymbol{\\psi} \\mid y) \\propto \\pi(\\psi) \\times|\\boldsymbol{Q}(\\boldsymbol{\\psi})|^{1 / 2} \\exp \\left(-\\frac{1}{2} \\boldsymbol{\\theta}^{\\prime} \\boldsymbol{Q}(\\boldsymbol{\\psi}) \\boldsymbol{\\theta}+\\sum^{n} \\log \\left(\\pi\\left(y_{i} \\mid \\theta_{i}, \\boldsymbol{\\psi}\\right)\\right)\\right) \\tag{5.1} \\end{equation}\\] 5.2 Approximation in INLA setting INLA is not going to try to estimate the whole posterior distribution from expression (5.1). Instead it will try to estimate the posterior marginal distribution effects for each latent parameter \\(\\boldsymbol{\\theta}\\) in the hyper parameter space \\(\\psi_{k}\\). Proper estimation methods however are beyond the scope of the analysis, further references are suggested in their respective part (2020) in section 2.2.2 and (2015) in section 4.7.2. For latent parameter \\(\\boldsymbol{\\theta}\\) : \\[\\begin{equation} \\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})=\\int \\pi(\\boldsymbol{\\theta}, \\boldsymbol{\\psi} \\mid \\mathbf{y}) \\pi(\\boldsymbol{\\psi} \\mid \\mathbf{y}) d \\psi \\tag{5.2} \\end{equation}\\] The posterior marginal for the hyper parameter is \\(\\boldsymbol{\\psi}\\) since it is composed by \\(k\\) elements, i.e. \\(\\boldsymbol{\\psi}=\\left\\{\\psi_{1}, \\ldots, \\psi_{K}\\right\\}\\) is: \\[ \\pi\\left(\\psi_{k} \\mid y\\right)=\\int \\pi(\\boldsymbol{\\psi} \\mid y) d \\psi_{-k} \\] where \\(\\psi_{-k}\\) is a vector of hyper parameters \\(\\psi\\) without the element \\(\\psi_{k}\\). Since hyperparam integrals are nested inside the parameter integrals a hierarchical structure can be imposed. This structure is welcomed very warmly since it is convenient later for hierarchical bayesian regression (next chapter 6.4). By unnesting the hierarchical: compute the Laplace approximation marginals for the hyper parameters: \\(\\tilde\\pi\\left(\\psi_{k} \\mid \\boldsymbol{y}\\right)\\) compute the Laplace approximation marginals for the parameters given the hyper parameter approximation: \\(\\tilde{\\pi}\\left(\\theta_{i} \\mid \\boldsymbol{y}\\right) \\approx \\int \\tilde{\\pi}\\left(\\theta_{i} \\mid \\boldsymbol{\\psi}, \\boldsymbol{y}\\right) \\underbrace{\\tilde{\\pi}(\\boldsymbol{\\psi} \\mid \\boldsymbol{y})}_{\\text {Estim. in step 1 }} \\mathrm{d} \\psi\\) Then plugging approximation in the integral observed in (5.2) it is obtained: \\[ \\tilde{\\pi}\\left(\\theta_{i} \\mid y\\right) \\approx \\int \\tilde{\\pi}\\left(\\theta_{i} \\mid \\boldsymbol{\\psi}, y\\right) \\tilde{\\pi}(\\boldsymbol{\\psi} \\mid y) \\mathrm{d} \\psi \\] At this point INLA uses the following approximation to compute marginals: \\[ \\tilde{\\pi}\\left(\\theta_{i} \\mid y\\right) \\approx \\sum_{j} \\tilde{\\pi}\\left(\\theta_{i} \\mid \\boldsymbol{\\psi}^{(j)}, y\\right) \\tilde{\\pi}\\left(\\boldsymbol{\\psi}^{(j)} \\mid y\\right) \\Delta_{j} \\] {\\(\\boldsymbol{\\psi}^{(j)}\\)} are a set of values of the hyper param \\(\\psi\\) grid used for numerical integration, each one associated to a specific weight \\(\\Delta_{j}\\). INLA catch this specific integration points by setting up a regular grid about the posterior mode of \\(\\psi\\) with CCD (central composite design) centered in the mode. Blangiardo-Cameletti source The approximation \\(\\tilde{\\pi}\\left(\\theta_{i} \\mid y\\right)\\) can take different forms and be computed in different ways. Rue, Martino, and Chopin (2009) also discuss how this approximation should be in order to reduce the numerical error. 5.2.1 further methods for approximations (prolly do not note include) Following GÃ³mez Rubio (2020), approximations of the joint posterior for the hyper paramer \\(\\tilde\\pi\\left(\\psi_{k} \\mid \\boldsymbol{y}\\right)\\) is used to compute the marginals for the latent effects anf hyper parameters in this way: \\[ \\left.\\tilde{\\pi}(\\boldsymbol{\\psi} \\mid \\mathbf{y}) \\propto \\frac{\\pi(\\boldsymbol{\\theta}, \\boldsymbol{\\psi}, y)}{\\tilde{\\pi}_{G}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\psi}, y)}\\right|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}^{*}(\\boldsymbol{\\psi})} \\] In the previous equation \\(\\tilde{\\pi}_{G}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\psi}, y)\\) is a gaussian approximation to the full condition of the latent effect \\({\\theta}^{*}(\\boldsymbol{\\psi})\\) is the mode for a given value of the hyper param vector \\(\\boldsymbol{\\psi}\\) At this point there exists three types of approximations for \\(\\pi\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\psi}, y\\right)\\) first with a gaussian approximation, estimating mean \\(\\mu_{i}(\\boldsymbol{\\psi})\\) and variance \\(\\sigma_{i}^{2}(\\boldsymbol{\\psi})\\). second using the Laplace Approximation. third using simplified Laplace Approximation (rivedere meglio) 5.3 R-INLA package in a bayesian perspective INLA computations and methodology is developed in the R-INLA project whose package is available on their website. Download is not available on CRAN (the Comprehensive R Archive Network) so a special source, which is maintained by authors and collaborators through a bitbucket repo, has to be specified. However download information are neatly showed in a dedicated section in the project website. The website offers also a forum where threads can be proposed and an active community is keen to answer. Furthermore the project is gaining importance due to its vast application a cases that recently are exponentiallly growing, examples are [italiano citato nel video onlie di RUE 2020, altro tizio citato da Rue] The core function of the package is inla()and works as many other regression functions like glm() , lm() or gam(). Inla takes as arguments the formula together with the data (expects a data.frame obj) on which estimation is desired. Many other more alternative methods inside the function can be specifird and are later presented in a table. Inla objects are inla.dataframe summary-type list object containing the results from model fitting. Results contained in the object are specified in the table below, even though some of them requires special method to be seen: (meglio se metto direttamente tabella generate in kable()) summary table list object, source: Krainski (2019) 5.3.1 Linear Predictor Follwing Krainski and Rubio (2019) observations \\(y(s_{1}), \\ldots, y(s_{n})\\) come from a Gaussian variable with \\(\\mu_{i}\\) and precision \\(\\tau\\). The mean moment \\(\\mu_{i}\\) has to be specified as linear predictor through a link function as in section @ref)(genregvec),i.e \\(\\eta_{i}=\\beta_{0}+\\sum_{m=1}^{M} \\beta_{m} x_{m i}+\\sum_{l=1}^{L} f_{l}\\left(z_{l i}\\right)\\). Simplifying the composition of the linear predictor and using spatial dataset SPDEtoy: Spatial covariates as long and lat have to be included in the linear predictor as well as the others, so that linear predictor has this form: \\(\\beta_{0}+\\beta_{1} s_{1 i}+\\beta_{2} s_{2 i}\\), where once again \\(\\beta_{0}\\) is the intercept and \\(\\beta_{j}\\) are the linear effect on covariates. It is possible to include also non-linear effects with the f() specifying the model The prior choice for intercept is uniform. Prior for Gaussian latent field are vague such that have 0 mean and 0.001 precision, then the prior in the precision \\(\\tau\\) is gamma with parameters 1 and 0.00005. Prior choice can then be changes. The summary of the model already described is: \\[ \\begin{aligned} y_{i} &amp; \\sim N\\left(\\mu_{i}, \\tau^{-1}\\right), i=1, \\ldots, 200 \\\\ \\mu_{i} &amp;=\\beta_{0}+\\beta_{1} s_{1 i}+\\beta_{2} s_{2 i} \\\\ \\beta_{0} &amp; \\sim \\text { Uniform } \\\\ \\beta_{j} &amp; \\sim N\\left(0,0.001^{-1}\\right), j=1,2 \\\\ \\tau &amp; \\sim G a(1,0.00005) \\end{aligned} \\] library(INLA) ## Loading required package: Matrix ## Loading required package: sp ## Warning: package &#39;sp&#39; was built under R version 4.0.3 ## Loading required package: parallel ## Loading required package: foreach ## Warning: package &#39;foreach&#39; was built under R version 4.0.3 ## This is INLA_20.09.16 built 2020-09-16 14:46:36 UTC. ## - See www.r-inla.org/contact-us for how to get help. ## - Save 375.4Mb of storage running &#39;inla.prune()&#39; data(&quot;SPDEtoy&quot;) m0 = inla(y ~ s1 + s2, data = SPDEtoy) summary(m0) ## ## Call: ## &quot;inla(formula = y ~ s1 + s2, data = SPDEtoy)&quot; ## Time used: ## Pre = 2.47, Running = 0.471, Post = 0.324, Total = 3.27 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) 10.132 0.242 9.656 10.132 10.608 10.132 0 ## s1 0.762 0.429 -0.081 0.762 1.606 0.762 0 ## s2 -1.584 0.429 -2.428 -1.584 -0.740 -1.584 0 ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant ## Precision for the Gaussian observations 0.308 0.031 0.251 0.307 ## 0.975quant mode ## Precision for the Gaussian observations 0.372 0.305 ## ## Expected number of effective parameters(stdev): 3.00(0.00) ## Number of equivalent replicates : 66.67 ## ## Marginal log-Likelihood: -423.18 References "]]
