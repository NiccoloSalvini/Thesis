[["prdm.html", "Chapter 5 Point Referenced Data Modeling 5.1 Gaussian Process (GP) 5.2 Spatial Covariance Function 5.3 The Stochastic Partial Differential Equation (SPDE) approach 5.4 Hedonic models Literature Review and Spatial Hedonic Price Models 5.5 Point Referenced Regression for univariate spatial data 5.6 Spatial Kriging (prediction) 5.7 Model Checking 5.8 Prior Specification", " Chapter 5 Point Referenced Data Modeling Geostatistical data are a collection of samples of geo type data indexed by coordinate Reference Systems (CRS), projected (e.g. Eastings and Northings) or unprojected (e.g. Latitude and Longitude), that originates from a spatially continuous phenomenon (Moraga 2019). Data as such can monitor a vast range of phenomena, e.g. disease cancer detection (Bell et al. 2006) at several sites, COVID19 spread in China (Li et al. 2020), PM pollution concentration in a North-Italian region Piemonte (Cameletti et al. 2012). Moreover house prices variation, as observed in Gómez Rubio (2020), where house prices smoothly vary between closer neighborhoods. All the Examples taken before might document a spatial nature of data according to which closer observations can display similar values, this phenomenon is named spatial autocorrelation. Spatial autocorrelation conceptually stems from geographer Waldo Tobler whose famous quote, known as first law of geography, inspires geostatisticians: Everything is related to everything else, but near things are more related than distant things  Waldo R. Tobler Spatial models are explicitly designed to take into account this behavior and can separate spatial patterns from simply random spatial variance. Spatial data can be partitioned into three spatial data type whose modeling tools are specific to their respective category. Areal Data Point Referenced Data Point Pattern Data (rivedere immagine ) RESTful scraping API designed in chapter 3 extracts point referenced covariates in the form of Latitude and Longitude nested in a hidden source code JSON object. Observations are then plotted in figure 5.1 obtaining a scattered map of houses with their respective geo-coordinates. Modeling methodologies described in this analysis will exclusively take into account point referenced techniques. Then in order to extend data from discrete measurements (i.e. point referenced) to a continuous spatial surface a stochastic process, namely Gaussian Process, has to be introduced and constrained to Stationarity and Isotropy. GP is then evaluated with a convenient covariance function, i.e. Matèrn. The reason why Matérn is selected as candidate for covariance function relies in its flexibility and the fact that GP represented with Matérn covariance are able to determine a GMRF ?? through the Stochastic Partial Differential Equations (SPDE) approach (Cameletti et al. 2012). The main benefit proceeding from a GF to a GMRF arises from the good computational properties that the latter appreciate. Hedonic Price Models brings to the present analysis the theoretical foundation according to which covariates are added to the model. Spatial kriging is essential to predict the process at new locations given the continuous spatial surface. In the end models have to be checked and verified with resampling schemes which are once again specific to the data type and the scope of the analysis. 5.1 Gaussian Process (GP) For simplicity lets consider \\(y\\) point of interest observations \\(y\\left(\\boldsymbol{s}_{1}\\right),y\\left(\\boldsymbol{s}_{2}\\right), \\ldots, y\\left(\\boldsymbol{s}_{n}\\right)\\) from a random spatial process \\(Y(s_n)\\), such that: \\(Y\\left(\\boldsymbol{s}_{1}\\right),Y\\left(\\boldsymbol{s}_{2}\\right), \\ldots, Y\\left(\\boldsymbol{s}_{n}\\right)\\) observed at location \\(\\boldsymbol{s}_{1}, \\ldots, \\boldsymbol{s}_{n}\\). In the context of point reference data modeling each observation has to be considered as a partial realization of an unobserved/underlying random spatial process \\(\\left\\{Y(s): s \\in D \\subset \\mathbb{R}^{2}\\right\\}\\), where surface \\(D\\) is a subset of r-dimensional Euclidean space \\(\\mathbb{R}^{r}\\). Moreover when \\(r = 1\\) it is the most simple stochastic process widely explored in literature i.e. time series process. However geostatistical data always displays \\(r = 2\\) (i.e. Latitutde and Longitude, Eastings and Northings) or eventually \\(r = 3\\), when elevation data is available. The stochastic process \\(Y\\) is observed in a fixed set of monitoring stations (arrows left in figure 5.2) and inference can be done regarding moments of the realized interpolated process. This information is essential to build a spatially continuous surface (3D surface right part in figure 5.2) over the y-studied variable in order to predict the phenomenon at locations not yet observed (Paci 2020). Figure 5.2: 3D scatterplot of point referenced data and a spatial surface for Stockton data, source Marta Blangiardo (2015) Definition 5.1 (GP definition) A collection of \\(n\\) random variables, such as \\(Y(s_{1}), Y(s_{2}) , \\ldots, Y(s_{n})\\) that are valid spatial processes are said to be a GP if for any set of spatial index \\(n\\) and for each set of corresponding locations \\(\\left\\{y\\left(s_{1}\\right), \\ldots, y\\left(s_{n}\\right)\\right\\}\\) follows a multivariate Gaussian distribution with mean \\(\\boldsymbol{\\mu}=\\left\\{\\mu\\left(s_{1}\\right), \\ldots, \\mu\\left(s_{n}\\right)\\right\\}\\) and covariance matrix \\(\\mathbf{Q}^{-1}_{i,j}, \\forall i \\neq j\\) The covariance matrix relates each observation to each of the others via a covariance function defined as \\(\\mathcal{C}(\\cdot)\\). GP in a spatial domain setting has to check two important properties in order to exploit INLA techniques, even though in some cases the assumptions can be both of the two relaxed: Stationary. Isotropy. Stationarity in a stochastic process can be strong, weak or intrinsic. The strong property forces the distribution of the process \\(\\left\\{y\\left(s_{1}\\right), \\ldots, y\\left(s_{n}\\right)\\right\\}\\) for any given spatial index \\(n\\) and its correspondent location sets \\(s_{1,\\ldots,n}\\) to be the same as the one in \\(\\left\\{y\\left(s_{1}+\\boldsymbol{h}\\right), \\ldots, y\\left(s_{n}+\\boldsymbol{h}\\right)\\right\\}\\), where \\(h\\) is a number belonging to \\(R^{2}\\). On the other hand the weak property ensures that if the GP mean moment is constant over the study domain \\(\\mu(\\mathbf{s}) \\equiv \\mu\\) (e.g. \\(E[Y(s)]=\\mu, \\forall s \\in D\\)) then the covariance functions does depend only on the distance (euclidean \\(\\left\\|s_{i}-s_{j}\\right\\|\\) distance) between each couple points. Weak stationarity consequences are the most interesting: It does not matter whether observations are placed either in a specific region, nor the direction towards they are oriented, the covariance functions \\(\\mathcal{C}(h)\\) can summarize the process through the separation vector \\(\\mathbf{h}\\) i.e. \\(\\mathcal{C}(\\mathbf{s}, \\mathbf{s}+\\mathbf{h})=\\mathcal{C}(\\mathbf{h}), \\forall \\mathbf{h} \\in \\mathbb{R}^{r}\\) (Banerjee, Carlin, and Gelfand 2014). In other words weak stationarity in GP implies being invariant under translation (Krainski 2019). The relationship between strong and weak is not bijective since being strong implies also being weak, but the opposite is not always true for non-Gaussian process. Furthermore through the intrinsic stationary property it is meant that \\(E[Y(\\mathbf{s}+\\mathbf{h})-Y(\\mathbf{s})]=0\\), the second moment of the latter expression can be written as \\(E[Y(\\mathbf{s}+\\mathbf{h})-Y(\\mathbf{s})]^{2}\\) leading to \\(\\operatorname{Var}(Y(\\mathbf{s}+\\mathbf{h})-Y(\\mathbf{s}))\\). Last expression is called variogram whose mathematicla form is \\(2 \\gamma(\\mathbf{h})\\), even tough its half,i.e. \\(\\gamma(\\mathbf{h})\\), is more interpretable, namely semivariogram (Cressie 2015a). Semivariograms are characterized by 3 tuning parameters circled in red in figure 5.3: range \\(\\sigma^{2}\\): At some offset distance, the variogram values will stop changing and reach a sort of plateau. The distance at which the effect occurs is called the range \\(\\frac{\\Delta\\gamma(\\mathrm{h})}{h} \\approx 0\\). sill \\(\\tau^{2}\\): The plateau value at which the variogram stops changing \\(\\frac{\\Delta\\gamma(\\mathrm{h})}{h} = 0\\). nugget \\(\\tau^{2}+\\sigma^{2}\\): The discontinuity at the origin. Although this theoretically should be zero, sampling error and short scale variability can cause it to be non-zero \\(\\gamma(\\mathrm{0})\\). Figure 5.3: Variogram explanation of its main components, author source The process is said to be Isotropic if the covariance function depends only on the between-points distance \\(\\left\\|\\mathbf{h}\\right\\|\\), so it is invariant under rotation (2019). A further way of seeing the property is that Isotropy implies concentric decaying contours (Paci 2020), green in 5.4, that resemble the vanishing of spatial dependence (@ Marta Blangiardo 2015), and so it does covariance values. In spatial statistics the assumption of isotropy is very common despite being very restrictive for describing the rich variety of interactions that can characterize spatial processes. Then if the last assumption does not hold and direction towards point are distant from each other matters within the spatial domain \\(D\\), then is said to be Anisotropic, purple in 5.4. Formalizing the results: \\[\\mathcal{C}(\\mathbf{h})=\\mathcal{C}(\\|\\mathbf{h}\\|)\\] Figure 5.4: Left: isotropy concentric decaying contours, Right: anisotropy concentric decaying contours, source Blanchet-Scalliet et al. (2019) 5.2 Spatial Covariance Function The covariance function \\(\\mathcal{C}(\\cdot)\\) ensures that all the values that are close together in input space will produce output values that are close together. \\(\\mathcal{C}(\\cdot)\\) needs to inherits the validity characteristics from the random spatial process, furthermore it has to be positive definite. The covariance function specification is a critical part of modeling and will affect kriging 5.6 estimates, parameters and related uncertainty (Cressie 2015b). In addition covariance function must share characteristic properties of functions, such as: (cerca di capire queste) Multiply valid covariance functions (summing independent random variables) Mixing covariance functions (mixing distributions) Convolving covariance functions, this will be very important  Covariance functions under stationary and isotropic GPs displays two important properties: they are constant in mean within \\(D\\) i.e. \\(\\mathcal{C}(\\mathbf{s}, \\mathbf{s}+\\mathbf{h})=\\mathcal{C}(\\mathbf{h}), \\forall \\mathbf{h} \\in \\mathbb{R}^{r}\\) and they depends on distance vector \\(\\mathbf{h}\\), not direction i.e. \\(\\mathcal{C}(\\mathbf{h})=\\mathcal{C}(\\|\\mathbf{h}\\|)\\) There are many covariance functions and ways to relate distant points on a spatial domain \\(D\\). Typically the choice of the Covariance can depend either on data or the scope of the analysis. Covariance functions are wrapped into hyper-parameters which are mainly three: Range: At some offset distance, the variogram values will stop changing and reach a plateau. The distance at which this occurs is called the range. Sill: The plateau value at which the variogram stops changing. Nugget: The discontinuity at the origin. Although this theoretically should be zero, sampling error and short scale variability can cause it to be non-zero partial sillò ( espressione della covariance function insieme a alle \\(\\sigma^2\\) come: \\(\\mathcal{C}(\\mathbf{s}+\\mathbf{h}, \\mathbf{s} \\mid \\theta)=\\sigma^{2} \\mathbf{R}(\\|h\\| ; \\phi)\\) ) spiega anche queste due sotto \\[ \\mathbf{w}=\\left(w\\left(\\mathbf{s}_{1}\\right), \\ldots, w\\left(\\mathbf{s}_{n}\\right)\\right)^{\\prime} \\sim \\mathrm{N}\\left(\\mathbf{0}, \\sigma^{2} \\mathbf{R}(\\phi)\\right) \\text { where } \\left.\\mathbf{R}(\\phi)_{i j}=\\rho\\left(\\left\\|\\mathbf{s}_{i}-\\mathbf{s}_{j}\\right\\| ; \\phi\\right)\\right) \\] \\(\\Sigma_{\\theta}=\\sigma^{2} \\mathbf{R}(\\phi)+\\tau^{2} I_{n}\\) Three of the most applied covariance functions are presented below. \\[ \\begin{aligned} &amp;\\text { Exponential } \\quad \\mathcal{C}(\\mathbf{h})=\\left\\{\\begin{array}{cl} \\tau^{2}+\\sigma^{2} &amp; \\text { if } h=0 \\\\ \\sigma^{2} \\exp (-\\phi h) &amp; \\text { if } h&gt;0 \\end{array}\\right.\\\\ &amp;\\text { Gaussian } \\quad \\mathcal{C}(\\mathbf{h})=\\left\\{\\begin{array}{cl} \\tau^{2}+\\sigma^{2} &amp; \\text { if } h=0 \\\\ \\sigma^{2} \\exp \\left(-\\phi^{2} h^{2}\\right) &amp; \\text { if } h&gt;0 \\end{array}\\right. \\\\ &amp;\\text { Matérn } \\quad \\mathcal{C}(\\mathbf{h})=\\left\\{\\begin{array}{cl} \\tau^{2}+\\sigma^{2} &amp; \\text { if } h=0 \\\\ \\frac{\\sigma^{2}}{2^{\\nu-1} \\Gamma(\\nu)}(\\phi h)^{\\nu} K_{\\nu}(\\phi h) &amp; \\text { if } h&gt;0 \\end{array}\\right. \\end{aligned} \\] 5.2.1 Matérn Covariance Function Matérn is crucial since when it is used together with a stationary and isotropic GP, the SPDE approach can provide a GMRF representation of the same process. Chapter 6 discloses this fundamental property. Matérn can also be accounted as the most used in geostatistics (Krainski et al. 2018) and (Gómez Rubio 2020). Matérn is tuned mainly by two parameters, a scaling one \\(\\kappa&gt;0\\), usually set equal to the range by the relation \\(\\sigma^{2}=\\frac{\\sqrt{8 \\lambda}}{\\kappa}\\)) and a smoothing one \\(\\nu&gt;0\\). A stationary and isotropic Matérn covariance function has this form: \\[ \\mathcal{C}(\\mathbf{h})=\\left\\{\\begin{array}{ll} \\tau^{2}+\\sigma^{2} &amp; \\text { if } h=0 \\\\ \\frac{\\sigma^{2}}{2^{\\nu-1} \\Gamma(\\nu)}(\\phi t)^{\\nu} K_{\\nu}(\\phi t) &amp; \\text { if } h&gt;0 \\end{array}\\right. \\] \\(\\Gamma(\\nu)\\) is a Gamma function depending on \\(\\nu\\) values, \\(K_{\\nu}(\\cdot)\\) is a modified Bessel function of second kind. The smoothness parameter \\(\\nu\\) in figure 5.5 below takes 4 different values showing the potentiality of Matérn to relates distances to covariance values. When \\(\\nu = 1\\)  When \\(\\nu = 1/2\\) it becomes the exponential covariance function, When \\(\\nu = 3/2\\) it uncovers a convenient closed form (Paci 2020), when \\(\\nu \\approx \\infty\\), in this case for representation purposes \\(\\nu = 80\\) it becomes Gaussian covariance function. Figure 5.5: Matérn corr. function with 4 values of \\(\\nu\\), kept \\(\\phi\\) fixed, authors source 5.3 The Stochastic Partial Differential Equation (SPDE) approach Locations in the spatial setting are considered as realizations of a stationary, isotropic unobserved GP \\(w(s)\\) to be estimated (5.1). Before approaching the problem with SPDE, GPs were treated as multivariate Gaussian densities and Cholesky factorizations were applied on the covariance matrices and then fitted with likelihood (Paci 2020). Matrices in the context of spatial and spatio-temporal models (Paci et al. 2017; Cameletti et al. 2012) settings are very dense and they were scaling with the order of \\(\\mathcal{O}\\left(n^{3}\\right)\\). Problem were linked to the computational costs needed for linear algebra operations for model fitting and spatial interpolation and prediction (Cameletti et al. 2012), having led to obvious big-n problem. The breakthrough came with Lindgren, Rue, and Lindström (2011) that proves that a stationary, isotropic (might be relaxed) GP with Matérn covariance can be represented as a GMRF using SPDE solutions by finite element method (Krainski 2019). In other words given a GP whose covariance matrix is \\(\\boldsymbol{Q^{-1}}\\), SPDE can provide a method to approximate \\(\\boldsymbol{Q^{-1}}\\) without the previous computational constraints. As a matter of fact SPDE are equations whose solutions are GPs with a chosen covariance function focused on satisfying the relationship SPDE specifies (2019). Benefits are many but the most important is that the representation of the GP through a GMRF provides a sparse representation of the spatial effect through a sparse precision matrix \\(\\boldsymbol{Q}\\) . Sparse matrices enable convenient inner computation properties of GMRF 4.2 which are exploited by INLA algorithm 4 leading to a more feasible big-O \\(\\mathcal{O}\\left(n^{3 / 2}\\right)\\). Mathematical details and deep understanding of the equations in SPDE are beyond the scope of the analysis. Luckily enough R-INLA has a set of functions that makes clear to the practitioner the minimal requirements to pass from discrete locations to their continuously indexed surface alter-ego. In few words SPDE approach uses a finite element (FEM method) representation to shape the Matérn field as a linear combination of basis functions defined on a triangulation of the domain \\(\\mathcal{D}\\) (2012). What it intenally does is splitting the domain \\(\\mathcal{D}\\) into a number of non-intersecting triangles which converge in a common edge or corner. Then the initial vertices of the triangles are set at \\(s_1 \\ldots s_d\\). In order to get a proper triangulation, useful for spatial prediction, additional vertices are then added. The more vertices are added the more the triangulation is accurate since many more triangles can better interpolate the surface reaching more complex shapes. Secondly SPDE projects the values of the trinagularization to the dicretized spatial surface with weighted sum of areas of the underlying triangles. A less superficial intuition is offered in the appendix in section ?? on how SPDE computes triangularized valuez and how it projects the triangulation to the GRMF. To illustrate the concept of triangulation Cameletti et al. (2012) provide a simple example for Piemonte PM10 concentration observed at 24 monitoring stations left in figure 5.6 and using 123 vertices and a Piemonte borders, right in figure 5.6. Figure 5.6: Left: monitoring stations in Piemonte region for PM10 pollution levels. Right: its triangulation using 123 vertices. Cameletti et al. (2012) source Any triangle height (the size of the spatial field at each vertix triangle) is calculated by weighted sum, with linear interpolation deciding the values within the triangle. Figure ?? shows a continously indexed random spatial field (left side of figure ??) with the corresponding SPDE on the basis of a triangulation (right panel ??). Figure 5.7: Left: example of a spatial random field where \\(X(s)= \\cos(s_1)+\\sin(s_2)\\), Right: \\(X(s)\\) SPDE representation given a triangulation, Cameletti et al. (2012) source 5.4 Hedonic models Literature Review and Spatial Hedonic Price Models The theoretical foundation of the Hedonic Price Models (from now on HPM) resides in the consumer utility theory of Lancaster (1966) together with Rosen (1974) market equilibrium. According to Lancaster the utility of a commodity does not exist by itself, instead it exists as the sum of the utilities associated to its separable characteristics. Integrating Lancater, Rosen introduces HPM and suggests that each separate commodity characteristics are priced by the markets on the basis of supply and demand equilibrium. Applying HPM to Real Estate in a market context, from the buy side house prices (indeed also rents) are set as the unit cost of each household attributes, conversely from the selling side the expenditures associated to build of each them. Formalizing the results, Hedonic Price \\(P\\) in Real Estate is expressed as a general \\(f\\) functional form that takes as input the house characteristics vector \\(\\mathbf{C} = \\{c_1,c_2, c_3, \\ldots c_n\\}\\). \\[P=f\\left(c_{1}, c_{2}, c_{3}, \\ldots, c_{n}\\right)\\] Vector \\(\\mathbf{C}\\) since now might contain a unidentified and presumably vast number of ungrouped characteristics. In this setting Malpezzi (2008) tried to organize house features by decomposing \\(\\mathbf{C}\\) into mutually exclusive and exhaustive subgroups. An overview of the vector components involved is given by Ling and Ling (2019) according to which the house price \\(P\\) is related to: \\(S\\), the structural characteristics of the house, \\(N\\), the neighborhood characteristics, \\(L\\), the locational characteristics, \\(C\\), the contract conditions and \\(T\\) time dimension. \\(\\beta\\) is the vector of the parameters to be estimated. Therefore: \\[P=f\\left(S, N, L, C, T, \\beta\\right)\\] Historically a first attempt to include spatial effect in urban economic literature is provided by Alonso (1964) miss ref. Its contribution was to raise voice on house prices (also rent) mainly depending on land price and a number of purely spatial covariates like CBD, the distance from City Business District. Other covariates were transport cost per kilometer and community income, even though they were defined also as spatial parameters through distances. The model proposed by Alonso is called monocentric since the centroid from which distances are calculated is only one. Moreover a first touch to spatial data theory was done since the CBD was defined as areal unit with well-defined boundaries of regular or irregular shape. However applications of the model were not convincing since empirical studies offered a different picture. Results displayed a Poly-centric areal structure (universities and Malls) which might be better explaining the variance of prices. The model also assumed that covariates like CBD are only informative within city center boundaries and then show no significance out of the core of the city. Poly-centric theory was also more coherent with the architectural and socio-economical evolution of cities during that times, therefore mono centric theory was then criticized and abandoned. Critics regarded also neighborhood quality measure and boundary problems Dubin (1987) miss ref. Dubin for these reasons developed a model including areal effects in the error term since handling these covariates was posing several hard challenges. Areal data choice for Dubin was forced since he was interested in land values, geostatics interest was not a focus also due to the difficulties in gathering accurate data. Coming to recent literature a change in focus has been made by switching from theory based model to estimation methods. As a consequence to the change in focus Ling and Ling (2019) said that practitioners should spend more time in variable selection and model specification with respect to their specific need. As Ling has observed the emerging trends are in the field of semi-parametric and non-parametric methods (2019). Historically semi-parametric regression considers models indexed by spatial coordinates Pace RK (1995). At the same time Kammann and Wand (2003) gave birth to geoadditive models where the spatial component is added as a covariate. [] A further aspect of the problem is posed by scholars that do not consider rents to be representative for the actual value of real estate. Nevertheless in empirical analysis rent value are considered a proxy for real estate pricing (Herath and Maier 2011). A further argument to endorse this hypothesis is brought by Manganelli, Morano, and Tajani (2013) considering housing a commodity, then the selling or the rental should be considered interchangeable economic actions with respect to same inner need to be satisfied. This is also truer to the thesis extent since Manganelli, Morano, and Tajani have centered their analysis exactly on italian real estate data. Moreover Capozza and Seguin (1996) discussed on how much rent-price ratio predicts future changes both in rents and prices. Among all the other discussions raised they brought the decomposition of rent-price ratio into two parts: the predictable part and the unexplained residuals part. The predictable part was discovered to be negatively correlated with price changes, in other words cities in which prices are relatively high with respect to rents are associated with higher capital gains that might justify that misalignment. This is also true for the opposite, that is cities in which prices are lower with respect to the rents, and this effect can not be associated to any local condition, realize lower capital gains. A further argument is offered by Clark (Clark 1995) which went after the Capozza and Seguin work. Rent-price ratio is negatively correlated with following future changes in rents. In other words prices are still higher when areas in which they are observed documents an increase in rent prices. All the literature review above is oriented to a long-run alignment of price and rent. 5.5 Point Referenced Regression for univariate spatial data Since in HPM the relationships between the characteristics of the house, i.e. vector \\(\\mathbf{C}\\) and the price \\(P\\) is not in any case fixed by econometric literature it is possible to assume any \\(f\\) functional form. The open possibility to apply a wide range of relationship between covariates fit in the INLA setting, since Latent Gaussian Models are prepared to accept a any linear and non linear \\(f\\) functions ?? through the f() method. Hedonic price models are, as a consequence, a subset of models that can be fitted into LGM and therefore by INLA method. Moreover what the vast majority of econometric literature (Greene, 2018) suggest to apply a is log-linear / square root model. This is due to the fact that log transformation / square root smooths the skewness of prices normalizing the curve, leading to more accurate estimates. Having an exponential family generating process lowers even further computational cost for reasons linked to the \\(\\tilde\\pi(\\boldsymbol{\\psi})\\) hyper param INLA approximation (Marta Blangiardo 2015). Notation is taken from the previous chapter 4, for brevity purposes \\(\\boldsymbol{\\beta}\\) \\(\\mathbf{X}\\) and \\(\\boldsymbol{y}\\) indicates vectors incorporating all their respective realizations and the \\(s\\) spatial component is left out in favor of the observation pedix \\(i\\). The simplest log-linear bayesian regression model assumes linear relationship between predictors and a Normal data generating process: (log has been taken out for simplicity, bu it will be then considered in the regression setting) (valuta lidea che per interpretabilità di modellarla come Gamma exponential family anzichè tenerla normale) \\[ \\log{(y_{i})} \\sim \\operatorname{Normal}(\\mu_{i}, \\sigma^{2}) \\] \\[ y_{i}=\\mu_{i}+\\varepsilon_{i} \\] then by the following relationship \\(E\\left(y_{i} \\mid \\beta_{0}, \\ldots, \\beta_{M}, x_{i 1}, \\ldots, x_{i M}\\right)=\\beta_{0}+\\sum_{m=1}^{M} \\beta_{m} x_{i m}\\) it is possible to specify a more general linear predictor (seen also in chapter 4) through an identity link function i.e. \\(\\eta_{i}=g\\left(\\mu_{i}\\right)=\\mu_{i}\\) obtaining: \\[ \\eta_{i}=\\beta_{0}+\\sum_{m=1}^{M} \\beta_{m} x_{m i}+\\sum_{l=1}^{L} f_{l}\\left(z_{l i}\\right) \\] Where, once again, the mean structure linearly depends on some \\(\\mathbf{X}\\) covariates, \\(\\boldsymbol{\\beta}\\) coefficients, \\(f_{l}(\\cdot), \\forall l \\in 1 \\ldots L\\) are a set of random effects defined in terms of a \\(\\boldsymbol{z}\\) set of covariates \\(\\boldsymbol{z}=\\left(z_{1}, \\ldots, z_{L}\\right)\\) (e.g. rw, ar1) and \\(\\varepsilon_{i}\\) white noise error. Priors have to be specified and a non informativeness for \\(\\tau^2 = 1/\\sigma^2\\) and \\(\\boldsymbol{\\beta}\\) is chosen, such that \\(\\pi(\\tau^2) \\propto 1\\) and \\(\\pi(\\boldsymbol\\beta) \\propto 1\\). As a consequence the conditional posterior for the parameters of interest \\(\\boldsymbol{\\beta}\\) is: \\[ \\boldsymbol{\\beta} \\mid \\sigma^{2}, \\boldsymbol{y}, \\boldsymbol{X} \\sim \\operatorname{MVNormal}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{y}, \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) \\] where the mean structure corresponds to the OLS estimator: \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{y}\\) for \\(\\beta\\) and then to obtain the marginal posterior for \\(\\boldsymbol{\\beta}\\) it is needed to integrate with respect to \\(\\sigma^2\\). In order to engage the spatial coordinate components into the regression setting \\(w_{i}\\) has to be added to the equation. \\(w_{i}\\) is set as a stationary and isotropic GP with mean 0 and variance as covariance function expressed as Matérn. Recall that GP The new regression setting integrates the spatial error part in the name of \\(w_{i}\\) and a non-spatial error part \\(\\varepsilon_{i}\\) distributed normally with mean 0 and variance \\(\\tau^2\\) ,i.e. \\(\\mathrm{N}\\left(0, \\tau^{2}\\right)\\), which offers its contribution error to the nuggets via the covariance function. Consequently there is one more parameter to estimate. It is worth mentioning that the distribution of \\(w_{i}\\) at a finite number of points is considered a realization of a multivariate Gaussian distribution. In this case, the likelihood estimation is possible and it is the multivariate Gaussian distribution with covariance \\(\\Sigma\\). \\[ \\log(y_{i})= \\beta_{0} + (\\mathbf{X})^{\\prime}\\boldsymbol{\\beta}+w_{i}+\\varepsilon_{i} \\] The covariance of the marginal distribution of \\(y_{i}\\) at a finite number of locations is \\(\\Sigma_{y} = \\Sigma + \\tau^2\\mathbf{I}\\), where \\(\\mathbf{I}\\) denotes the indicator function (i.e., \\(\\mathbf{I}(i = i^{\\prime})= 1\\) if \\(i = i^{\\prime}\\), and 0 otherwise). This is a short extension of the basic GF model, and gives one additional parameter to estimate \\[\\begin{equation} \\log(y_{i})=\\mu_{i}+\\varepsilon_{i} \\tag{5.1} \\end{equation}\\] where \\(y_{i}\\) is normally distributed as \\(y_{i} \\sim \\operatorname{Normal}\\left(\\mu_{i}, \\sigma^{2}\\right)\\) and \\(\\mu_{i}\\) is the mean structure that linearly depends on some \\(\\mathbf{X}\\) covariates, \\(\\boldsymbol{\\beta}\\) coefficients, \\(f_{l}(\\cdot), \\forall l \\in 1 \\ldots L\\) are a set of random effects defined in terms of a \\(\\boldsymbol{z}\\) set of covariates \\(\\boldsymbol{z}=\\left(z_{1}, \\ldots, z_{L}\\right)\\) (e.g. rw, ar1) and \\(\\varepsilon_{i}\\) white noise error. Please recall that the \\(i\\)th pedices are the observations and the \\(m\\)th pedices are the covariates. The structure is a repetition of what already seen in chapter ?? \\[\\begin{equation} \\eta_{i}=\\beta_{0}+\\sum_{m=1}^{M} \\beta_{m} x_{m i}+\\sum_{l=1}^{L} f_{l}\\left(z_{l i}\\right) \\tag{5.2} \\end{equation}\\] The link function specified in this case is still identity, so that \\(\\eta_{i}=g\\left(\\mu_{i}\\right)=\\mu_{i}\\). Nevertheless GLMs can be applied with different link function. when response variable has to stay between \\([0,1]\\) (e.g. probabilities), the link function might be Logit, which leads to logistic regression. More generally expressed in vector notation: \\[\\begin{equation} \\log(y_{i})\\left(\\mathbf{s}_{i}\\right)=\\mathbf{x}\\left(\\mathbf{s}_{i}\\right)^{\\prime} \\beta_{j}+\\varepsilon\\left(\\mathbf{s}_{i}\\right) \\tag{5.3} \\end{equation}\\] where its OLS estimator is: \\[\\begin{equation} \\hat{\\beta}=\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{y} \\tag{5.4} \\end{equation}\\] Moreover In the context of bayesian analysis a prior distribution has to be imposed on the regression coefficients \\(\\beta = \\left\\{\\beta_{0}, \\ldots, \\beta_{J}\\right\\}\\) as well ad on the variance \\(\\sigma^{2}\\) of \\(y_{i}\\). When no expert information is provided vague priors are introduced, meaning that the regression should not be weighted too much on the priors choice. Vague priors might be: \\(\\beta_{m} \\sim \\operatorname{Normal}\\left(0,10^{6}\\right)\\) for the beta coefficients \\(\\log (\\tau)=\\log \\left(1 / \\sigma^{2}\\right) \\sim \\log \\operatorname{Gamma}\\left(1,10^{-5}\\right)\\) for precision Spatial modeling goal is to include spatial information from location into the model. This is done within the bayesian frameoùwork and INLA by adding \\(w(\\mathbf{s})\\) in the previous equation (5.3). \\[y\\left(\\mathbf{s}_{i}\\right)=\\mathbf{x}\\left(\\mathbf{s}_{i}\\right)^{\\prime}\\beta_{j}+w(\\mathbf{s})+\\varepsilon\\left(\\mathbf{s}_{i}\\right)\\] The \\(w(\\mathbf{s})\\) in the context of the analysis is approached as a stationary and isotropic GP5.1 whose distribution by definition is multivariate Gaussian with mean \\(\\boldsymbol{\\mu}(\\mathbf{s}) = 0\\) and function of the spatial index \\(\\mathbf{s}\\) and covariance function \\(\\mathcal{C}( \\cdot \\mid \\theta)\\) . \\(\\varepsilon(\\mathbf{s})\\) is iid and mean centered in 0 with variance \\(\\tau^{2}\\) and is called non-spatial error since it contributes to the nugget. The error term is pure since it interferes with the covariance function so that the model can embody the spatial component. One of the major advantages of having a a spatial process embedded into a GP is likelihood based inference. 5.5.1 Parameter estimation Gaussian spatial models can be considered as GLM with a particular specification of the precision matrix \\(\\Sigma_{\\theta}=\\sigma^{2} \\mathbf{R}(\\phi)+\\tau^{2} I_{n}\\), then the likelihood can be computed by: \\[\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\boldsymbol{\\beta} \\sim \\mathrm{N}\\left(\\mathbf{X} \\beta, Q_{\\theta}\\right)\\] where, \\(\\boldsymbol{\\theta}=\\left(\\sigma^{2}, \\tau^{2}, \\phi\\right)\\) Since likelihood estimation is possible then MLE can be computed for \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\theta}\\) are \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\theta}}\\). Then the estimation in vector notation is: \\[\\hat{\\boldsymbol{\\beta}}_{M L E}=\\left(\\mathbf{X}^{\\prime} Q^{-1} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} Q^{-1} \\mathbf{y}\\] 5.6 Spatial Kriging (prediction) In Geostatistics the main interest resides in the spatial prediction of the spatial latent field pr the response variable at location not yet observed. Assumed the model in the previous section, suppose that \\(y^{\\star}\\) is not a observed occurrence of the response variable at location \\(s_{0}\\) (not in the data) of the GP \\(w_{i}\\) spatial surface estimated through observed refereced points in \\(\\boldsymbol{y}\\). As a consequence of exchangeability (first step previous section @ref(inlahier )) then \\(\\boldsymbol{y}^{\\otimes}=\\left\\{\\boldsymbol{y}, y^{\\star}\\right\\}\\). Then considering INLA notation it is obtained: \\[ \\begin{aligned} &amp;\\pi\\left(y^{\\star} \\mid \\boldsymbol{y}\\right)=\\frac{\\pi\\left(\\boldsymbol{y}, y^{\\star}\\right)}{\\pi(\\boldsymbol{y})} \\text { from the conditional probability }\\\\ &amp;=\\frac{\\int \\pi\\left(y^{\\star} \\mid \\theta\\right) \\pi(\\boldsymbol{y} \\mid \\theta) \\pi(\\theta) \\mathrm{d} \\theta}{\\pi(\\boldsymbol{y})} \\text { by exchangeability }\\\\ &amp;=\\frac{\\int \\pi\\left(y^{\\star} \\mid \\theta\\right) \\pi(\\theta \\mid y) \\pi(y) \\mathrm{d} \\theta}{\\pi(y)} \\text { applying Bayes&#39; theorem }\\\\ &amp;=\\int \\pi\\left(y^{\\star} \\mid \\boldsymbol{\\theta}\\right) \\pi(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta} \\end{aligned} \\] A DAG representation might offr the intuition behind Prediction in spatial models: Spatial prediction representation through DAG, source Marta Blangiardo (2015) where \\(\\pi\\left(y^{\\star} \\mid \\boldsymbol{y}\\right)\\) is said predictive distribution and it is meaningful only in the Bayesian framework since the posterior distribution is treated as a random variable, which is totally not true in frequentist statistics. 5.7 Model Checking Once the model is set up and fitted a resampling scheme has to be chosen in order to evaluate the model performance. One of the most used method to assess beyasian model quality is LOOCV cross validation and defualt choice fo R-INLA package. From data is left out one single observation and so that the Validation set is \\(\\boldsymbol{y}_{v} = \\boldsymbol{y}_{-i}\\) and the Assessement set is a \\(\\boldsymbol{y}_{a} = \\boldsymbol{y}_{i}\\) the rest of the observations. Two KPI are assumed to be representative: CPO conditional predictive ordinate (pettit, 1990): \\(CPO_{i} = \\pi(y^{\\star} \\mid \\boldsymbol{y}_{v})\\) PIT probability integral tranform (dawid, 1984): \\(PIT_{i} = \\pi(y^{\\star} &lt; y_{i} \\mid \\boldsymbol{y}_{v})\\) These quantities are used by default by setting control options in the inla(control.compute = list()) list object by setting them equal o TRUE. Inla also provides an inner method to authomatically handlee failing in computing those two quantities, leadind to values of 1 when predictions are not reliable and the ipposite for 0.Moreover the empirical distribution of the PIT can be used to asses predictive performance: if it is Uniform, so there are not values that strongly differ from the others then the model is correctly checked. Otherwise if the dostribtuon almost approxiamtes any of the other possibles then the Cross validation assessement prediction has led incorrectly predict the out of the bag validation sample. Posteerior checking method exploits a full cross validation where \\(\\boldsymbol{y}_{a} = \\boldsymbol{y}_{v}\\) and it is called predictive checks. Th assessement set now is equal to the validation set,a s a consequence all the observation are evaluated twice. 4 quantities are driver to model estimate quality: the posterior predictive distribution: \\(\\pi(y^{\\star} \\mid \\boldsymbol{y}) = \\int \\pi(y^{\\star} \\mid \\theta_{i})\\pi({\\theta_{i}} \\mid \\boldsymbol{y})\\mathrm{d}\\theta_{i}\\) which is the likelihood of a replicate observation. When values are small that indicates that are those values are coming from tails, since the area under the curve (i.e. probability) is less. If this happens for many observation then outliers are driving the model leading to poor estimates the posterior predictive p-value whose math expression is:\\(\\pi(y^{\\star} \\leq y_{i} \\mid \\boldsymbol{y})\\) for which values near to 0 and 1 indicates poor perfomances. Root Mean Square Predictive Error RMSE: \\(\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(y_{i}-{y}^{\\star}_{i})^{2}}\\) \\(R^2\\) R-INLA has already antiticipated in chapter 4 section?? have designed function to compute statistics on posterior distribution as inla.pmarginal() returning the cumulative density distribution. 5.8 Prior Specification References "]]
