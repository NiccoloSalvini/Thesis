<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 INLA computation | RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</title>
  <meta name="description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 INLA computation | RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />
  <meta property="og:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 INLA computation | RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  
  <meta name="twitter:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/logo/spatial_logo.png" />
  <link rel="shortcut icon" href="images/logo/spatial_logo.ico" type="image/x-icon" />
<link rel="prev" href="Infrastructure.html"/>
<link rel="next" href="prdm.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/logo/spatial_logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Web Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#reverse"><i class="fa fa-check"></i><b>2.1</b> A Gentle Introduction on Web Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#anatomy-of-a-url-and-reverse-engineering"><i class="fa fa-check"></i><b>2.2</b> Anatomy of a url and reverse engineering</a><ul>
<li class="chapter" data-level="2.2.1" data-path="scraping.html"><a href="scraping.html#scraping-with-rvest"><i class="fa fa-check"></i><b>2.2.1</b> Scraping with <code id="ContentArchitecture">rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#ProperScraping"><i class="fa fa-check"></i><b>2.3</b> Searching Technique for Scarping</a></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#best-practices"><i class="fa fa-check"></i><b>2.4</b> Scraping Best Practices and Security provisions</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#HTTPmethod"><i class="fa fa-check"></i><b>2.5</b> HTTP overview</a><ul>
<li class="chapter" data-level="2.5.1" data-path="scraping.html"><a href="scraping.html#spoofing"><i class="fa fa-check"></i><b>2.5.1</b> User Agent and further Identification Headers Spoofing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#possibly"><i class="fa fa-check"></i><b>2.6</b> Dealing with failure</a></li>
<li class="chapter" data-level="2.7" data-path="scraping.html"><a href="scraping.html#parallelscraping"><i class="fa fa-check"></i><b>2.7</b> Parallel Scraping</a><ul>
<li class="chapter" data-level="2.7.1" data-path="scraping.html"><a href="scraping.html#parallel-furrrfuture"><i class="fa fa-check"></i><b>2.7.1</b> Parallel furrr+future</a></li>
<li class="chapter" data-level="2.7.2" data-path="scraping.html"><a href="scraping.html#parallel-foreachdofuture"><i class="fa fa-check"></i><b>2.7.2</b> Parallel foreach+doFuture</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="scraping.html"><a href="scraping.html#challenges"><i class="fa fa-check"></i><b>2.8</b> Open Challenges and Further Improvemements</a></li>
<li class="chapter" data-level="2.9" data-path="scraping.html"><a href="scraping.html#legal-profiles"><i class="fa fa-check"></i><b>2.9</b> Legal Profiles</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> API Technology Stack</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#restful-api"><i class="fa fa-check"></i><b>3.1</b> RESTful API</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>3.1.1</b> Plumber HTTP API</a></li>
<li class="chapter" data-level="3.1.2" data-path="Infrastructure.html"><a href="Infrastructure.html#sanitize"><i class="fa fa-check"></i><b>3.1.2</b> Sanitization</a></li>
<li class="chapter" data-level="3.1.3" data-path="Infrastructure.html"><a href="Infrastructure.html#DoS"><i class="fa fa-check"></i><b>3.1.3</b> Denial Of Service (DoS)</a></li>
<li class="chapter" data-level="3.1.4" data-path="Infrastructure.html"><a href="Infrastructure.html#logging"><i class="fa fa-check"></i><b>3.1.4</b> Logging</a></li>
<li class="chapter" data-level="3.1.5" data-path="Infrastructure.html"><a href="Infrastructure.html#docs"><i class="fa fa-check"></i><b>3.1.5</b> RESTful API documentation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>3.2</b> Docker</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.1</b> REST-API container</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>3.3</b> NGINX reverse Proxy Server and Authorization</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#docker-compose"><i class="fa fa-check"></i><b>3.3.1</b> Docker-Compose</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#HTTPS"><i class="fa fa-check"></i><b>3.4</b> HTTPS(ecure) and SSL certificates</a></li>
<li class="chapter" data-level="3.5" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>3.5</b> AWS EC2 instance</a></li>
<li class="chapter" data-level="3.6" data-path="Infrastructure.html"><a href="Infrastructure.html#sdwf"><i class="fa fa-check"></i><b>3.6</b> Software development Workflow</a></li>
<li class="chapter" data-level="3.7" data-path="Infrastructure.html"><a href="Infrastructure.html#further-integrations"><i class="fa fa-check"></i><b>3.7</b> Further Integrations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inla.html"><a href="inla.html"><i class="fa fa-check"></i><b>4</b> INLA computation</a><ul>
<li class="chapter" data-level="4.1" data-path="inla.html"><a href="inla.html#the-class-of-latent-gaussian-models-lgm"><i class="fa fa-check"></i><b>4.1</b> The class of Latent Gaussian Models LGM{# }</a></li>
<li class="chapter" data-level="4.2" data-path="inla.html"><a href="inla.html#approx"><i class="fa fa-check"></i><b>4.2</b> INLA Approximate Inference setting</a></li>
<li class="chapter" data-level="4.3" data-path="inla.html"><a href="inla.html#inlahier"><i class="fa fa-check"></i><b>4.3</b> INLA as a Hierarchical Model</a></li>
<li class="chapter" data-level="4.4" data-path="inla.html"><a href="inla.html#rinla"><i class="fa fa-check"></i><b>4.4</b> R-INLA package in Bayesian Regression</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>5</b> Point Referenced Data Modeling</a><ul>
<li class="chapter" data-level="5.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>5.1</b> Gaussian Process (GP)</a></li>
<li class="chapter" data-level="5.2" data-path="prdm.html"><a href="prdm.html#spatial-covariance-function"><i class="fa fa-check"></i><b>5.2</b> Spatial Covariance Function</a><ul>
<li class="chapter" data-level="5.2.1" data-path="prdm.html"><a href="prdm.html#Matern"><i class="fa fa-check"></i><b>5.2.1</b> Matérn Covariance Function</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="prdm.html"><a href="prdm.html#hedonic-models-literature-review-and-spatial-hedonic-price-models"><i class="fa fa-check"></i><b>5.3</b> Hedonic models Literature Review and Spatial Hedonic Price Models</a></li>
<li class="chapter" data-level="5.4" data-path="prdm.html"><a href="prdm.html#univariateregr"><i class="fa fa-check"></i><b>5.4</b> Point Referenced Regression for univariate spatial data</a><ul>
<li class="chapter" data-level="5.4.1" data-path="prdm.html"><a href="prdm.html#parameter-estimation"><i class="fa fa-check"></i><b>5.4.1</b> Parameter estimation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="prdm.html"><a href="prdm.html#kriging"><i class="fa fa-check"></i><b>5.5</b> Spatial Kriging</a></li>
<li class="chapter" data-level="5.6" data-path="prdm.html"><a href="prdm.html#model-checking"><i class="fa fa-check"></i><b>5.6</b> Model Checking</a></li>
<li class="chapter" data-level="5.7" data-path="prdm.html"><a href="prdm.html#prior-specification"><i class="fa fa-check"></i><b>5.7</b> Prior Specification</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="spde.html"><a href="spde.html"><i class="fa fa-check"></i><b>6</b> SPDE approach</a><ul>
<li class="chapter" data-level="6.1" data-path="spde.html"><a href="spde.html#set-spde-problem"><i class="fa fa-check"></i><b>6.1</b> Set SPDE Problem</a></li>
<li class="chapter" data-level="6.2" data-path="spde.html"><a href="spde.html#spde-within-r-inla"><i class="fa fa-check"></i><b>6.2</b> SPDE within R-INLA</a></li>
<li class="chapter" data-level="6.3" data-path="spde.html"><a href="spde.html#first-point-krainsky-rubio-too-technical"><i class="fa fa-check"></i><b>6.3</b> First Point Krainsky Rubio TOO TECHNICAL</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>7</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="exploratory.html"><a href="exploratory.html#prep"><i class="fa fa-check"></i><b>7.1</b> Data preparation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="exploratory.html"><a href="exploratory.html#maps-and-geo-visualisations"><i class="fa fa-check"></i><b>7.1.1</b> Maps and Geo-Visualisations</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="exploratory.html"><a href="exploratory.html#counts-and-first-orientations"><i class="fa fa-check"></i><b>7.2</b> Counts and First Orientations</a></li>
<li class="chapter" data-level="7.3" data-path="exploratory.html"><a href="exploratory.html#text-mining-in-estate-review"><i class="fa fa-check"></i><b>7.3</b> Text Mining in estate Review</a></li>
<li class="chapter" data-level="7.4" data-path="exploratory.html"><a href="exploratory.html#missing-assessement-and-imputation"><i class="fa fa-check"></i><b>7.4</b> Missing Assessement and Imputation</a><ul>
<li class="chapter" data-level="7.4.1" data-path="exploratory.html"><a href="exploratory.html#missing-assessement"><i class="fa fa-check"></i><b>7.4.1</b> Missing assessement</a></li>
<li class="chapter" data-level="7.4.2" data-path="exploratory.html"><a href="exploratory.html#covariates-imputation"><i class="fa fa-check"></i><b>7.4.2</b> Covariates Imputation</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="exploratory.html"><a href="exploratory.html#model-specification"><i class="fa fa-check"></i><b>7.5</b> Model Specification</a></li>
<li class="chapter" data-level="7.6" data-path="exploratory.html"><a href="exploratory.html#mesh-building"><i class="fa fa-check"></i><b>7.6</b> Mesh building</a><ul>
<li class="chapter" data-level="7.6.1" data-path="exploratory.html"><a href="exploratory.html#shinyapp-for-mesh-assessment"><i class="fa fa-check"></i><b>7.6.1</b> Shinyapp for mesh assessment</a></li>
<li class="chapter" data-level="7.6.2" data-path="exploratory.html"><a href="exploratory.html#building-spde-model-on-mesh"><i class="fa fa-check"></i><b>7.6.2</b> BUilding SPDE model on mesh</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="exploratory.html"><a href="exploratory.html#spatial-kriging-prediction"><i class="fa fa-check"></i><b>7.7</b> Spatial Kriging (Prediction)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modelspec.html"><a href="modelspec.html"><i class="fa fa-check"></i><b>8</b> Model Selection &amp; Fitting</a><ul>
<li class="chapter" data-level="8.1" data-path="modelspec.html"><a href="modelspec.html#model-criticism"><i class="fa fa-check"></i><b>8.1</b> Model Criticism</a></li>
<li class="chapter" data-level="8.2" data-path="modelspec.html"><a href="modelspec.html#spatial-kriging"><i class="fa fa-check"></i><b>8.2</b> Spatial Kriging</a></li>
<li class="chapter" data-level="8.3" data-path="modelspec.html"><a href="modelspec.html#model-checking-1"><i class="fa fa-check"></i><b>8.3</b> Model Checking</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>9</b> Shiny Application</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="9.1" data-path="appendix.html"><a href="appendix.html#gp-basics"><i class="fa fa-check"></i><b>9.1</b> GP basics</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inla" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> INLA computation</h1>
<p>Due to convergence issues and heavy calculations, Bayesian Inference - by MCMC <span class="citation">(Brooks et al. <a href="#ref-mcmc" role="doc-biblioref">2011</a>)</span> and MC techniques - may be a challenge in terms of computational burdens, this is also more critical for spatial and spatio-temporal model settings <span class="citation">(Cameletti et al. <a href="#ref-Cameletti2012" role="doc-biblioref">2012</a>)</span>. The computational aspect refers in particular to the ineffectiveness of linear algebra operations with large dense covariance matrices which are common into the aforementioned settings.
INLA <span class="citation">(Rue, Martino, and Chopin <a href="#ref-Rue2009" role="doc-biblioref">2009</a>)</span> stands for Integrated Nested Laplace Approximation and constitutes a computational alternative to traditional Baeysian Inference estimation methods focused on a special type of gaussian spatial processes Gaussian Markov Random Field (GMRF) <span class="citation">(Rue and Held <a href="#ref-GMRFRue" role="doc-biblioref">2005</a>)</span>. INLA zoom in models that can be expressed as Latent GMRF which are special types of spatial processes shaping spatial data dependence observed on areal units as regular grid, lattice or geospatial data.
This turns out to cause a computational gain reducing time of model fitting <span class="citation">(Gómez Rubio <a href="#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span> since matrices in this context are very sparse. Furthermore, INLA approximates the posterior marginal distributions of the model parameters with Laplace approximation which results in a considerably high accuracy.
Other benefits of using INLA algorithm are capitalized by Rue in <span class="citation">(Rencontres Mathématiques <a href="#ref-YT:Rue" role="doc-biblioref">2018</a>)</span>:</p>
<ul>
<li>Can define very complex/nested parameters models within that framework.</li>
<li>Most important statistical models are actually LGM.</li>
<li>Very good support for spatial models.</li>
<li>Extension to spatio-temporal model</li>
</ul>
<p>The chronological steps followed in the arguments presentation retraces the one from Blangiardo and Cameletti <span class="citation">(Marta Blangiardo <a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span> and Rubio <span class="citation">(Gómez Rubio <a href="#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span>, which is also the default option for all the main authorities in the field. The choice is also to overlook mathematical dense details and directly jump on the statistical intuition and the practical application of the algorithm.</p>
<!-- As a consequence a problem can be reshaped into the LGM framework with the explicit purpose to explore its benefits. When models are reduced to LGMs then joint posterior distribution can be rewritten and then approximated with INLA. -->
<p>In the end it is presented the R-INLA project and the package which are found in last section focusing on the core functions and main arguments.
Notation is inherited by <span class="citation">Marta Blangiardo (<a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span> because it is more straightforward, neverthelesss it differs from the original paper by Rue, Chopin and Martino <span class="citation">(<a href="#ref-Rue2009" role="doc-biblioref">2009</a>)</span>. As further remarks, bold symbols are intended as vectors, so each time they occur they have to be considered like the <em>ensamble</em> of their values. The notation <span class="math inline">\(\pi(\cdot)\)</span> is a generic notation for the density of its arguments.</p>
<div id="the-class-of-latent-gaussian-models-lgm" class="section level2">
<h2><span class="header-section-number">4.1</span> The class of Latent Gaussian Models LGM{# }</h2>
<p>Given some observations <span class="math inline">\(y_{i \ldots n}\)</span>, the goal is to set up a Latent Gaussian Model LGM. As a first step it is convenient (even though not strictly necessary as in <span class="citation">(<a href="#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span>) to specify an <em>exponential family</em> distribution function characterized by some parameters <span class="math inline">\(\phi_{i}\)</span> (usually expressed by the mean <span class="math inline">\(\left.E\left(y_{i}\right)\right)\)</span>) and other hyper-parameters <span class="math inline">\(\psi_{k} ,\forall k \in \ 1\ldots K\)</span>. . Furthermore, these observations will have an associated likelihood <span class="citation">(<a href="#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span>. The parameter <span class="math inline">\(\phi_{i}\)</span> can be defined as an additive <em>latent linear predictor</em> <span class="math inline">\(\eta_{i}\)</span>, as in Krainski and Rubio (<span class="citation">(<a href="#ref-Krainski-Rubio" role="doc-biblioref">2019</a>)</span>), through a <em>link function</em> <span class="math inline">\(g(\cdot)\)</span>, i.e. <span class="math inline">\(g\left(\phi_{i}\right)=\eta_{i}\)</span>. A comprehensive expression of the linear predictor takes into account all the possible effects on covariates:</p>
<p><span class="math display">\[
\eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)
\]</span></p>
<p>where <span class="math inline">\(\beta_{0}\)</span> is the intercept, <span class="math inline">\(\boldsymbol{\beta}=\left\{\beta_{1}, \ldots, \beta_{M}\right\}\)</span> are the coefficients that quantifies the linear effects of covariates <span class="math inline">\(\boldsymbol{x}=\left({x}_{1}, \ldots, {x}_{M}\right)\)</span> and <span class="math inline">\(f_{l}(\cdot), \forall l \in 1 \ldots L\)</span> are a set of random effects defined in terms of a <span class="math inline">\(\boldsymbol{z}\)</span> set of covariates <span class="math inline">\(\boldsymbol{z}=\left(z_{1}, \ldots, z_{L}\right)\)</span> (e.g. rw, ar1) <span class="citation">(Marta Blangiardo <a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span>. As a consequence of the extended possibilities of combining mixed effects into LGMs, they contain a wide range of models e.g. GLM, GAM, GLMM, linear models and spatio-temporal models. This constitutes one of the main advantages of INLA algorithm, since it can fit many different models and integrate older ones with newer parameters. Furthermore INLA contributors recently are extending the methodology to many different areas of application and integrating the LG class with many other random effects <em>miss lit Martins et al., 2013 </em>.
With that said all the latent field components can be grouped into a variable denoted with <span class="math inline">\(\boldsymbol{\theta}\)</span> such that: <span class="math inline">\(\boldsymbol{\theta}=\left\{\beta_{0}, \boldsymbol{\beta}, f\right\}\)</span> whose distribution depends on the hyper-paramenter <span class="math inline">\(\boldsymbol{\psi}\)</span>. The analogue can be repeated for hyper-parameters obtaining <span class="math inline">\(\boldsymbol{\psi} = \left\{\psi_{1}, \ldots, \psi_{K}\right\}\)</span>.
Then the probability distribution function conditioned to both parameters and hyper parameters is:</p>
<p><span class="math display">\[
y_{i} \mid \boldsymbol{\theta}, \boldsymbol{\psi} \sim \pi\left(y_{i} \mid \boldsymbol{\theta},\boldsymbol{\psi}\right)
\]</span></p>
<p>Since data <span class="math inline">\(\left(y_{1}, \ldots, y_{n}\right)\)</span> is drawn by the same distribution family but it is also conditioned to parameters which are said <em>conditional independent</em> <span class="citation">(Rue and Held <a href="#ref-GMRFRue" role="doc-biblioref">2005</a>)</span> (i.e. <span class="math inline">\(\pi\left(\theta_{i}, \theta_{j} \mid \theta_{-i, j}\right)=\pi\left(\theta_{i} \mid \theta_{-i, j}\right) \pi\left(\theta_{j} \mid \theta_{-i, j}\right)\)</span>) , then the joint distribution is given by the product of all the independent parameters i.e. the likelihood. Note that the product index <span class="math inline">\(i\)</span> ranges from 1 to <span class="math inline">\(n\)</span>, i.e. <span class="math inline">\(\mathbf{I} = \left\{1 \ldots n \right\}\)</span>.
In the case when an observation is missing, i.e. <span class="math inline">\(i \notin \mathbf{I}\)</span>, INLA automatically discards missing values from the model estimation <span class="citation">(<a href="#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span>, this would be critical during missing values imputation <a href="#missassimp"><strong>??</strong></a>.
The likelihood expression is, where <span class="math inline">\(\boldsymbol\theta^{\prime}\)</span> is the transposed version of <span class="math inline">\(\boldsymbol\theta\)</span> and the <span class="math inline">\(|\cdot|\)</span> is the determinant:</p>
<p><span class="math display" id="eq:likelihood">\[\begin{equation}
\pi(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i \in \mathbb{I}} \pi\left(y_{i} \mid \theta^{\prime}_{i}, \boldsymbol{\psi}\right)
\tag{4.1}
\end{equation}\]</span></p>
<p>Each data point is connected to a single combination <span class="math inline">\(\theta_{i}\)</span> in the <span class="math inline">\(\boldsymbol{\theta}\)</span> <em>latent field</em>. In fact the latent aspect of the field regards the undergoing existence of many parameter combination. Furthermore hyper-parameters are by definition independent, in other words <span class="math inline">\(\boldsymbol{\psi}\)</span> is the product of many univariate priors <span class="citation">(Gómez Rubio <a href="#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span>. A Multivariate Normal distribution prior is imposed on the latent field <span class="math inline">\(\boldsymbol{\theta}\)</span> such that it is centered in 0 with precision matrix <span class="math inline">\(\boldsymbol{Q(\psi)}\)</span> (the inverse of the covariance matrix <span class="math inline">\(\boldsymbol{Q}^{-1}(\boldsymbol{\psi})\)</span>) depending only on <span class="math inline">\(\boldsymbol{\psi}\)</span> hyper-parameter vector i.e., <span class="math inline">\(\boldsymbol{\theta} \sim \operatorname{Normal}\left(\mathbf{0}, \boldsymbol{Q}^{-1}(\boldsymbol{\psi})\right)\)</span>. As a notation remark some authors choose to keep the covariance matrix expression as <span class="math inline">\(\boldsymbol{Q}\)</span> and its inverse precision matrix as <span class="math inline">\(\boldsymbol{Q}^{-1}\)</span>, equation <a href="inla.html#eq:gmrf">(4.2)</a>. Using the covariance instead of precision is strongly not encouraged essentially for two reasons: the first is a practical one and regards the default hyper-paramater argument option in R INLA library, which adopts precision matrix notation. While the second accounts <span class="citation">(Rue and Held <a href="#ref-GMRFRue" role="doc-biblioref">2005</a>)</span> the relationship between conditional independence and the
zero structure of the precision matrix, left in figure <a href="inla.html#fig:precvscov">4.1</a>, that is dense in the covariance matrix, right in figure <a href="inla.html#fig:precvscov">4.1</a> notation case.</p>
<div class="figure"><span id="fig:precvscov"></span>
<img src="images/precvscov.jpg" alt="" />
<p class="caption">Figure 4.1: Precision Matrix in GMRF vs the Covariance matrix, source <span class="citation">Rue and Held (<a href="#ref-GMRFRue" role="doc-biblioref">2005</a>)</span></p>
</div>
<p>The exponential family density function can be rewritten as:</p>
<p><span class="math display" id="eq:gmrf">\[\begin{equation}
\pi(\boldsymbol{\theta} \mid \boldsymbol{\psi})=(2 \pi)^{-n / 2}| \boldsymbol{Q(\psi)}|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta} \boldsymbol{Q(\psi)} \boldsymbol{\theta}\right)
\tag{4.2}
\end{equation}\]</span></p>
<p>The conditional independence assumption on each <span class="math inline">\(\theta_i\)</span> element of the latent field <span class="math inline">\(\boldsymbol{\theta}\)</span> leads <span class="math inline">\(\boldsymbol{Q^{-1}(\psi)}\)</span> to be a sparse precision matrix since for a general pair of combinations <span class="math inline">\(\theta_{i}\)</span> and <span class="math inline">\(\theta_{j}\)</span>, when $ i j$, the resulting element in the precision matrix is 0 i.e. <span class="math inline">\(\theta_{i} \perp \theta_{j} \mid \theta_{-i, j} \Longleftrightarrow Q_{i j}(\boldsymbol{\psi})=0\)</span> <span class="citation">(<a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span>, where the notation <span class="math inline">\(x_{-i}\)</span> denotes all elements in <span class="math inline">\(\boldsymbol{x}\)</span> but <span class="math inline">\(x_{-i}\)</span>.
A probability distribution whose characteristics are the aforementioned is named <em>Gaussian Markov random field</em> (<strong>GMRF</strong>). GMRF as a matter of fact are Gaussian Variables with <em>Markov properties</em> encoded in the precision matrix <span class="math inline">\(\boldsymbol{Q}\)</span> <span class="citation">(Rue, Martino, and Chopin <a href="#ref-Rue2009" role="doc-biblioref">2009</a>)</span>. The computational gain of doing inference with a GMRF is directly related to the sparse precision matrix <span class="math inline">\(\boldsymbol{Q}\)</span> structure. In fact, it can be done using numerical methods for the sparse matrices in linear algebras, leading to a substantial computational benefit <span class="citation">(Cameletti et al. <a href="#ref-Cameletti2012" role="doc-biblioref">2012</a>)</span>, <span class="citation">(Rue and Held <a href="#ref-GMRFRue" role="doc-biblioref">2005</a>)</span>, for the specific algorithms.
Once priors distributions are specified both for<span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\boldsymbol{\psi}\)</span>, then the joint posterior distribution is obtained by the product of the <em>GMRF</em> <a href="inla.html#eq:gmrf">(4.2)</a> density, the <em>likelihood</em> <a href="inla.html#eq:likelihood">(4.1)</a> and the hyper-parameter prior distribution:</p>
<p><span class="math display">\[
\pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y)\propto  \underbrace{\pi(\boldsymbol{\psi})}_{\text {prior }} \times \underbrace{\pi(\theta \mid \psi)}_{\text {GMRF }} \times \underbrace{\prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)}_{\text {likelihood }}
\]</span>
Which can be further rewritten as in <span class="citation">(Marta Blangiardo <a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span> as:</p>
<p><span class="math display">\[
\begin{aligned}
\pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y) &amp; \propto \pi(\boldsymbol{\psi}) \times \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \pi(y \mid \boldsymbol{\theta}, \boldsymbol{\psi}) \\
&amp; \propto \pi(\boldsymbol{\psi}) \times \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right) \\
&amp; \propto \pi(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}\right) \times \prod_{i}^{n} \exp \left(\log \left(\pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right)
\end{aligned}
\]</span></p>
<p>In the end joining exponents by their multiplicative property</p>
<p><span class="math display" id="eq:jointpostdistr">\[\begin{equation}
\pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y) \propto \pi(\psi) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}+\sum^{n} \log \left(\pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right)
\tag{4.3}
\end{equation}\]</span></p>
</div>
<div id="approx" class="section level2">
<h2><span class="header-section-number">4.2</span> INLA Approximate Inference setting</h2>
<p>INLA is not going to try to estimate the whole joint posterior distribution from expression <a href="inla.html#eq:jointpostdistr">(4.3)</a>. Instead it will try to estimate the posterior marginal distribution for each <span class="math inline">\(\theta_{i}\)</span> combination in the latent parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>, given the hyper parameter priors specification <span class="math inline">\(\psi_{k}\)</span>. Proper estimation methodsa through Laplace Approximation however are beyond the scope of the analysis, further excellent references focused on the task are Rubio <span class="citation">(<a href="#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span> in section 2.2.2 and Blangiardo &amp; Cameletti <span class="citation">(<a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span> in section 4.7.2.
The final goal of Bayesian Inference is to compute marginal posterior distribution function for each latent parameter element <span class="math inline">\(\theta_{i}\)</span> in <span class="math inline">\(\boldsymbol\theta\)</span></p>
<p><span class="math display" id="eq:latentparam">\[\begin{equation}
  \pi(\theta_{i} \mid \boldsymbol{y})=\int \pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \mathbf{y}) \pi(\boldsymbol{\psi} \mid \mathbf{y}) d \psi
\tag{4.4}
\end{equation}\]</span></p>
<p>As well as the marginal posterior distribution for each hyper-parameter <span class="math inline">\(\psi_{k} \in \boldsymbol\psi\)</span>,</p>
<p><span class="math display" id="eq:hyperparam">\[\begin{equation}
  \pi\left(\psi_{k} \mid y\right)=\int \pi(\boldsymbol{\psi} \mid y) d \psi_{-k}
  \tag{4.5}
\end{equation}\]</span></p>
<!-- Inla computes the posetrior marginals sfor the poster. the terms inside the integrals are approcimated usign laplaece and nad then thsi integrals can be integrate d numericcally by methods. One you have posterior distribution you can compute quantities of interest like means or quantiles.  -->
<p>Both of the integrals in <a href="inla.html#eq:hyperparam">(4.5)</a> and <a href="inla.html#eq:latentparam">(4.4)</a> are integrated over the <span class="math inline">\(\boldsymbol\psi\)</span>, as a result am approximation of the joint posterior distribution is desired <span class="citation">(Krainski et al. <a href="#ref-Krainski2018" role="doc-biblioref">2018</a>)</span>. Following the notation by Rue <span class="citation">(<a href="#ref-Rue2009" role="doc-biblioref">2009</a>)</span> the integral approximations for hyper-parameters are <span class="math inline">\(\tilde\pi\left(\boldsymbol{\psi} \mid \boldsymbol{y}\right)\)</span> and are plugged in <a href="inla.html#eq:latentparam">(4.4)</a> to obtain the approximation for the posterior marginal of the latent parameter.</p>
<p><span class="math display">\[
\tilde{\pi}\left(\theta_{i} \mid y\right) \approx \sum_{j} \tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}^{(j)}, y\right) \tilde{\pi}\left(\boldsymbol{\psi}^{(j)} \mid y\right) \Delta_{j}
\]</span>
where <span class="math inline">\(\Delta_{j}\)</span> are the weights associated with a set of <span class="math inline">\(\psi_{k}\)</span> in a grid <span class="citation">(Krainski et al. <a href="#ref-Krainski2018" role="doc-biblioref">2018</a>)</span>. The estimate of <span class="math inline">\(\tilde\pi\left(\boldsymbol{\psi} \mid \boldsymbol{y}\right)\)</span> can be determined in various ways. In order to minimize numerical error <span class="citation">Rue, Martino, and Chopin (<a href="#ref-Rue2009" role="doc-biblioref">2009</a>)</span> also addresses how this approximation should be.</p>
<!-- - step 1: compute the Laplace approximation $\tilde\pi\left(\boldsymbol{\psi} \mid \boldsymbol{y}\right)$  for each hyper parameters marginal: $\tilde\pi\left(\psi_{k} \mid \boldsymbol{y}\right)$ -->
<!-- - step 2: compute Laplace approximation $\tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right)$ marginals for the parameters given the hyper parameter approximation in step 1: $\tilde{\pi}\left(\theta_{i} \mid \boldsymbol{y}\right) \approx \int \tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) \underbrace{\tilde{\pi}(\boldsymbol{\psi} \mid \boldsymbol{y})}_{\text {Estim. in step 1 }} \mathrm{d} \psi$ -->
<!-- Then plugging approximation in the integral observed in \@ref(eq:latentparam) it is obtained: -->
<!-- $$ -->
<!-- \tilde{\pi}\left(\theta_{i} \mid y\right) \approx \int \tilde{\pi}\left(\theta_{i} \mid  \boldsymbol{\psi}, y\right) \tilde{\pi}(\boldsymbol{\psi} \mid y) \mathrm{d} \psi -->
<!-- $$ -->
<!-- In the end INLA by its default approximation strategy through  _simplified Laplace approximation_  uses the following numerical approximation to compute marginals:  -->
<!-- $$ -->
<!-- \tilde{\pi}\left(\theta_{i} \mid y\right) \approx \sum_{j} \tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}^{(j)}, y\right) \tilde{\pi}\left(\boldsymbol{\psi}^{(j)} \mid y\right) \Delta_{j} -->
<!-- $$ -->
<!-- where {$\boldsymbol{\psi}^{(j)}$} are a set of values of the hyper param $\psi$ grid used for numerical integration, each of which associated to a specific weight $\Delta_{j}$. The more the weight $\Delta_{j}$ is heavy the more the integration point is relevant. Details on how INLA finds those points is beyond the scope, but the strategy and grids seraches are offered in the appendix follwing both Rubio and Blangiardo. -->
<!-- ### further approximations (prolly do not note include) -->
<!-- INLA focus on this specific integration points by setting up a regular grid about the posterior mode of $\psi$ with CCD (central composite design) centered in the mode [@Bayesian_INLA_Rubio]. -->
<!-- ![CCD to spdetoy dataset, source @Blangiardo-Cameletti](images/CCDapplied.PNG) -->
<!-- The approximation $\tilde{\pi}\left(\theta_{i} \mid y\right)$ can take different forms and be computed in different ways. @Rue2009 also discuss how this approximation should be in order to reduce the numerical error [@Krainski-Rubio]. -->
<!-- Following @Bayesian_INLA_Rubio, approximations of the joint posterior for the hyper paramer $\tilde\pi\left(\psi_{k} \mid \boldsymbol{y}\right)$  is used to compute the marginals for the latent effects and hyper parameters in this way:  -->
<!-- $$ -->
<!-- \left.\tilde{\pi}(\boldsymbol{\psi} \mid \mathbf{y}) \propto \frac{\pi(\boldsymbol{\theta}, \boldsymbol{\psi}, y)}{\tilde{\pi}_{G}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, y)}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{*}(\boldsymbol{\psi})} -->
<!-- $$ -->
<!-- In the previous equation $\tilde{\pi}_{G}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, y)$ is a gaussian approximation to the full condition of the latent effect ${\theta}^{*}(\boldsymbol{\psi})$ is the mode for a given value of the hyper param vector $\boldsymbol{\psi}$  -->
<!-- At this point there exists three types of approximations for $\pi\left(\boldsymbol{\theta} \mid \boldsymbol{\psi}, y\right)$ -->
<!-- - first with a gaussian approximation, estimating mean $\mu_{i}(\boldsymbol{\psi})$ and variance $\sigma_{i}^{2}(\boldsymbol{\psi})$.  -->
<!-- - second using the _Laplace Approximation._  -->
<!-- - third using _simplified Laplace Approximation_ -->
<!-- (rivedere meglio) -->
<!-- prova questo setting  -->
<!-- ### Hierarchical Bayesian models{#hiermod} -->
<!-- Spatial Models are characterized by many parameters which in turn are tuned by other hyper-parameters. Traditionally Bayesian hierarchical models are not widely adopted since they have high computational burdens, indeed they can handle very complex interactions via random components, especially when dealing with spatio temporal data @Ling.  Blangiardo e Cameletti -@Blangiardo-Cameletti tried to approach the problem from a different angle offering an intuitive solution on how hierarchy relates different levels parameters. This is done by reversing the problem and starting from data back to parameters, instead the other way round. So taking a few steps back the problem can be reformulated by starting from grouping observation into categories and then trying to impose a hierarchical structure on data based on the categories. As a result observations might fall into different categories, underlining their natural characteristics, such as: some of them might belong to category _levels_ like males or females, married or not-married. Moreover diving into the specific problem house prices can be faceted by which floor they belong or whether they are assigned to different energy classes and many others more. As an example Blangiardo and Cameletti example consider grouping data according to just a single 9 _levels_ category. Data for the reasons stated before can be organized such that each single observation (squares in figure below) belongs to its respective mutually exclusive and collectively exhaustive category (circles in figure).   -->
<!-- ![9 levels cat vs observaitions, source @Blangiardo-Cameletti](images/simple.PNG) -->
<!-- Furthermore data can be partitioned into two meta-categories, _fist level_ and _second level_,  highlighting the parameter and hyper paramter chain roles. _First level_ are identified by sampling observations which are drawn by the same probability distribution (squares) . _Second level_ (circles) are categories and might be associated to a set of parameters $\theta=\left\{\theta_{1}, \ldots, \theta_{J}\right\}$. -->
<!-- Since the structure is hierarchical, a DAG (Directed Acyclical Graph) -@Blangiardo-Cameletti representation might sort out ideas. If categories are represented by different $\theta_{j}$ nodes and edges (arrows in the figure) are the logical belonging condition to the category then a single parameter $\theta$ model has the right figure form:  -->
<!-- ![DAG representation of hierarchical structure, source @Blangiardo-Cameletti](images/thetas.PNG)  ![chis, Blangiardo-Cameletti's source](images/chis.PNG) -->
<!-- To fully take into account the hierarchical structure of the data the model should also consider further lelvels. Since $\left\{\theta_{1}, \ldots, \theta_{J}\right\}$ are assumed to come from the same distribution $\pi(\theta_{j})$, then they are also assumed to be sharing information [@Blangiardo-Cameletti], (left figure).  When a further parameter $\boldsymbol{\psi}=\left\{\psi_{1}, \ldots, \psi_{K}\right\}$ is introduced, for which a prior distribution is specified, then the conditional distribution of $\boldsymbol{\theta}$ given $\boldsymbol{\psi}$ is: -->
<!-- $$ -->
<!-- \pi\left(\theta_{1}, \ldots, \theta_{J} \mid \boldsymbol{\psi}\right)=\int \prod_{j=1}^{J} \pi\left(\theta_{j} \mid \psi\right) \pi(\psi) \mathrm{d} \psi -->
<!-- $$ -->
<!-- This is possible thanks to the conditional independence property already encountered in chapter \@ref(inla), which means that each single $\theta$ is conditional independent given $\psi$ -->
<!-- This structure can extended to allow more than two levels of hierarchy since the marginal prior distributions of $\theta$ can be decomposed into the product of their conditional priors distributions given some hyper parameter $\psi$ as well as their prior distribution $\pi(\psi)$. -->
<!-- $$ -->
<!-- \pi(\boldsymbol{\theta})=\int \pi\left(\boldsymbol{\theta} \mid \boldsymbol{\psi}_{1}\right) \pi\left(\boldsymbol{\psi}_{1} \mid \boldsymbol{\psi}_{2}\right) \ldots \pi\left(\boldsymbol{\psi}_{L-1} \mid \boldsymbol{\psi}_{L}\right) \pi\left(\boldsymbol{\psi}_{L}\right) \mathrm{d} \boldsymbol{\psi}_{1} \ldots \mathrm{d} \boldsymbol{\psi}_{L} -->
<!-- $$ -->
<!-- $\boldsymbol{\psi}_{l}$ identifies the hyper pram for the $l_{th}$ level of hierarchy. Each further parameter level $\psi$ is conditioned to its previous in hierarchy level $l-1$ so that the parameter hierarchy chain is respected and all the linear combinations of parameters are carefully evaluated. The *Exchangeability* property enables to have higher $H$ nested DAG (i.e. add further $L$ levels) and to extend the dimensions in which the problem is evaluated, considering also time together with space. From a theoretical point of view there are no constraints to how many $L$ levels can be included in the model, but as a drawback the more the model is nested the more it suffers in terms of interpretability and computational power. Empirical studies have suggest that three levels are the desired amount since they offer a good bias vs variance trade-off. -->
</div>
<div id="inlahier" class="section level2">
<h2><span class="header-section-number">4.3</span> INLA as a Hierarchical Model</h2>
<p>INLA setting presented in section <a href="inla.html#approx">4.2</a> can be reorganized following a <em>Hierarchical structure</em> which allows to handle different level parameters.
Since each of the element of the latent field <span class="math inline">\(\boldsymbol{\theta}\)</span> defined in section <a href="#LGM"><strong>??</strong></a>, which groups all the latent components, is assumed to be similar to each of the other. And since each element comes from a distribution <span class="math inline">\(\pi\left(\theta_{j} \mid \psi\right)\)</span> sampled with the same hyper parameters, <span class="math inline">\(\boldsymbol \psi\)</span>. Then there are at least two different levels of the analysis. One lower that regards the <span class="math inline">\(\theta_j\)</span> depending on the one higher the <span class="math inline">\(\boldsymbol\psi\)</span>.
The fact that <span class="math inline">\(\theta_j\)</span> are generated by the same distribution authorizes each <span class="math inline">\(\theta_j\)</span> and <span class="math inline">\(\theta_i\)</span> (<span class="math inline">\(i \neq j\)</span>) to <em>exchange</em> information, which it is totally different from model settings seen before since they were totally independent. As an example under the frequentist assumption of iid samples the joint prior distribution for <span class="math inline">\(\boldsymbol\theta\)</span> can be rewritten in terms of the product of the each marginal distributions, i.e <em>likelihood</em>:</p>
<p><span class="math display">\[
\pi\left(\theta_{1}, \ldots, \theta_{J}\right)=\prod_{j=1}^{J} \pi\left(\theta_{j}\right)=\pi(\theta)^{J}
\]</span>
Indeed when a hierarchical structure is imposed on the parameters <span class="math inline">\(\boldsymbol\theta\)</span> each single one is said <em>Exchangeable</em> to the other with respect to the same random generating process. All the <span class="math inline">\(\theta_j\)</span> share the same distribution characterized by the hyper-parameters <span class="math inline">\(\boldsymbol\psi\)</span>.</p>
<p><span class="math display" id="eq:exchange">\[\begin{equation}
  \pi\left(\theta_{1}, \ldots, \theta_{J} \mid \psi\right)=\int \prod_{j=1}^{J} \pi\left(\theta_{j} \mid \psi\right) \pi(\psi) \mathrm{d} \psi
\tag{4.6}
\end{equation}\]</span></p>
<p>One of the major benefits of expressing hierarchy through integral in <a href="inla.html#eq:exchange">(4.6)</a> is the fact that levels can be extended to more than, say 3 (spatio-temporal models <span class="citation">(<span class="citeproc-not-found" data-reference-id="PACI2017149"><strong>???</strong></span>)</span>).
Then when a Hierarchical structure is imposed on INLA at first, following <a href="#LGM"><strong>??</strong></a>, it is required to specify a probability distribution for <span class="math inline">\(\boldsymbol{y} = \left(y\left(s_{1}\right), \ldots, y\left(s_{n}\right)\right)=\left(y_{1}, \ldots, y_{n}\right)\)</span>. A Gaussian distribution for simplicity is chosen.</p>
<p>As a <em>first level</em> parameters are picked up an <strong>exponential family</strong> sampling distribution (i.e. Normally distributed, Gamma one other choice), which is <em>exchangeable</em> with respect to the <span class="math inline">\(\boldsymbol{\theta}=\left\{\beta_{0}, \boldsymbol{\beta}, f\right\}\)</span> <em>latent field</em> and hyper parameters <span class="math inline">\(\boldsymbol{\psi_{1}}\)</span>, which includes also the ones coming from the latent Matérn GP process <span class="math inline">\(w_{i}\)</span>. The Spatial Guassian Process is centered in 0 and with Matérn covariance function as <span class="math inline">\(\tau^2\)</span>. <span class="math inline">\(w_{i}\)</span> addresses the spatial autocorrelation between observation through a Matérn covariance function <span class="math inline">\(\mathcal{C}(\cdot | \boldsymbol\psi_{1})\)</span> which in turn is tuned by hyper param included in <span class="math inline">\(\boldsymbol{\psi_1}\)</span>. Moreover the <span class="math inline">\(w_{i}\)</span> surface has to be passed in the formula method definition <a href="#example"><strong>??</strong></a> via the <code>f()</code> function, so that INLA takes into cosideration the spatial component.</p>
<p><span class="math display">\[
\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}_{1} \sim \mathrm{N}\left(\beta_{0}+ (\mathbf{X}_{i})^{\prime}\boldsymbol{\beta} + w_{i} ,  \tau^2 I_{n}\right)=\prod_{i=1}^{n} \mathrm{N}\left(y_{i} \mid \theta_{i}, \psi_{1}\right)
\]</span></p>
<p>Then at the <em>second level</em> the latent field <span class="math inline">\(\boldsymbol{\theta}\)</span> is characterized by a Normal distribution given the remaining hyper parameters <span class="math inline">\(\boldsymbol{\psi}_2\)</span>, recall the covariance matrix <span class="math inline">\(\boldsymbol{Q}^{-1}(\boldsymbol{\psi_{2}})\)</span>, depending on <span class="math inline">\(\boldsymbol{\psi_{2}}\)</span> hyperparameters, is handled now by a Matérn covariace function depeding on its hyperparamter. This is done in order to map the GP spatial surface into a GMRF by SPDE solutions.</p>
<p><span class="math display">\[
\boldsymbol{\theta} \mid \boldsymbol{\psi}_{2} \sim \mathrm{N}\left(\boldsymbol{0}, \mathcal{C}( \cdot , \cdot  \mid \boldsymbol{\psi}_{2})\right)
\]</span></p>
<p>In the end hyper parameters <span class="math inline">\(\boldsymbol{\psi}=\left\{\boldsymbol{\psi_{1}}, \boldsymbol{\psi}_{2}\right\}\)</span> having some specified prior distribution i.e. <span class="math inline">\(\boldsymbol{\psi} \sim \pi(\boldsymbol{\psi})\)</span>,</p>
</div>
<div id="rinla" class="section level2">
<h2><span class="header-section-number">4.4</span> R-INLA package in Bayesian Regression</h2>
<p>INLA library and algoririthm is developed by the R-INLA project whose package is available on their website at their <a href="http://www.r-inla.org">source</a> repository. Users can also enjoy on INLA website a forum where daily discussion group are opened and an active community is keen to answer. Moreover It also contains a number of reference books, among which some of them are fully open sourced.
The core function of the package is <code>inla()</code>and it works as many other regression functions like <code>glm()</code>, <code>lm()</code> or <code>gam()</code>. Inla function takes as arguments the formula (where are response and linear predictor), the data (expects a data.frame obj) on which estimation is desired together with the distribution of the data. Many other methods inside the function can be added through lists, such as <code>control.family</code> and <code>control.fixed</code> which let the analyst specifying priors distribution both for <span class="math inline">\(\boldsymbol{\theta}\)</span> parameters, <span class="math inline">\(\boldsymbol{\psi}\)</span> hyper parameters and the outcome precision <span class="math inline">\(\tau\)</span>, default values are non-informative.
<code>control.fixed</code> as said regulates prior specification through a plain list when there only a single fixed effect, instead it does it with nested lists when fixed effects are greater than 2, a guided example might better display the behaviour:
<code>control.fixed = list(mean = list(a = 1, b = 2, default = 0))</code>
In the chuck above it is assigned prior mean equal to 1 for fixed effect “a” and equal 2 for “b”; the rest of the prior means are set equal to 0.
Inla objects are inla.dataframe summary-type lists containing the results from model fitting. Objects contained in the <code>inla()</code> output function are summarized in <a href="#fig:summartable">4.2</a>. Following Krainski &amp; Rubio <span class="citation">(<a href="#ref-Krainski-Rubio" role="doc-biblioref">2019</a>)</span> data <span class="math inline">\(y(s_{1}), \ldots, y(s_{n})\)</span> a taken from a generated toydataset and a hierarchical bayesian linear regression is fitted.</p>
<p><img src="images/" alt="summary table list object, source: Krainski (2019)" />summarytable.PNG</p>
<p>SPDEtoy dataset, that has a spatial component, and is generated from a <span class="math inline">\(y_{i}\)</span> Gaussian variable; its moments are <span class="math inline">\(\mu_{i}\)</span> and precision <span class="math inline">\(\tau\)</span>.</p>
<div class="figure"><span id="fig:SPDEplot"></span>
<img src="images/cotour_toy.png" alt="" />
<p class="caption">Figure 4.3: SPDEtoy plot, author’s source</p>
</div>
<p>The mean moment in the Gaussian distribution <span class="math inline">\(\mu_{i}\)</span> is expressed as the <em>linear predictor</em> <span class="math inline">\(\eta_{i}\)</span> (i.e. <span class="math inline">\(E\left(y_{i} \mid \beta_{0}, \ldots, \beta_{M}, x_{i 1}, \ldots, x_{i M}\right) = \eta_{i}\)</span> ). The function that maps the linear predictor into the parameter space is identity i.e. <span class="math inline">\(\eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)\)</span>.
After including in the model formula <span class="math inline">\(s_{1}\)</span> and <span class="math inline">\(s_{2}\)</span> spatial coordintates the linear predictor takes the following form: <span class="math inline">\(\eta_{i}=\beta_{0}+\beta_{1} s_{1 i}+\beta_{2} s_{2 i}\)</span>, where once again <span class="math inline">\(\beta_{0}\)</span> is the fixed effect i.e. intercept and the <span class="math inline">\(\beta_{j}\)</span> are the linear effect on covariates. INLA allows also to include non-linear effects with the <code>f()</code> method inside the formula. <code>f</code> are pivotal in the follwing chpater <a href="prdm.html#prdm">5</a> since it incorporates the spatial component in the model through the Matérn covariance function,<a href="prdm.html#Matern">5.2.1</a>
As the last step prior distributions has to be chosen. The intercept prior is a uniform. Indeed Priors for Gaussian latent parameters are chosen to be vagues and they are set as 0 mean and 0.001 precision. In the end prior for the precision <span class="math inline">\(\tau\)</span> is Gamma with parameters 1 and 0.00005. Models are sensitive to prior choices, as a consequence they are later revised at need.</p>
<p>The summary of the model specifications are:</p>
<p><span class="math display">\[
\begin{aligned}
y_{i} &amp; \sim N\left(\mu_{i}, \tau^{-1}\right), i=1, \ldots, 200 \\
\mu_{i} &amp;=\beta_{0}+\beta_{1} s_{1 i}+\beta_{2} s_{2 i} \\
\beta_{0} &amp; \sim \text { Uniform } \\
\beta_{j} &amp; \sim N\left(0,0.001^{-1}\right), j=1,2 \\
\tau &amp; \sim G a(1,0.00005)
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="inla.html#cb1-1"></a><span class="kw">data</span>(<span class="st">&quot;SPDEtoy&quot;</span>)</span>
<span id="cb1-2"><a href="inla.html#cb1-2"></a>formula =<span class="st"> </span>y <span class="op">~</span><span class="st"> </span>s1 <span class="op">+</span><span class="st"> </span>s2</span>
<span id="cb1-3"><a href="inla.html#cb1-3"></a>m0 =<span class="st"> </span><span class="kw">inla</span>(formula, <span class="dt">data =</span> SPDEtoy)</span></code></pre></div>
<table>
<caption>(#tab:table_INLA)Output summary statistics for the model m0</caption>
<colgroup>
<col width="14%" />
<col width="13%" />
<col width="12%" />
<col width="13%" />
<col width="13%" />
<col width="13%" />
<col width="13%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">0.025quant</th>
<th align="right">0.5quant</th>
<th align="right">0.975quant</th>
<th align="right">mode</th>
<th align="right">kld</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">10.1321487</td>
<td align="right">0.2422118</td>
<td align="right">9.6561033</td>
<td align="right">10.1321422</td>
<td align="right">10.6077866</td>
<td align="right">10.1321497</td>
<td align="right">7e-07</td>
</tr>
<tr class="even">
<td align="left">s1</td>
<td align="right">0.7624296</td>
<td align="right">0.4293757</td>
<td align="right">-0.0814701</td>
<td align="right">0.7624179</td>
<td align="right">1.6056053</td>
<td align="right">0.7624315</td>
<td align="right">7e-07</td>
</tr>
<tr class="odd">
<td align="left">s2</td>
<td align="right">-1.5836768</td>
<td align="right">0.4293757</td>
<td align="right">-2.4275704</td>
<td align="right">-1.5836906</td>
<td align="right">-0.7404955</td>
<td align="right">-1.5836811</td>
<td align="right">7e-07</td>
</tr>
</tbody>
</table>
<p>The table in @ref(tab:table_INLA) offers summary of the posterior marginal values for intercept and covariates’ coefficients, as well as precision. Marginals distributions both for parameters and hyper-parameters can be conveniently plotted as in figure <a href="inla.html#fig:marginalsplot">4.4</a>. From the table it can also be seen that the mean for s2 is negative, so the more is the value of the y-coordinate, the more the response decreases. That is factual looking at the SPDEtoy cotour plot in figure<a href="inla.html#fig:SPDEplot">4.3</a>.</p>
<div class="figure"><span id="fig:marginalsplot"></span>
<img src="images/marginal_distr.png" alt="" />
<p class="caption">Figure 4.4: Linear predictor marginals, plot recoded in <code>ggplot2</code>, author’s source</p>
</div>
<p>R-INLA enables also r-base style function to compute statistics on marginal posterior distributions for the density, distribution as well as the quantile function respectively with <code>inla.dmarginal</code>, <code>inla.pmarginal</code> and <code>inla.qmarginal</code>. One option which, packed into a dedicated function, computes the higher posterior density credibility interval <code>inla.hpdmarginal</code> for a given covariate’s coefficient, such that <span class="math inline">\(\int_{q_{1}}^{q_{2}} \tilde{\pi}\left(\beta_{2} \mid \boldsymbol{y}\right) \mathrm{d} \beta_{2}=0.90\)</span> with .1 Confidence Level, whose result is in table @ref(tab:higer_posterior_density_interval).</p>
<table>
<caption>(#tab:higer_posterior_density_interval)Higer Posterior Density Interval with .1 Confidence Level probability</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">low</th>
<th align="right">high</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">level:0.9</td>
<td align="right">-2.291268</td>
<td align="right">-0.879445</td>
</tr>
</tbody>
</table>
<p>Note that the interpretation is different from the traditional frequentist approach: in Bayesian statistics <span class="math inline">\(\beta_{j}\)</span> comes from probability distribution, while frequenstists considers <span class="math inline">\(\beta_{j}\)</span> as fixed unknown quantity whose estimator (random variable conditioned to data) is used to infer the value <span class="citation">(<a href="#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-mcmc">
<p>Brooks, S., A. Gelman, G. L. Jones, and X.-L Meng. 2011. <em>Handbook of Markov Chain Monte Carlo</em>. <em>Handbook of Markov Chain Monte Carlo</em>.</p>
</div>
<div id="ref-Cameletti2012">
<p>Cameletti, Michela, Finn Lindgren, Daniel Simpson, and Håvard Rue. 2012. “Spatio-Temporal Modeling of Particulate Matter Concentration Through the SPDE Approach.” <em>AStA Advances in Statistical Analysis</em> 97 (2): 109–31. <a href="https://doi.org/10.1007/s10182-012-0196-3">https://doi.org/10.1007/s10182-012-0196-3</a>.</p>
</div>
<div id="ref-Bayesian_INLA_Rubio">
<p>Gómez Rubio, Virgilio. 2020. <em>Bayesian Inference with Inla</em>. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9781315175584">https://doi.org/10.1201/9781315175584</a>.</p>
</div>
<div id="ref-Krainski2018">
<p>Krainski, Elias, Virgilio Gómez-Rubio, Haakon Bakka, Amanda Lenzi, Daniela Castro-Camilo, Daniel Simpson, Finn Lindgren, and Håvard Rue. 2018. <em>Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA</em>. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9780429031892">https://doi.org/10.1201/9780429031892</a>.</p>
</div>
<div id="ref-Krainski-Rubio">
<p>Krainski, Elias T. 2019. <em>Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and Inla</em>. Chapman; Hall/CRC.</p>
</div>
<div id="ref-Blangiardo-Cameletti">
<p>Marta Blangiardo, Michela Cameletti. 2015. <em>Spatial and Spatio-Temporal Bayesian Models with R-Inla</em>. Wiley.</p>
</div>
<div id="ref-YT:Rue">
<p>Rencontres Mathématiques, Centre International de. 2018. “Håvard Rue: Bayesian Computation with Inla.” Youtube. <a href="https://www.youtube.com/watch?v=a8QvxCjWieg&amp;t=2538s">https://www.youtube.com/watch?v=a8QvxCjWieg&amp;t=2538s</a>.</p>
</div>
<div id="ref-GMRFRue">
<p>Rue, Havard, and Leonhard Held. 2005. <em>Gaussian Markov Random Fields</em>. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9780203492024">https://doi.org/10.1201/9780203492024</a>.</p>
</div>
<div id="ref-Rue2009">
<p>Rue, Håvard, Sara Martino, and Nicolas Chopin. 2009. “Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 71 (2): 319–92. <a href="https://doi.org/10.1111/j.1467-9868.2008.00700.x">https://doi.org/10.1111/j.1467-9868.2008.00700.x</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Infrastructure.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="prdm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/thesis/edit/master/04-inla.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
