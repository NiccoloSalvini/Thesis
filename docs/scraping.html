<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Scraping | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</title>
  <meta name="description" content="Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Scraping | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />
  <meta property="og:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Scraping | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA" />
  
  <meta name="twitter:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="intro.html"/>
<link rel="next" href="Infrastructure.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"> End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#what-is-web-scraping"><i class="fa fa-check"></i><b>2.1</b> What is Web Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#immobiliare.it-webscraping"><i class="fa fa-check"></i><b>2.2</b> Immobiliare.it Webscraping</a><ul>
<li class="chapter" data-level="2.2.1" data-path="scraping.html"><a href="scraping.html#measure-the-shortest-path"><i class="fa fa-check"></i><b>2.2.1</b> measure the shortest path</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#scraping-best-practices-and-robot.txt"><i class="fa fa-check"></i><b>2.3</b> Scraping Best Practices and Robot.txt</a></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#user-agents-proxies-handlers"><i class="fa fa-check"></i><b>2.4</b> User agents, Proxies, Handlers</a><ul>
<li class="chapter" data-level="2.4.1" data-path="scraping.html"><a href="scraping.html#user-agents-spoofing"><i class="fa fa-check"></i><b>2.4.1</b> User agents Spoofing</a></li>
<li class="chapter" data-level="2.4.2" data-path="scraping.html"><a href="scraping.html#handlers"><i class="fa fa-check"></i><b>2.4.2</b> Handlers</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#scraping-with-rvest"><i class="fa fa-check"></i><b>2.5</b> Scraping with <code>rvest</code></a><ul>
<li class="chapter" data-level="2.5.1" data-path="scraping.html"><a href="scraping.html#parallel-computing"><i class="fa fa-check"></i><b>2.5.1</b> Parallel Computing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#further-improvements"><i class="fa fa-check"></i><b>2.6</b> Further Improvements</a></li>
<li class="chapter" data-level="2.7" data-path="scraping.html"><a href="scraping.html#legal-challenges-ancora-non-validato"><i class="fa fa-check"></i><b>2.7</b> Legal Challenges (ancora non validato)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> Infrastructure</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker-container"><i class="fa fa-check"></i><b>3.2</b> Docker Container</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-docker"><i class="fa fa-check"></i><b>3.2.1</b> What is Docker?</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#what-are-the-main-andvantages-of-using-docker"><i class="fa fa-check"></i><b>3.2.2</b> What are the main andvantages of using Docker</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.3</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api"><i class="fa fa-check"></i><b>3.3</b> API</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-an-api"><i class="fa fa-check"></i><b>3.3.1</b> What is an API</a></li>
<li class="chapter" data-level="3.3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#plumber-api"><i class="fa fa-check"></i><b>3.3.2</b> Plumber API</a></li>
<li class="chapter" data-level="3.3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#immobiliare-api-design"><i class="fa fa-check"></i><b>3.3.3</b> Immobiliare API design</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#aws-ec2-server"><i class="fa fa-check"></i><b>3.4</b> AWS EC2 server</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>4</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="exploratory.html"><a href="exploratory.html#what-data-is"><i class="fa fa-check"></i><b>4.1</b> What data is</a></li>
<li class="chapter" data-level="4.2" data-path="exploratory.html"><a href="exploratory.html#data-glimpse"><i class="fa fa-check"></i><b>4.2</b> Data Glimpse</a></li>
<li class="chapter" data-level="4.3" data-path="exploratory.html"><a href="exploratory.html#explorative-analysis"><i class="fa fa-check"></i><b>4.3</b> Explorative Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="exploratory.html"><a href="exploratory.html#semivariogram-covariogram"><i class="fa fa-check"></i><b>4.3.1</b> Semivariogram Covariogram</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="exploratory.html"><a href="exploratory.html#gaussian-random-fields"><i class="fa fa-check"></i><b>4.4</b> Gaussian random fields</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inla-spde.html"><a href="inla-spde.html"><i class="fa fa-check"></i><b>5</b> INLA computation</a><ul>
<li class="chapter" data-level="5.1" data-path="inla-spde.html"><a href="inla-spde.html#inla"><i class="fa fa-check"></i><b>5.1</b> INLA</a></li>
<li class="chapter" data-level="5.2" data-path="inla-spde.html"><a href="inla-spde.html#laplace-approximation"><i class="fa fa-check"></i><b>5.2</b> Laplace Approximation</a></li>
<li class="chapter" data-level="5.3" data-path="inla-spde.html"><a href="inla-spde.html#the-class-of-latent-gaussian-models"><i class="fa fa-check"></i><b>5.3</b> The Class of Latent Gaussian Models</a></li>
<li class="chapter" data-level="5.4" data-path="inla-spde.html"><a href="inla-spde.html#approximate-bayesian-inference-with-inla"><i class="fa fa-check"></i><b>5.4</b> Approximate Bayesian inference with INLA</a></li>
<li class="chapter" data-level="5.5" data-path="inla-spde.html"><a href="inla-spde.html#stochastic-partial-differential-equation"><i class="fa fa-check"></i><b>5.5</b> Stochastic partial differential equation</a></li>
<li class="chapter" data-level="5.6" data-path="inla-spde.html"><a href="inla-spde.html#the-projector-matrix"><i class="fa fa-check"></i><b>5.6</b> The Projector Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>6</b> Point Referenced Data Modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="prdm.html"><a href="prdm.html#gaussian-random-fields-grf"><i class="fa fa-check"></i><b>6.1</b> Gaussian Random Fields (GRF)</a></li>
<li class="chapter" data-level="6.2" data-path="prdm.html"><a href="prdm.html#the-stochastic-partial-differential-equation-approach-spde"><i class="fa fa-check"></i><b>6.2</b> The Stochastic Partial Differential Equation approach (SPDE)</a></li>
<li class="chapter" data-level="6.3" data-path="prdm.html"><a href="prdm.html#hedonic-class-models-in-points-ref-bayesian-data-analysis"><i class="fa fa-check"></i><b>6.3</b> Hedonic class models in points ref bayesian data analysis</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="shiny-web-app.html"><a href="shiny-web-app.html"><i class="fa fa-check"></i><b>7</b> Shiny Web App</a><ul>
<li class="chapter" data-level="7.1" data-path="shiny-web-app.html"><a href="shiny-web-app.html#example-one"><i class="fa fa-check"></i><b>7.1</b> Example one</a></li>
<li class="chapter" data-level="7.2" data-path="shiny-web-app.html"><a href="shiny-web-app.html#example-two"><i class="fa fa-check"></i><b>7.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>8</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="scraping" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Scraping</h1>
<div id="what-is-web-scraping" class="section level2">
<h2><span class="header-section-number">2.1</span> What is Web Scraping</h2>
<p>Web scraping is a techniques that involves informatics and statistics aimed at extracting data from static or dynamic internet web pages. It can be done automatically, simultaneously as well as by scheduler at certain fixed times. Given the substantial unavailability of fresh data and the absence of API’s to call regarding Milan Real Estate rental markets, this practice has to be applied. The main idea and hope is to open source the scraping so that in the future further analysis will be available without putting effort in gathering data.
Data in webpages appear organized and accessible, however each website has its own content architecture. For content architecture it is meant the way information are stored into a website, typically content is subdivided within nested folders that constitutes a <em>first</em> layer of hierarchy. Some popular examples regards social-networks where posts can be scrolled thoruouth a single page, the wall, instead personal profile belongs to a dedicated url. Online newspapers display their articles in the front wall and by accessing to one of them all the related following articles sometimes can be reached by some sort of arrow pointing right or left. Online Retailers as Amazon, based on a query string, groups inside the webpage (i.e. page number 1) a fixed number of items, having their dedicated url attached to the item. Then Amazon offers the opportunity to skip to the following page, i.e. page 2, searching for another fixed set of items and so on until the ending url. Moreover the way information are displayed in the website reflects both the content the final user expectation on the website and the design expression of the web developer. For these reasons for each category of usage exists a multitude of content architectures for multiple front end languages. Web design, as already said, occupies an important role since the more sophisticated and graphical technologies are implied in the process of building a web page, the more it is harder to scrape information.</p>
<p><img src="images/content-vs-html-title.png" alt="content vs HTML" />
Content distribution is managed by HTTP protocols, which are a common standard to connect web clients. Web browsers (i.e. one among the others web clients) download the content and parses them making possible to read them. The athomical unit of measurement in scraping is the url, which are the locations where this data exchanges abstractly happens.
A <em>second</em> layer of hierarchycity is constituted by the inner nature of the language used for content creation and organization i.e. HTML. HTML stands for Hyper Text Markup Language and … blablabla
HTML due to its hyerarchical structure can be schematically represented as a tree graph where each nodes is a url destination and edges are the connections between a content and the others. In this framework nodes’ respective Weights can be thought as the run time cost to pass from a node to a following children node. In addition the content inside the node take the name of payloads, which is the scope of the scraping processes. Tree base graph are directed graphs (digraph) in the sense that hierarchy by default gives an ordering sense. Therefore from each node the user might access to a number of children nodes through an edge that measures the time spent to get to it. From the same children nodes by means of chronology path it can only be accessed its respective single father-node. CSS stands for Cascading Style Sheets and takes care of the style of the webpage. The combination of two offers a wide flexibility, once again expressed by the vast amount of different websites on the internet. Some components also can be tuned by Javascript language, which adds a further layer of difficulty during scraping. As a matter of fact since Javascript components are dynamic within the webpage, scraping requires specialized libraries and different parser in order to get payloads (e.g. content).</p>
<div class="figure">
<img src="images/htmlstructure.PNG" alt="" />
<p class="caption">HTML strucure, Vincenzo Nardelli creations</p>
</div>
</div>
<div id="immobiliare.it-webscraping" class="section level2">
<h2><span class="header-section-number">2.2</span> Immobiliare.it Webscraping</h2>
<p>What scraping is supposed to do is to access within nodes through edges, minimizing the total run time of the functions. The main idea is to lower the number of nodes, weighting the path by the edges’ weights. By that this becomes a shortest path problem which is propedeutic in the digraph framework. Once provided the root node (concerning the scraping goals) all the paths are available to be reached. Scraping for all the reasons explained really depends on the website the scraper has chosen to extract information from. The graph structure below sketches how scraping has been especially designed for immobiliare.it.</p>
<div class="figure">
<img src="images/tree_stru.jpg" alt="" />
<p class="caption">immobiliare html tree struc</p>
</div>
<div class="figure">
<img src="images/children_str.jpg" alt="" />
<p class="caption">children_str</p>
</div>
<p>The root node allows to generate all the subsequent, ideally parallel root nodes. Parallel root nodes have the same html structure of the “original” root nodes. The difference between root url page 1 and 2 relies in appending at the end of the url the corresponding page number reference the user is in. This can be conveniently done with a <code>str_c()</code> pasting at the end of the “original” root url <code>"&amp;page=1"</code>, <code>"&amp;page=2"</code>, <code>"&amp;page=3"</code> until the ending criteria is matched. The criteria by author choice is to fix the number to 300. Within each root url the children href nodes are stored. Href nodes have as payloads the single rental ad url. A dedicated scraping function has been scripted in order to grab these urls <code>scrape_href()</code> so that they can be own root nodes to further single rental ad scraping. Right inside the html children nodes are stored the most important covariates.</p>
<div id="measure-the-shortest-path" class="section level3">
<h3><span class="header-section-number">2.2.1</span> measure the shortest path</h3>
<p>The approcah followed is to generate all the root nodes url with the aim to then generate all the children urls. The first passage is mandatory since</p>
</div>
</div>
<div id="scraping-best-practices-and-robot.txt" class="section level2">
<h2><span class="header-section-number">2.3</span> Scraping Best Practices and Robot.txt</h2>
<p>Robots.txt files are (rivedi citation) a way to kindly ask webbots, spiders, crawlers, wanderers and the like to access or not access certain parts of a webpage. The de facto ‘standard’ never made it beyond a informal “Network Working Group INTERNET DRAFT”. Nonetheless, the use of robots.txt files is widespread (e.g. <a href="https://en.wikipedia.org/robots.txt" class="uri">https://en.wikipedia.org/robots.txt</a>, <a href="https://www.google.com/robots.txt" class="uri">https://www.google.com/robots.txt</a>) and bots from Google, Yahoo and the like will adhere to the rules defined in robots.txt files, although their <em>interpretation</em> of those rules might differ.</p>
<p>Robots.txt files are plain text and always found at the root of a website’s domain. The syntax of the files in essence follows a fieldname: value scheme with optional preceding user-agent: … lines to indicate the scope of the following rule block. Blocks are separated by blank lines and the omission of a user-agent field (which directly corresponds to the HTTP user-agent field) is seen as referring to all bots. # serves to comment lines and parts of lines. Everything after # until the end of line is regarded a comment. Possible field names are: user-agent, disallow, allow, crawl-delay, sitemap, and host. For further notions <span class="citation">(Meissner and Ren <a href="#ref-robotstxt" role="doc-biblioref">2020</a>, <span class="citation">@google:robottxt</span>)</span></p>
<p>Some interpretation problems:</p>
<ul>
<li>finding no robots.txt file at the server (e.g. HTTP status code 404) implies that everything is allowed</li>
<li>subdomains should have there own robots.txt file if not it is assumed that everything is allowed</li>
<li>redirects involving protocol changes - e.g. upgrading from http to https - are followed and considered no domain or subdomain change - so whatever is found at the end of the redirect is considered to be the - robots.txt file for the original domain</li>
<li>redirects from subdomain www to the doamin is considered no domain change - so whatever is found at the end of the redirect is considered to be the robots.txt file for the subdomain originally requested</li>
</ul>
<p>For the thesis purposes it has been designed a dedicated function to inspect whether the domain requires specific actions or prevents some activity on thw target website. The following <code>checkpermission()</code> function has been integrated inside the scraping architecture and it is called once at the very beginning.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="scraping.html#cb1-1"></a><span class="kw">library</span>(robotstxt)</span>
<span id="cb1-2"><a href="scraping.html#cb1-2"></a>dominio =<span class="st"> &quot;immobiliare.it&quot;</span></span>
<span id="cb1-3"><a href="scraping.html#cb1-3"></a></span>
<span id="cb1-4"><a href="scraping.html#cb1-4"></a>checkpermission =<span class="st"> </span><span class="cf">function</span>(dom) {</span>
<span id="cb1-5"><a href="scraping.html#cb1-5"></a>    </span>
<span id="cb1-6"><a href="scraping.html#cb1-6"></a>    robot =<span class="st"> </span><span class="kw">robotstxt</span>(<span class="dt">domain =</span> dom)</span>
<span id="cb1-7"><a href="scraping.html#cb1-7"></a>    vd =<span class="st"> </span>robot<span class="op">$</span><span class="kw">check</span>()[<span class="dv">1</span>]</span>
<span id="cb1-8"><a href="scraping.html#cb1-8"></a>    <span class="cf">if</span> (vd) {</span>
<span id="cb1-9"><a href="scraping.html#cb1-9"></a>        <span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">robot.txt for&quot;</span>, dom, <span class="st">&quot;is okay with scraping!&quot;</span>)</span>
<span id="cb1-10"><a href="scraping.html#cb1-10"></a>    } <span class="cf">else</span> {</span>
<span id="cb1-11"><a href="scraping.html#cb1-11"></a>        <span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">robot.txt does not like what you&#39;re doing&quot;</span>)</span>
<span id="cb1-12"><a href="scraping.html#cb1-12"></a>        <span class="co">## stop()</span></span>
<span id="cb1-13"><a href="scraping.html#cb1-13"></a>    }</span>
<span id="cb1-14"><a href="scraping.html#cb1-14"></a>}</span>
<span id="cb1-15"><a href="scraping.html#cb1-15"></a><span class="kw">checkpermission</span>(dominio)</span></code></pre></div>
<pre><code>## 
## robot.txt for immobiliare.it is okay with scraping!</code></pre>
<p>Further improvements in this direction came from the <code>polite</code> package <span class="citation">(Perepolkin <a href="#ref-polite" role="doc-biblioref">2019</a>)</span> which combines the power of the <code>robotstxt</code>, the <code>ratelimitr</code> to rate-limiting requests and the <code>memoise</code> for response caching. This package is wrapped up around 3 simple but effective ideas:</p>
<blockquote>
<p>The three pillars of a polite session are seeking permission, taking slowly and never asking twice.</p>
</blockquote>
<p>The three pillars constitute the Ethical web scraping manifesto <span class="citation">(Densmore <a href="#ref-densmore_2019" role="doc-biblioref">2019</a>)</span> which are common
shared practises that are aimed to self regularize scrapers. This has not nothing to do with law but since many scrapers themselves, as website administrators or analyst, have fought with bots.
Bots might fake out real client logs and might stain analytics, so here it is born the choice to fine common ground and politely ask for permission.</p>
</div>
<div id="user-agents-proxies-handlers" class="section level2">
<h2><span class="header-section-number">2.4</span> User agents, Proxies, Handlers</h2>
<p>HTTP requests to the website server by web clients come with some mandatory information packed in it. The process according to which HTTP protocols allow to exchage information can be easily thought with an analogy coming from real wordl. As a generic person A rings the door’s bell of person B’s house. A comes to B door with its personal information, its name, surname, where he lives etc. At this point B may either answer to A requests by opening the door and let him enter given the set of information he has, or it may not since B is not sure of the real intentions of A. This typical everyday situation in nothing more what happens billions of times on the internet everyday, the user (in the example above A) is interacting with a server website (part B) sending packets of information. If a server does not trust the information provided by the user, if the requests are too many, if the requests seems to be scheduled due to fixed sleeping time, a server can block the requests. In certain cases it can even forbid the user to be on the website. The language the two parties talks are coded in numbers that ranges from 100 to 511, each of which ha its own significance. A popular case of this type of interaction occurs when users are not connected to internet so the server responds 404, page not found. Servers are built with a immune-system like software that raises barriers and block users to prevent dossing or other illegal practices.</p>
<div class="figure">
<img src="images/how_web_works.png" alt="" />
<p class="caption">How Web Works</p>
</div>
<p>This procedure is a day to day issue to people that are trying to collect information from websites. Google does it everyday with its spider crawlers, which are very sophisticated bots that performs scarping over a enormous range of websites. This challenge can be addressed in multiple ways, there are some specific Python packages that overcome this issue. The are also certain types of scraping as the Selenium web driver automation that simulates browser automation. Selenium allows the user not to be easily detected by the server immune system and peaceful. In here precautions have not been taken lightly, and a simple but effective approach is proposed.</p>
<div id="user-agents-spoofing" class="section level3">
<h3><span class="header-section-number">2.4.1</span> User agents Spoofing</h3>
<p>A user agent <span class="citation">(“User Agent: Learn Your Web Browsers User Agent Now” <a href="#ref-whoishostingthis.com" role="doc-biblioref">2020</a>)</span> is a string of characters in each web browser that serves as an identification agent. The user agent permits the web server to be able to identify the user operating system and the browser. Then, the web server uses the exchanged information to determine what content should be presented to particular operating systems and web browsers on a series of devices. The user agent string contains the user application or software, the operating system (and their versions), the web client, the web client’s version, and the web engine responsible for the content display (such as AppleWebKit). The user agent string is sent in form of a HTTP request header. Since the User Agents acts as middle man between the client request and the server response what it would be better doing is to actively faking it so that each time a web browser presents himself to a web server it has a different specifications, different web client, different operating system and so on.</p>
<p>The simple approach followed was building a vector of samples of different existing and updated User Agents (UA). Then whenever a request from a browser is served to a web server 1 random string is drawn from the user agents pool. So each time the user is sending the requests it appears to have a different “identity”. Below the user agents rotating pool:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="scraping.html#cb3-1"></a><span class="kw">set.seed</span>(<span class="dv">27</span>)</span>
<span id="cb3-2"><a href="scraping.html#cb3-2"></a>agents =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&quot;</span>, </span>
<span id="cb3-3"><a href="scraping.html#cb3-3"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&quot;</span>, </span>
<span id="cb3-4"><a href="scraping.html#cb3-4"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&quot;</span>, </span>
<span id="cb3-5"><a href="scraping.html#cb3-5"></a>    <span class="st">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14&quot;</span>, </span>
<span id="cb3-6"><a href="scraping.html#cb3-6"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&quot;</span>, </span>
<span id="cb3-7"><a href="scraping.html#cb3-7"></a>    <span class="st">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36&quot;</span>, </span>
<span id="cb3-8"><a href="scraping.html#cb3-8"></a>    <span class="st">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36&quot;</span>, </span>
<span id="cb3-9"><a href="scraping.html#cb3-9"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&quot;</span>, </span>
<span id="cb3-10"><a href="scraping.html#cb3-10"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;</span>, </span>
<span id="cb3-11"><a href="scraping.html#cb3-11"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0&quot;</span>)</span>
<span id="cb3-12"><a href="scraping.html#cb3-12"></a>agents[<span class="kw">sample</span>(<span class="dv">1</span>)]</span></code></pre></div>
<pre><code>## [1] &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&quot;</code></pre>
<p>An improvement to this could be using also rotating proxies. A proxy server acts as a gateway between the user and the server. It’s an intermediary server himself, separating end clients from the websites they are browsing. Proxy servers provide varying layers of functionality, security, and privacy are some of the examples.
While the user is exploiting a proxy server, internet traffic flows through the proxy server on its way to the server you requested. The request then comes back through that same proxy server and then the proxy server forwards the data received from the website to you.
Many proxy servers are offered in a paid version so in this case since security barriers of the target website are not high they will not be implemented. It has to be mentioned that many online services are providing free proxies but the turnaround of this solutions are many, two of them are:
- Proxies to be free are widely shared among people, so as long as someone has used them for illegal purposes the user is inheriting their mistakes when caught.
- Some of those proxies, pretty all the ones coming from low ranked websites, are tracked so there might be a user privacy violation issue.</p>
</div>
<div id="handlers" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Handlers</h3>
<p>During scraping many difficulties might be met. Starting from the things that have been previously explained at the chapter start. Some of them are: a varying url structure so that the html tree hierarchies are changed, payloads are not correctly parsed with respect to previous scraping sessions. ending with the ones that have just been said a few lines ago. Handlers and trycatch error workarounds are explicitly built in this sense. The continuous testing of the scraping functioning while developing has required the maintainer to track where the error occurs. A few numbers: the “agglomerative” function <code>get.data.catsing()</code> triggers more than 36 scrapping functions that are going to catch 36 different data pieces. If one of them went missing then the other one would be missing too. Then when row-data is binded together one entry column might not exists making the process fail.<br />
Then the solution to that is to call inside the aggolmerative function as much as trycatch as many scrapping functions are involved. The trycatch can leverage the gap by introducing a specified quantity and alerting that something went wrong. On top of that many other handlers are called throughout the procedure:</p>
<ul>
<li><code>get_ua()</code> verifies that the user agent coming from the session request is not the default one</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="scraping.html#cb5-1"></a>get_ua =<span class="st"> </span><span class="cf">function</span>(sess) {</span>
<span id="cb5-2"><a href="scraping.html#cb5-2"></a>    <span class="kw">stopifnot</span>(<span class="kw">is.session</span>(sess))</span>
<span id="cb5-3"><a href="scraping.html#cb5-3"></a>    <span class="kw">stopifnot</span>(<span class="kw">is_url</span>(sess<span class="op">$</span>url))</span>
<span id="cb5-4"><a href="scraping.html#cb5-4"></a>    ua =<span class="st"> </span>sess<span class="op">$</span>response<span class="op">$</span>request<span class="op">$</span>options<span class="op">$</span>useragent</span>
<span id="cb5-5"><a href="scraping.html#cb5-5"></a>    <span class="kw">return</span>(ua)</span>
<span id="cb5-6"><a href="scraping.html#cb5-6"></a>}</span></code></pre></div>
<ul>
<li><code>is_url()</code> verifies that the url input needed has the canonic form. This is done by a REGEX query.</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="scraping.html#cb6-1"></a>is_url =<span class="st"> </span><span class="cf">function</span>(url) {</span>
<span id="cb6-2"><a href="scraping.html#cb6-2"></a>    re =<span class="st"> &quot;^(?:(?:http(?:s)?|ftp)://)(?:</span><span class="ch">\\</span><span class="st">S+(?::(?:</span><span class="ch">\\</span><span class="st">S)*)?@)?(?:(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;](?:-)*)*(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;])+)(?:</span><span class="ch">\\</span><span class="st">.(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;](?:-)*)*(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;])+)*(?:</span><span class="ch">\\</span><span class="st">.(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;]){2,})(?::(?:</span><span class="ch">\\</span><span class="st">d){2,5})?(?:/(?:</span><span class="ch">\\</span><span class="st">S)*)?$&quot;</span></span>
<span id="cb6-3"><a href="scraping.html#cb6-3"></a>    <span class="kw">grepl</span>(re, url)</span>
<span id="cb6-4"><a href="scraping.html#cb6-4"></a>}</span></code></pre></div>
<ul>
<li><code>.get_delay()</code> checks through the robotxt file if a delay between each request is kindly welcomed.</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="scraping.html#cb7-1"></a>.get_delay =<span class="st"> </span><span class="cf">function</span>(domain) {</span>
<span id="cb7-2"><a href="scraping.html#cb7-2"></a>  </span>
<span id="cb7-3"><a href="scraping.html#cb7-3"></a>  <span class="kw">message</span>(<span class="kw">sprintf</span>(<span class="st">&quot;Refreshing robots.txt data for %s...&quot;</span>, domain))</span>
<span id="cb7-4"><a href="scraping.html#cb7-4"></a>  </span>
<span id="cb7-5"><a href="scraping.html#cb7-5"></a>  cd_tmp =<span class="st"> </span>robotstxt<span class="op">::</span><span class="kw">robotstxt</span>(domain)<span class="op">$</span>crawl_delay</span>
<span id="cb7-6"><a href="scraping.html#cb7-6"></a>  </span>
<span id="cb7-7"><a href="scraping.html#cb7-7"></a>  <span class="cf">if</span> (<span class="kw">length</span>(cd_tmp) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb7-8"><a href="scraping.html#cb7-8"></a>    star =<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(cd_tmp, useragent<span class="op">==</span><span class="st">&quot;*&quot;</span>)</span>
<span id="cb7-9"><a href="scraping.html#cb7-9"></a>    <span class="cf">if</span> (<span class="kw">nrow</span>(star) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) star =<span class="st"> </span>cd_tmp[<span class="dv">1</span>,]</span>
<span id="cb7-10"><a href="scraping.html#cb7-10"></a>    <span class="kw">as.numeric</span>(star<span class="op">$</span>value[<span class="dv">1</span>])</span>
<span id="cb7-11"><a href="scraping.html#cb7-11"></a>  } <span class="cf">else</span> {</span>
<span id="cb7-12"><a href="scraping.html#cb7-12"></a>    10L</span>
<span id="cb7-13"><a href="scraping.html#cb7-13"></a>  }</span>
<span id="cb7-14"><a href="scraping.html#cb7-14"></a>  </span>
<span id="cb7-15"><a href="scraping.html#cb7-15"></a>}</span>
<span id="cb7-16"><a href="scraping.html#cb7-16"></a></span>
<span id="cb7-17"><a href="scraping.html#cb7-17"></a>get_delay =<span class="st">  </span>memoise<span class="op">::</span><span class="kw">memoise</span>(.get_delay)</span></code></pre></div>
</div>
</div>
<div id="scraping-with-rvest" class="section level2">
<h2><span class="header-section-number">2.5</span> Scraping with <code>rvest</code></h2>
<p>To start a general scraping function it is required a target url i.e. the root node. Then a <code>html_session</code> object, which is the analogue of a GET/POST HTTP request, is created and opened by specifying the root url and the request data that the user might need to send to the web server, (see left part to the dashed line of the image below). Server request are handled by <code>httr</code> since it is the R-gold-standard for server GET and POST HTTP requests and also a dependency of <code>rvest</code>. Information to be attached to the web server request have been already explored in the previous sections, tough they are mainly three: User Agents, emails references and proxy servers. Once the connection is established (request response 200) all the following operations rely on the opened session, in other words for the time being in the session the user will be authorized with the provided characteristics through the request. One of the major privileges of the structure represented below is the iterability along a predefined set of parent/root urls. Since all the parent url can be generated by the first, the structure is self containing and portable. What it is worth noting is that inside each url other sub-urls are given. Those urls are the ones corresponding to each single rental advetisement that have to be inspected singularly one by one by the workflow represented below.</p>
<p><img src="images/workflow.png" alt="workflow" />
What it comes in the right part of the dashed vertical line is a succession of functions that follow a logical step by step path. <code>rvest</code> under the hood handles, in this precise order: reading the html structure of the web page within the session <code>read_html()</code>, parsing the html content looking for a single node <code>html_nodes()</code> and then converting it into readable text <code>html_text()</code>. The entire process can be thought as sort of autoencoder by adding decoding layer after layer on top of the starting url, until reaching the final requested node. the reason why the process appear straightforward is a result of the bleding of <code>rvest</code> together with the tidyverse pipe <code>%&gt;%</code> operator. Below it is shown a function that scrapes the price and composes the API.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="scraping.html#cb8-1"></a>scrapeprice.imm =<span class="st"> </span><span class="cf">function</span>(session) {</span>
<span id="cb8-2"><a href="scraping.html#cb8-2"></a>  </span>
<span id="cb8-3"><a href="scraping.html#cb8-3"></a>  opensess =<span class="st"> </span><span class="kw">read_html</span>(session)</span>
<span id="cb8-4"><a href="scraping.html#cb8-4"></a>  price  =<span class="st"> </span>opensess <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-5"><a href="scraping.html#cb8-5"></a><span class="st">    </span><span class="kw">html_nodes</span>(<span class="dt">css =</span><span class="st">&quot;.im-mainFeatures__title&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-6"><a href="scraping.html#cb8-6"></a><span class="st">    </span><span class="kw">html_text</span>() <span class="op">%&gt;%</span></span>
<span id="cb8-7"><a href="scraping.html#cb8-7"></a><span class="st">    </span><span class="kw">str_trim</span>() </span>
<span id="cb8-8"><a href="scraping.html#cb8-8"></a>  </span>
<span id="cb8-9"><a href="scraping.html#cb8-9"></a>  <span class="cf">if</span>(<span class="kw">is.null</span>(price) <span class="op">||</span><span class="st"> </span><span class="kw">identical</span>(price, <span class="kw">character</span>(<span class="dv">0</span>))) {</span>
<span id="cb8-10"><a href="scraping.html#cb8-10"></a>    price2 =<span class="st"> </span>opensess <span class="op">%&gt;%</span></span>
<span id="cb8-11"><a href="scraping.html#cb8-11"></a><span class="st">      </span><span class="kw">html_nodes</span>(<span class="dt">css =</span><span class="st">&#39;.im-features__value , .im-features__title&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-12"><a href="scraping.html#cb8-12"></a><span class="st">      </span><span class="kw">html_text</span>() <span class="op">%&gt;%</span></span>
<span id="cb8-13"><a href="scraping.html#cb8-13"></a><span class="st">      </span><span class="kw">str_trim</span>()</span>
<span id="cb8-14"><a href="scraping.html#cb8-14"></a>    </span>
<span id="cb8-15"><a href="scraping.html#cb8-15"></a>    <span class="cf">if</span> (<span class="st">&quot;prezzo&quot;</span> <span class="op">%in%</span><span class="st"> </span>price2) {</span>
<span id="cb8-16"><a href="scraping.html#cb8-16"></a>      pos =<span class="st"> </span><span class="kw">match</span>(<span class="st">&quot;prezzo&quot;</span>,price2)</span>
<span id="cb8-17"><a href="scraping.html#cb8-17"></a>      <span class="kw">return</span>(price2[pos<span class="op">+</span><span class="dv">1</span>])  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-18"><a href="scraping.html#cb8-18"></a><span class="st">        </span><span class="kw">str_replace_all</span>(<span class="kw">c</span>(<span class="st">&quot;€&quot;</span>=<span class="st">&quot;&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">.&quot;</span>=<span class="st">&quot;&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-19"><a href="scraping.html#cb8-19"></a><span class="st">        </span><span class="kw">str_extract</span>( <span class="st">&quot;</span><span class="ch">\\</span><span class="st">-*</span><span class="ch">\\</span><span class="st">d+</span><span class="ch">\\</span><span class="st">.*</span><span class="ch">\\</span><span class="st">d*&quot;</span>) <span class="op">%&gt;%</span><span class="st">  </span></span>
<span id="cb8-20"><a href="scraping.html#cb8-20"></a><span class="st">        </span><span class="kw">str_replace_na</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-21"><a href="scraping.html#cb8-21"></a><span class="st">        </span><span class="kw">str_replace</span>(<span class="st">&quot;NA&quot;</span>, <span class="st">&quot;Prezzo Su Richiesta&quot;</span>)</span>
<span id="cb8-22"><a href="scraping.html#cb8-22"></a>    } <span class="cf">else</span> {</span>
<span id="cb8-23"><a href="scraping.html#cb8-23"></a>      <span class="kw">return</span>(<span class="ot">NA_character_</span>)</span>
<span id="cb8-24"><a href="scraping.html#cb8-24"></a>    }</span>
<span id="cb8-25"><a href="scraping.html#cb8-25"></a>  } <span class="cf">else</span> {</span>
<span id="cb8-26"><a href="scraping.html#cb8-26"></a>    <span class="kw">return</span>(price) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-27"><a href="scraping.html#cb8-27"></a><span class="st">      </span><span class="kw">str_replace_all</span>(<span class="kw">c</span>(<span class="st">&quot;€&quot;</span>=<span class="st">&quot;&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">.&quot;</span>=<span class="st">&quot;&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-28"><a href="scraping.html#cb8-28"></a><span class="st">      </span><span class="kw">str_extract</span>( <span class="st">&quot;</span><span class="ch">\\</span><span class="st">-*</span><span class="ch">\\</span><span class="st">d+</span><span class="ch">\\</span><span class="st">.*</span><span class="ch">\\</span><span class="st">d*&quot;</span>) <span class="op">%&gt;%</span><span class="st">  </span></span>
<span id="cb8-29"><a href="scraping.html#cb8-29"></a><span class="st">      </span><span class="kw">str_replace_na</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-30"><a href="scraping.html#cb8-30"></a><span class="st">      </span><span class="kw">str_replace</span>(<span class="st">&quot;NA&quot;</span>, <span class="st">&quot;Prezzo Su Richiesta&quot;</span>)</span>
<span id="cb8-31"><a href="scraping.html#cb8-31"></a>      </span>
<span id="cb8-32"><a href="scraping.html#cb8-32"></a>  }</span>
<span id="cb8-33"><a href="scraping.html#cb8-33"></a>  </span>
<span id="cb8-34"><a href="scraping.html#cb8-34"></a>}</span></code></pre></div>
<p>The function takes as a single argument a session which is established in one other function. Then It reads the session storing the information into an obj called the <code>opensess</code>. Another obj is created, namely price, right after the pipe operator a css query into the html is called. The css query <code>.im-mainFeatures__title</code> points to a precise data stored in immobiliare web page header, right below the main title. Expectation are that price is a one-element chr vector, containing the price and some other unnecessary characters. Then the algorithm enters into the first <code>if</code> statement. The handler checks is the object <code>price</code> is empty. If it doesn’t the algorithm jumps to the end and returns the cleaned quantity. But If it does it takes again the <code>opensess</code> and redirect to a second css query <code>.im-features__value , .im-features__title</code> where price, once again, could be found. Please note that This is all done within the same session, so no more request information has to be sent. Since the latter css query points to data stored inside a list, for the time being the newly created obj price2 is a list containing various information. Then the flow enters into the second <code>if</code> statement that checks whether <code>"prezzo"</code> is in the list or not, if it does it returns the +1 position index element with respect to the “prezzo” positioning. This happens because data in price2 list are stored by couples sequentially, say: [title, “Appartamento Sempione”, energy class, “G”, “prezzo”, 1200/al mese]. When it returns the element corresponding to +1 position index it applies also some data wrangling with <code>stringr</code> to keep out overabundant characters. The function then escapes in the else statement by setting <code>price2 = NA_Character_</code>. the Character quality has to be imposed due to fact that if the function is evaluated for a url and returns a quantity x and then is evaluated for url2 and outputs NA (no character) then results can not be combined into dataframe.</p>
<p>The skeleton shown can be shared among the other scraping functions, what it changes is the item “prezzo” which is then replaced by other covariates reasearched, say energy class etc.
Moreover some other functions need to have heavy cleaning steps in order to be usable. As a consequence functions need also to be broken down into pieces by single .R files, so that they can be adapted with respect to their singular necessities, see the nex paragraph.</p>
<p>Once all the functions have been created they need to be called together and then data coming after them need to be combined. This is done by <code>get,data.catsing()</code> which at first checks the validity of the url, then takes the same url as input and filters it as a session object. Then simultaneously all the functions are called and then combined. All this happens inside a <code>foreach</code> parallel loop called by <code>scrape.all.info()</code></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="scraping.html#cb9-1"></a>scrape.all.info =<span class="st"> </span><span class="cf">function</span>(<span class="dt">url =</span> <span class="st">&quot;https://www.immobiliare.it/affitto-case...&quot;</span>,</span>
<span id="cb9-2"><a href="scraping.html#cb9-2"></a>                           <span class="dt">vedi =</span> <span class="ot">FALSE</span>, </span>
<span id="cb9-3"><a href="scraping.html#cb9-3"></a>                           <span class="dt">scrivi =</span> <span class="ot">FALSE</span>, </span>
<span id="cb9-4"><a href="scraping.html#cb9-4"></a>                           <span class="dt">silent =</span> <span class="ot">FALSE</span>){</span>
<span id="cb9-5"><a href="scraping.html#cb9-5"></a>  <span class="cf">if</span> (silent) {</span>
<span id="cb9-6"><a href="scraping.html#cb9-6"></a>    start =<span class="st"> </span><span class="kw">as_hms</span>(<span class="kw">Sys.time</span>()); <span class="kw">cat</span>(<span class="st">&#39;Starting the process...</span><span class="ch">\n\n</span><span class="st">&#39;</span>)</span>
<span id="cb9-7"><a href="scraping.html#cb9-7"></a>    <span class="kw">message</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">The process has started in&#39;</span>,<span class="kw">format</span>(start,<span class="dt">usetz =</span> <span class="ot">TRUE</span>))  </span>
<span id="cb9-8"><a href="scraping.html#cb9-8"></a>  }</span>
<span id="cb9-9"><a href="scraping.html#cb9-9"></a>  <span class="co"># open parallel multisession</span></span>
<span id="cb9-10"><a href="scraping.html#cb9-10"></a>  cl =<span class="st"> </span><span class="kw">makeCluster</span>(<span class="kw">detectCores</span>()<span class="op">-</span><span class="dv">1</span>) <span class="co">#using max cores - 1 for parallel processing</span></span>
<span id="cb9-11"><a href="scraping.html#cb9-11"></a>  <span class="kw">registerDoParallel</span>(cl)</span>
<span id="cb9-12"><a href="scraping.html#cb9-12"></a>  start =<span class="st"> </span><span class="kw">as_hms</span>(<span class="kw">Sys.time</span>())</span>
<span id="cb9-13"><a href="scraping.html#cb9-13"></a>  </span>
<span id="cb9-14"><a href="scraping.html#cb9-14"></a>  <span class="cf">if</span> (silent) {</span>
<span id="cb9-15"><a href="scraping.html#cb9-15"></a>    <span class="kw">message</span>(<span class="st">&#39;</span><span class="ch">\n\n</span><span class="st">Start all the requests at time:&#39;</span>, <span class="kw">format</span>(start,<span class="dt">usetz =</span> T))  </span>
<span id="cb9-16"><a href="scraping.html#cb9-16"></a>  }</span>
<span id="cb9-17"><a href="scraping.html#cb9-17"></a>  ALL =<span class="st"> </span><span class="kw">foreach</span>(<span class="dt">i =</span> <span class="kw">seq_along</span>(links),</span>
<span id="cb9-18"><a href="scraping.html#cb9-18"></a>                <span class="dt">.packages =</span> lista.pacchetti,</span>
<span id="cb9-19"><a href="scraping.html#cb9-19"></a>                <span class="dt">.combine =</span> <span class="st">&quot;bind_rows&quot;</span>,</span>
<span id="cb9-20"><a href="scraping.html#cb9-20"></a>                <span class="dt">.multicombine =</span> <span class="ot">FALSE</span>,</span>
<span id="cb9-21"><a href="scraping.html#cb9-21"></a>                <span class="dt">.export =</span><span class="st">&quot;links&quot;</span> ,</span>
<span id="cb9-22"><a href="scraping.html#cb9-22"></a>                <span class="dt">.verbose =</span> <span class="ot">TRUE</span>,</span>
<span id="cb9-23"><a href="scraping.html#cb9-23"></a>                <span class="dt">.errorhandling=</span><span class="st">&#39;pass&#39;</span>) <span class="op">%dopar%</span><span class="st"> </span>{</span>
<span id="cb9-24"><a href="scraping.html#cb9-24"></a>                  <span class="kw">source</span>(<span class="st">&quot;utils.R&quot;</span>)</span>
<span id="cb9-25"><a href="scraping.html#cb9-25"></a>                  <span class="kw">sourceEntireFolder</span>(<span class="st">&quot;functions_singolourl&quot;</span>)</span>
<span id="cb9-26"><a href="scraping.html#cb9-26"></a>                  get.data.catsing =<span class="st"> </span><span class="cf">function</span>(singolourl){</span>
<span id="cb9-27"><a href="scraping.html#cb9-27"></a>                    </span>
<span id="cb9-28"><a href="scraping.html#cb9-28"></a>                    <span class="co"># dormi()</span></span>
<span id="cb9-29"><a href="scraping.html#cb9-29"></a>                    <span class="co"># </span></span>
<span id="cb9-30"><a href="scraping.html#cb9-30"></a>                    <span class="cf">if</span>(<span class="op">!</span><span class="kw">is_url</span>(singolourl)){</span>
<span id="cb9-31"><a href="scraping.html#cb9-31"></a>                      <span class="kw">stop</span>(<span class="kw">paste0</span>(<span class="st">&quot;The following url does not seem either to exist or it is invalid&quot;</span>, singolourl))</span>
<span id="cb9-32"><a href="scraping.html#cb9-32"></a>                    }</span>
<span id="cb9-33"><a href="scraping.html#cb9-33"></a>                    </span>
<span id="cb9-34"><a href="scraping.html#cb9-34"></a>                    session =<span class="st"> </span><span class="kw">html_session</span>(singolourl, <span class="kw">user_agent</span>(agents[<span class="kw">sample</span>(<span class="dv">1</span>)]))</span>
<span id="cb9-35"><a href="scraping.html#cb9-35"></a>                    <span class="cf">if</span> (<span class="kw">class</span>(session) <span class="op">==</span><span class="st"> &quot;session&quot;</span>) {</span>
<span id="cb9-36"><a href="scraping.html#cb9-36"></a>                      session =<span class="st"> </span>session<span class="op">$</span>response  </span>
<span id="cb9-37"><a href="scraping.html#cb9-37"></a>                    }</span>
<span id="cb9-38"><a href="scraping.html#cb9-38"></a>                      </span>
<span id="cb9-39"><a href="scraping.html#cb9-39"></a>                    id         =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapehouse.ID</span>(session)},</span>
<span id="cb9-40"><a href="scraping.html#cb9-40"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapehouse.ID&quot;</span>) })</span>
<span id="cb9-41"><a href="scraping.html#cb9-41"></a>                    lat        =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapelat.imm</span>(session)},</span>
<span id="cb9-42"><a href="scraping.html#cb9-42"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapelat.imm&quot;</span>) })</span>
<span id="cb9-43"><a href="scraping.html#cb9-43"></a>                    long       =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapelong.imm</span>(session)},</span>
<span id="cb9-44"><a href="scraping.html#cb9-44"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapelong.imm&quot;</span>) })</span>
<span id="cb9-45"><a href="scraping.html#cb9-45"></a>                    location   =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">take.address</span>(session)},</span>
<span id="cb9-46"><a href="scraping.html#cb9-46"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in take.address&quot;</span>) })</span>
<span id="cb9-47"><a href="scraping.html#cb9-47"></a>                    condom     =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapecondom.imm</span>(session)},</span>
<span id="cb9-48"><a href="scraping.html#cb9-48"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapecondom.imm&quot;</span>) })</span>
<span id="cb9-49"><a href="scraping.html#cb9-49"></a>                    buildage   =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapeagebuild.imm</span>(session)},</span>
<span id="cb9-50"><a href="scraping.html#cb9-50"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapeagebuild.imm&quot;</span>) })</span>
<span id="cb9-51"><a href="scraping.html#cb9-51"></a>                    </span>
<span id="cb9-52"><a href="scraping.html#cb9-52"></a>                  ...</span>
<span id="cb9-53"><a href="scraping.html#cb9-53"></a>                  </span>
<span id="cb9-54"><a href="scraping.html#cb9-54"></a>                   combine =<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">ID        =</span> id,</span>
<span id="cb9-55"><a href="scraping.html#cb9-55"></a>                                     <span class="dt">LAT       =</span> lat, </span>
<span id="cb9-56"><a href="scraping.html#cb9-56"></a>                                     <span class="dt">LONG      =</span> long,</span>
<span id="cb9-57"><a href="scraping.html#cb9-57"></a>                                     <span class="dt">LOCATION  =</span> location,</span>
<span id="cb9-58"><a href="scraping.html#cb9-58"></a>                                     <span class="dt">CONDOM    =</span> condom,</span>
<span id="cb9-59"><a href="scraping.html#cb9-59"></a>                                     <span class="dt">BUILDAGE  =</span> buildage,</span>
<span id="cb9-60"><a href="scraping.html#cb9-60"></a>                                    </span>
<span id="cb9-61"><a href="scraping.html#cb9-61"></a>                  ...</span>
<span id="cb9-62"><a href="scraping.html#cb9-62"></a>                  </span>
<span id="cb9-63"><a href="scraping.html#cb9-63"></a>                  </span>
<span id="cb9-64"><a href="scraping.html#cb9-64"></a>                  <span class="kw">return</span>(combine) </span>
<span id="cb9-65"><a href="scraping.html#cb9-65"></a>                  <span class="er">}</span></span>
<span id="cb9-66"><a href="scraping.html#cb9-66"></a>  <span class="kw">stopCluster</span>(cl)</span>
<span id="cb9-67"><a href="scraping.html#cb9-67"></a>  <span class="kw">return</span>(ALL)</span>
<span id="cb9-68"><a href="scraping.html#cb9-68"></a><span class="er">}</span></span></code></pre></div>
<p>Below it is printed the tree structure folder that composes the main elements of the scraping procedure. It can be spotted that the two folders, namely functions_singolourl and functions_url enclose all the single functions that allows to grab single information from session. Folders with a customized function are then sourced within the two main functions, scrape.all and scrape.all.info so data can be extracted.</p>
<pre><code> levelName
1  immobiliare.it-WebScraping     
2   ¦--functions_singolourl       
3   ¦   ¦--0scrapesqfeetINS.R     
4   ¦   ¦--0scrapenroomINS.R      
5   ¦   ¦--0scrapepriceINS.R      
6   ¦   ¦--0scrapetitleINS.R      
7   ¦   ¦--ScrapeAdDate.R         
8   ¦   ¦--ScrapeAge.R            
9   ¦   ¦--ScrapeAgeBuilding.R    
10  ¦   ¦--ScrapeAirConditioning.R
11  ¦   ¦--ScrapeAptChar.R        
12  ¦   ¦--ScrapeCatastInfo.R     
13  ¦   ¦--ScrapeCompart.R        
14  ¦   ¦--ScrapeCondom.R         
15  ¦   ¦--ScrapeContr.R          
16  ¦   ¦--ScrapeDisp.R           
17  ¦   ¦--ScrapeEnClass.R        
18  ¦   ¦--ScrapeFloor.R          
19  ¦   ¦--ScrapeHasMulti.R       
20  ¦   ¦--ScrapeHeating.R        
21  ¦   ¦--ScrapeHouseID.R        
22  ¦   ¦--ScrapeHouseTxtDes.R    
23  ¦   ¦--ScrapeLAT.R            
24  ¦   ¦--ScrapeLONG.R           
25  ¦   ¦--ScrapeLoweredPrice.R   
26  ¦   ¦--ScrapeMetrature.R      
27  ¦   ¦--ScrapePhotosNum.R      
28  ¦   ¦--ScrapePostAuto.R       
29  ¦   ¦--ScrapePropType.R       
30  ¦   ¦--ScrapeReaReview.R      
31  ¦   ¦--ScrapeStatus.R         
32  ¦   ¦--ScrapeTotPiani.R       
33  ¦   ¦--ScrapeType.R           
34  ¦   °--take_location.R        
35  ¦--scrapeALL.R                
36  ¦--scrapeALLINFO.R            
37  ¦--functions_url              
38  ¦   ¦--ScrapeHREF.R           
39  ¦   ¦--ScrapePrice.R          
40  ¦   ¦--ScrapePrimaryKey.R     
41  ¦   ¦--ScrapeRooms.R          
42  ¦   ¦--ScrapeSpace.R          
43  ¦   °--ScrapeTitle.R          
44  ¦--libs.R                     
45  ¦--utils.R                    
46  ¦--README.Rmd                 
47  ¦--README.md                  
48  °--simulations                
49      ¦--rt_match_vs_forloop.R  
50      °--runtime_simul.R</code></pre>
<div id="parallel-computing" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Parallel Computing</h3>
<p>Since many html sessions are opened and within each session many requests are sent computations can take a while. For the sake of the analysis and the app this should not bother the end user because scraping tasks are performed daily and a single day is sufficient amount of runtime. In any case functions are optimized following the criteria stated before. Run time is crucial when dealing with time series and time to market in real estate is very important, this leads to have always fresh data. A way to secure fresh new data is to have lightweight computation on a single machine o heavy computation divided among a bunch of different machines, in this case sessions. All the following runtime examinations are performed on the <code>scrape.all</code> functions. A first attempt was using <code>furrr</code> package <span class="citation">(Vaughan and Dancho <a href="#ref-furrr" role="doc-biblioref">2018</a>)</span> which enables mapping through a list with the <code>purrr</code> , along with a <code>future</code> parallel backend. This has shows decent results, but its run time increases when more requests are sent. This leads to a preventive conclusion about the computational complexity: it has to be at least linear. Empirical demonstrations have been made:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="scraping.html#cb11-1"></a><span class="kw">library</span>(furrr)</span>
<span id="cb11-2"><a href="scraping.html#cb11-2"></a></span>
<span id="cb11-3"><a href="scraping.html#cb11-3"></a>vecelaps =<span class="st"> </span><span class="kw">c</span>()</span>
<span id="cb11-4"><a href="scraping.html#cb11-4"></a>start =<span class="st"> </span><span class="kw">c</span>()</span>
<span id="cb11-5"><a href="scraping.html#cb11-5"></a>end =<span class="st"> </span><span class="kw">c</span>()</span>
<span id="cb11-6"><a href="scraping.html#cb11-6"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">len</span>(list.of.pages.imm[<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>])) {</span>
<span id="cb11-7"><a href="scraping.html#cb11-7"></a>    start[i] =<span class="st"> </span><span class="kw">Sys.time</span>()</span>
<span id="cb11-8"><a href="scraping.html#cb11-8"></a>    <span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n\n</span><span class="st"> run iteration&quot;</span>, i, <span class="st">&quot;over 20 total</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb11-9"><a href="scraping.html#cb11-9"></a>    list.of.pages.imm[<span class="dv">1</span><span class="op">:</span>i] <span class="op">%&gt;%</span><span class="st"> </span>furrr<span class="op">::</span><span class="kw">future_map</span>(get.data.caturl, <span class="dt">.progress =</span> T) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb11-10"><a href="scraping.html#cb11-10"></a><span class="st">        </span><span class="kw">bind_rows</span>()</span>
<span id="cb11-11"><a href="scraping.html#cb11-11"></a>    </span>
<span id="cb11-12"><a href="scraping.html#cb11-12"></a>    end[i] =<span class="st"> </span><span class="kw">Sys.time</span>()</span>
<span id="cb11-13"><a href="scraping.html#cb11-13"></a>    vecelaps[i] =<span class="st"> </span>end[i] <span class="op">-</span><span class="st"> </span>start[i]</span>
<span id="cb11-14"><a href="scraping.html#cb11-14"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="scraping.html#cb12-1"></a>furrrmethod =<span class="st"> </span><span class="kw">tibble</span>(start,</span>
<span id="cb12-2"><a href="scraping.html#cb12-2"></a>                      end,</span>
<span id="cb12-3"><a href="scraping.html#cb12-3"></a>                      vettoelaps)</span>
<span id="cb12-4"><a href="scraping.html#cb12-4"></a></span>
<span id="cb12-5"><a href="scraping.html#cb12-5"></a><span class="co"># ggplot (themed) run time meausurament method 1</span></span>
<span id="cb12-6"><a href="scraping.html#cb12-6"></a>p =<span class="st"> </span><span class="kw">ggplot</span>(furrrmethod,<span class="kw">aes</span>(<span class="dt">x=</span><span class="dv">1</span><span class="op">:</span><span class="dv">20</span>, <span class="dt">y=</span>vettoelaps)) <span class="op">+</span></span>
<span id="cb12-7"><a href="scraping.html#cb12-7"></a><span class="st">  </span><span class="kw">geom_line</span>( <span class="dt">color=</span><span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb12-8"><a href="scraping.html#cb12-8"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb12-9"><a href="scraping.html#cb12-9"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Num URLS evaluated&quot;</span>) <span class="op">+</span></span>
<span id="cb12-10"><a href="scraping.html#cb12-10"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;run time (in seconds)&quot;</span>) <span class="op">+</span></span>
<span id="cb12-11"><a href="scraping.html#cb12-11"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Run-Time for First method (furrr multisession)&quot;</span>) <span class="op">+</span></span>
<span id="cb12-12"><a href="scraping.html#cb12-12"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method=</span>lm) <span class="op">+</span></span>
<span id="cb12-13"><a href="scraping.html#cb12-13"></a><span class="st">  </span><span class="kw">theme_nicco</span>()</span>
<span id="cb12-14"><a href="scraping.html#cb12-14"></a>p</span></code></pre></div>
<div class="figure">
<img src="images/run_timefurrr.png" alt="" />
<p class="caption">computational complexity analysis with Furrr</p>
</div>
<p>On the x-axis the number of urls evaluated together, iteration after iteration the urls considered are increased by one. On the y-axis the time measured in seconds. Looking at the smoothing curve in between an easy guess might be linear time <span class="math inline">\(O(n)\)</span>.</p>
<p>A second attempt tried to explore the <code>foreach</code> package <span class="citation">(Microsoft and Weston <a href="#ref-foreach" role="doc-biblioref">2020</a>)</span>. This interesting package enables a new looping construct for executing R code in an iterative way. With all the variety of existing (<code>apply</code>, <code>lapply</code>, <code>sapply</code>, <code>eapply</code>, <code>mapply</code>, <code>rapply</code>,) looping constructs, it might be doubted that there is a need for yet another construct. The main reason for using the <code>foreach</code> package is that it supports <em>parallel execution</em>, that is, it can execute those repeated operations on multiple processors/cores on the computer, or on multiple nodes of a cluster. The construction is straightforward:</p>
<ul>
<li>start clusters on processors cores</li>
<li>define the iterator, in this case i = to the elements that are going to be iterated through</li>
<li><code>.packages</code>: Inherits the packages that are used in the tasks define below</li>
<li><code>.combine</code>: Define the combining function that bind results at the end (say cbind, rbind or tidyverse::bind_rows). It has to be a string.</li>
<li><code>.errorhandling</code>: specifies how a task evaluation error should be handle.</li>
<li><code>%dopar%</code>: the dopar keyword suggests foreach with parallelization method</li>
<li>then the function within the elements are iterated</li>
<li>close clusters</li>
</ul>
<p>One major important thing concerns the fact that th function within iterators repeats itself should be standalone. For standalone it is meant that the body function should be defined inside, as it would be a an empty environment. As a matter of fact packages has to be taken inside each time, and if the function is not defined inside body (or is not source from some other locations) the clusters can not operate and an error is printed.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="scraping.html#cb13-1"></a>cl =<span class="st"> </span><span class="kw">makeCluster</span>(<span class="kw">detectCores</span>()<span class="op">-</span><span class="dv">1</span>) </span>
<span id="cb13-2"><a href="scraping.html#cb13-2"></a><span class="kw">registerDoParallel</span>(cl)</span>
<span id="cb13-3"><a href="scraping.html#cb13-3"></a></span>
<span id="cb13-4"><a href="scraping.html#cb13-4"></a>vettoelaps1 =<span class="st"> </span><span class="kw">c</span>()</span>
<span id="cb13-5"><a href="scraping.html#cb13-5"></a>start1 =<span class="st"> </span><span class="kw">c</span>()</span>
<span id="cb13-6"><a href="scraping.html#cb13-6"></a>end1 =<span class="st"> </span><span class="kw">c</span>()</span>
<span id="cb13-7"><a href="scraping.html#cb13-7"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">len</span>(list.of.pages.imm[<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>])) {</span>
<span id="cb13-8"><a href="scraping.html#cb13-8"></a>  start1[j] =<span class="st"> </span><span class="kw">Sys.time</span>()</span>
<span id="cb13-9"><a href="scraping.html#cb13-9"></a>  <span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n\n</span><span class="st"> run iteration&quot;</span>,j,<span class="st">&quot;over 20 total</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb13-10"><a href="scraping.html#cb13-10"></a>  <span class="kw">foreach</span>(<span class="dt">i =</span> <span class="kw">seq_along</span>(list.of.pages.imm[<span class="dv">1</span><span class="op">:</span>j]),</span>
<span id="cb13-11"><a href="scraping.html#cb13-11"></a>          <span class="dt">.packages =</span> lista.pacchetti,</span>
<span id="cb13-12"><a href="scraping.html#cb13-12"></a>          <span class="dt">.combine =</span> <span class="st">&quot;bind_rows&quot;</span>,</span>
<span id="cb13-13"><a href="scraping.html#cb13-13"></a>          <span class="dt">.errorhandling=</span><span class="st">&#39;pass&#39;</span>) <span class="op">%dopar%</span><span class="st"> </span>{</span>
<span id="cb13-14"><a href="scraping.html#cb13-14"></a>            <span class="kw">source</span>(<span class="st">&quot;main.R&quot;</span>)</span>
<span id="cb13-15"><a href="scraping.html#cb13-15"></a>            x =<span class="st"> </span><span class="kw">get.data.caturl</span>(list.of.pages.imm[i])</span>
<span id="cb13-16"><a href="scraping.html#cb13-16"></a>            }</span>
<span id="cb13-17"><a href="scraping.html#cb13-17"></a>  end1[j] =<span class="st"> </span><span class="kw">Sys.time</span>()</span>
<span id="cb13-18"><a href="scraping.html#cb13-18"></a>  vettoelaps1[j] =<span class="st"> </span>end1[j] <span class="op">-</span>start1[j]</span>
<span id="cb13-19"><a href="scraping.html#cb13-19"></a>}</span>
<span id="cb13-20"><a href="scraping.html#cb13-20"></a><span class="kw">stopCluster</span>(cl)</span></code></pre></div>
<div class="figure">
<img src="images/run_timeforeach.png" alt="" />
<p class="caption">computational complexity analysis with Furrr</p>
</div>
<p>It can be seen quite easily that the curve is flattened and resembles someway logarithmic time <span class="math inline">\(O(log(n))\)</span>.</p>
<p>A further improvement could be obtained using a new package called <code>doAzureParallel</code> which is built on top of foreach. doAzureParallel enables different Virtual Machines operates parallel computing throughout Microsoft Azure cloud, but this comes at a substantial monetary cost. This would be a perfect match given that parallel methods seen before accelerates the number of requests sent among different processors or cluster, even though actually what it is really needed it is something that separates session. Unleashing Virtual Machines permits from one hand to further increase computational power and the number of potential requests, from the other it can splits requests among different user agents (a pool for each VM) masquerading even better the scraping automation.</p>
</div>
</div>
<div id="further-improvements" class="section level2">
<h2><span class="header-section-number">2.6</span> Further Improvements</h2>
<p>The main challenge remains unsolved since each single elements has been finely optimized but API continues to be unstable. What it can not be optimized are the system ad choices to change the layout of the page or to change the url structure (allocation of datain the web page). The way the API is designed really facilitates fast debugging but this can not be automatized. The API continues to necessitate to have a continuous back end review to verify thw working status. Moreover Error messages can not really be undestood sometimes, this is due to functions that are called within a parallel beckend that does not allow [refererenza su stack overflow di errore di print] to print error on console. So each time an error occur the “main” functions needs to be taken out of from the foreach parallel backend and evaluated in isolation where the loop ends. This is trivial but for the time being no solutions have been found.</p>
</div>
<div id="legal-challenges-ancora-non-validato" class="section level2">
<h2><span class="header-section-number">2.7</span> Legal Challenges (ancora non validato)</h2>
<p>“Data that are online and public are always available for all” is never a good answer to the question “Can I use that data to my scope”. <a href="https://www.immobiliare.it/">Immobiliare.it</a> is not providing any source of data from its own database neither it is planning to do so in the future. It has not even provided a paid API through which might be possible to perform analysis. However the golden standard for scraping was respected since the robot.txt file is clear allowing any actions as demonstrated above.
What it worth noting is that some other popular player other like Idealista is using a different approach. Some of procedures that has been applied to the immboliare scraping was not possibile on the Idealista. This could be due to many reason:</p>
<ul>
<li>Idealista content is composed by Javascript so and html parser can no get that.</li>
<li>Idealista blocks also certain web browser that have a demonstrated “career” in scraping procedures.</li>
</ul>
<p>All of this leads to accept that entry barriers to scrape are for sure higher than the one faced for Immobiliare. The reticence to share data could be a reflex on how big idealista is; as a matter of fact it has a heavy market presence in some of the Europe real estates country as Spain and France. So what they thought was to raise awareness on scraping procedure that in a certain way can hurt their business. This has been validated by the fact that prior filtering houses on their website a checkbox has to be signed. The checkbox make the user sign an agreement on their platform according to which data can not be misused and it belongs their intellectual property.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-densmore_2019">
<p>Densmore, James. 2019. “Ethics in Web Scraping.” <em>Medium</em>. Towards Data Science. <a href="https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01">https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01</a>.</p>
</div>
<div id="ref-robotstxt">
<p>Meissner, Peter, and Kun Ren. 2020. <em>Robotstxt: A ’Robots.txt’ Parser and ’Webbot’/’Spider’/’Crawler’ Permissions Checker</em>. <a href="https://CRAN.R-project.org/package=robotstxt">https://CRAN.R-project.org/package=robotstxt</a>.</p>
</div>
<div id="ref-foreach">
<p>Microsoft, and Steve Weston. 2020. <em>Foreach: Provides Foreach Looping Construct</em>. <a href="https://CRAN.R-project.org/package=foreach">https://CRAN.R-project.org/package=foreach</a>.</p>
</div>
<div id="ref-polite">
<p>Perepolkin, Dmytro. 2019. <em>Polite: Be Nice on the Web</em>. <a href="https://CRAN.R-project.org/package=polite">https://CRAN.R-project.org/package=polite</a>.</p>
</div>
<div id="ref-whoishostingthis.com">
<p>“User Agent: Learn Your Web Browsers User Agent Now.” 2020. <em>WhoIsHostingThis.com</em>. <a href="https://www.whoishostingthis.com/tools/user-agent/">https://www.whoishostingthis.com/tools/user-agent/</a>.</p>
</div>
<div id="ref-furrr">
<p>Vaughan, Davis, and Matt Dancho. 2018. <em>Furrr: Apply Mapping Functions in Parallel Using Futures</em>. <a href="https://CRAN.R-project.org/package=furrr">https://CRAN.R-project.org/package=furrr</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Infrastructure.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/Thesis/edit/master/02-scraping.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf", "Niccolo_Salvini_Thesis.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
