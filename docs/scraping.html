<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Scraping | Spatial Machine Learning modelling: End-to-End web app solution</title>
  <meta name="description" content="Chapter 2 Scraping | Spatial Machine Learning modelling: End-to-End web app solution by Niccolò Salvini" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Scraping | Spatial Machine Learning modelling: End-to-End web app solution" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/transspatial.PNG" />
  <meta property="og:description" content="Chapter 2 Scraping | Spatial Machine Learning modelling: End-to-End web app solution by Niccolò Salvini" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Scraping | Spatial Machine Learning modelling: End-to-End web app solution" />
  
  <meta name="twitter:description" content="Chapter 2 Scraping | Spatial Machine Learning modelling: End-to-End web app solution by Niccolò Salvini" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/transspatial.PNG" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="intro.html"/>
<link rel="next" href="Infrastructure.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Spatial Machine Learning Modelling: End-to-End web App solution</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#preface"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#what-is-scraping"><i class="fa fa-check"></i><b>2.1</b> What is Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#user-agents-proxies-handlers"><i class="fa fa-check"></i><b>2.2</b> User agents, Proxies, Handlers</a><ul>
<li class="chapter" data-level="2.2.1" data-path="scraping.html"><a href="scraping.html#user-agents-spoofing"><i class="fa fa-check"></i><b>2.2.1</b> User agents Spoofing</a></li>
<li class="chapter" data-level="2.2.2" data-path="scraping.html"><a href="scraping.html#handlers"><i class="fa fa-check"></i><b>2.2.2</b> Handlers</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#how-they-are-designed-with-rvest"><i class="fa fa-check"></i><b>2.3</b> How they are designed with <code>rvest</code></a><ul>
<li class="chapter" data-level="2.3.1" data-path="scraping.html"><a href="scraping.html#from-generic-and-specific-structure"><i class="fa fa-check"></i><b>2.3.1</b> From Generic and Specific structure</a></li>
<li class="chapter" data-level="2.3.2" data-path="scraping.html"><a href="scraping.html#parallel-computing"><i class="fa fa-check"></i><b>2.3.2</b> parallel computing</a></li>
<li class="chapter" data-level="2.3.3" data-path="scraping.html"><a href="scraping.html#foreach-package-and-runtime-benchmark-with-future"><i class="fa fa-check"></i><b>2.3.3</b> <code>foreach</code> package and runtime benchmark with future</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#what-are-the-advantages-of-workflow"><i class="fa fa-check"></i><b>2.4</b> What are the advantages of workflow</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#legal-challenges"><i class="fa fa-check"></i><b>2.5</b> Legal challenges</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> Infrastructure</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker-container"><i class="fa fa-check"></i><b>3.2</b> Docker Container</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-docker"><i class="fa fa-check"></i><b>3.2.1</b> What is Docker?</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#what-are-the-main-andvantages-of-using-docker"><i class="fa fa-check"></i><b>3.2.2</b> What are the main andvantages of using Docker</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.3</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api"><i class="fa fa-check"></i><b>3.3</b> API</a></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-an-api"><i class="fa fa-check"></i><b>3.4</b> What is an API</a><ul>
<li class="chapter" data-level="3.4.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-in-practice-an-api-does"><i class="fa fa-check"></i><b>3.4.1</b> What in practice an API does</a></li>
<li class="chapter" data-level="3.4.2" data-path="Infrastructure.html"><a href="Infrastructure.html#plumber-api"><i class="fa fa-check"></i><b>3.4.2</b> Plumber API</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="methodologies.html"><a href="methodologies.html"><i class="fa fa-check"></i><b>4</b> Methodologies</a></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a><ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>5.1</b> Example one</a></li>
<li class="chapter" data-level="5.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>5.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>6</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Spatial Machine Learning modelling: End-to-End web app solution</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="scraping" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Scraping</h1>
<div id="what-is-scraping" class="section level2">
<h2><span class="header-section-number">2.1</span> What is Scraping</h2>
<p>Lo web scraping è una tecnica di estrazione dei dati da pagine internet statiche o dinamiche in maniera automatica e simulatanea <span class="citation">(Wikipedia <a href="#ref-wiki:xxx" role="doc-biblioref">2020</a>)</span>. L’impossibilità di reperire dati aperti aggiornati riguardo l’affitto sul mercato italiano mi ha spinto a sviluppare sofisticate tecniche di estrazione di dati orientate ad alleggerire lo sforzo e aumentare la velocità di reperimento: da una parte nel preprocessing del dataset, nella successiva del fragente del modelling, per finire con la reattività di risposta dell’applicazione.
Le informazioni sui siti appaiono spesso ordinate e semplici, tuttavia ogni sito web ha una propria architettura e un proprio linguaggio. Per architettura intendo struttura gerarchica secondo cui è organizatto un sito internet: una semplificazione della struttura di un sito web può essere un insieme di cartelle innestate una dentro l’altra collegate tra loro da riferimenti tramite l’url. la natura gerarchica della struttura prevede che si usi un linguaggio che fa propria questa caratteristica, HTML è il preferito. L’html si organizza in nodi ed angoli, esattamente come un grafo; che aggiunta la componente gerarchica fa sì che questo sia un albero. Difatti spesso ci si riferisce alla struttura delle pagine web come html tree. Ogni elemento nella pagina ha un suo preciso posto nel codice sorgente della stessa e ha un preciso valore o più valori. Possiamo immaginare ogni nodo della pagina come una lista di valori che è collegata ad un nodo precedente detto padre da una struttura gerarchica superiore, ed eventiualmente ad un nodo successivo detto figlio. Pertanto tutte le informazioni che giacciono sotto al nodo padre sono parenti del nodo padre e sono direttamente collegate (directed nel senso dell’interpretazione), parallelamente ci saranno altri nodi padre che saranno adiacenti al nodo padre, i quali avranno nodi figli e così via.
La complessità della pagina e del codice è tanto maggiore quanto il livello dell’albero aumenta, tanto più l’albero è folto tanto più sarà difficile individurare il ramo o la foglia che ci interessa. Ragionevolemnte accade lo stesso per la funzione di scraping e il tempo di scraping.
Html organizza i contenuti e le relazioni tra loro, il css (Cascading Style Sheets) invece si occupa dello stile e della formattazione degli stessi. il css è uno strumento molto potente in mano ad uno scraper perchè permette di recuperare informazioni simili tra loro ma che occupano nodi con posizione gerarchica diversa all’interno della pagina. Pertanto una volta letto l’html della pagina sarà necessario recuperare la query css per raccogliere tutti gli elementi di interesse tramite la funzione di scraping.
Successivamente occorre notare che l’encoding da html a stringa di testo non è quasi mai lineare, spesso occorre riformattare, cancellare spazi, convertire la natura dell’oggetto estratto etc.
Il successivo elemento di complessità incontrato durante questa prima fase è stato interfacciarsi con un server attento alle richieste GET degli utenti. I dati viaggiano in pacchetti da un server che ospita un sito internet al nostro laptop. tutte le volte che cerchiamo di accedere ad un sito stiamo mandando una richiesta di ricezione di pacchetti dati ad un server in qualche luogo remoto del mondo. Quandi bussiamo alla porta del server se non siamo sospetti e superiamo i criteri autostabiliti dal server questo risponde, e lo fa con un numero che spazia da 200 a 500, due esempi: 200 se la risposta è positiva, 404 se la risposta è negativa. I criteri secondo cui gli utenti sono calssificati secondo utente normale o utente sospetto (aka bot) sono sintetizzati in un documento di testo chiamato robot.txt. Questo file di testo raccoglie tra le altre due infromazioni principali il delay time, cioè il tempo preferito dal server che deve intercorrere tra una richiesta dati e la successiva e quale utente è autorizzato ad accedere. Ogni utente posside un indirizzo IP che nelle richieste a server si codifica in user agent, cioè una stringa di testo dove vengono raccolte le infromzioni significative circa il dispositivo da cui provengono le richieste, un esempio:</p>
<p>‘Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36’,</p>
<p>dove ogni segmento della stringa rispecchia una caratteristica del laptop del richiedente, Chrome/54.0.2840.71 è la versione del browser chrome da cui proviene la richiesta Safari/537.36’, è il motore di ricerca etc.</p>
</div>
<div id="user-agents-proxies-handlers" class="section level2">
<h2><span class="header-section-number">2.2</span> User agents, Proxies, Handlers</h2>
<p>Everytime a user enters a website what he is really doing is sending an HTTP request to the website server with some information packed. This can be easily thought as a generic person A that rings the door’s bell of person B’s house. A comes to the B door with its personal information, its name, surname, where he lives etc. At this point B may either answer to A requests by opening the door and let him enter given the set of information he has, or it may not since B is not sure of the real intentions of A. This typical everyday situation in nothing more what happens billions of times on the internet everyday, the user (in the example above A) is interacting with a server website (part B) sending packets of information. If a server does not trust the information provided by the user, if the requests are too many, if the requests seems to be scheduled due to fixed sleeping time, a server can block the requests. In certain cases it can even forbid the user to be on the website. The language the two parties talks are coded in numbers that ranges from 100 to 511, each of which ha its own significance. A popular case of this type of interaction occurs when users are not connected to internet so the server responds 404, page not found. Servers are built with a immune-system like software that raises barriers and block users to prevent dossing or other illegal practises.</p>
<div class="figure">
<img src="images/how_web_works.png" alt="" />
<p class="caption">How Web Works</p>
</div>
<p>This procedure is a daily issue to people that are trying to collect information from websites. Google does it everyday with its spider crawlers, which are very sophisticated bots that performs scarping over a enormous range of websites. This challenge can be addressed in multiple ways, there are some specific Python packages that overcome this issue. The are also certain types of scraping as the Selenium web driver automation that simulates browser automation. Selenium allows the user not to be easily detected by the server immune system and peaceful. In here precautions have not been taken lightly, and a simple but effective approach is proposed.</p>
<div id="user-agents-spoofing" class="section level3">
<h3><span class="header-section-number">2.2.1</span> User agents Spoofing</h3>
<p>A user agent <span class="citation">(“User Agent: Learn Your Web Browsers User Agent Now” <a href="#ref-whoishostingthis.com" role="doc-biblioref">2020</a>)</span> is a string of characters in each browser that serves as an identification agent. The user agent permits the web server to be able to identify the user operating system and the browser. Then, the web server uses the exchanged information to determine what content should be presented to particular operating systems and web browsers on a series of devices. The user agent string contains the user application or software, the operating system (and their versions), the web client, the web client’s version, and the web engine responsible for the content display (such as AppleWebKit). The user agent string is sent in form of a HTTP request header. Since the User Agents acts as middle man between the client request and the server response what it would be better doing is to actively faking it so that each time a web browser presents himself to a web server it has a different specifications, different web client, different operating system and so on.</p>
<p>The simple approach followed was building a vector of samples of different existing and updated User Agents (UA). Then whenever a request from a browser is served to a web server 1 random string is drawn from the user agents pool. So each time the user is sending the requests it appears to have a different “identity”. Below the user agents rotating pool:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="scraping.html#cb1-1"></a><span class="kw">set.seed</span>(<span class="dv">27</span>)</span>
<span id="cb1-2"><a href="scraping.html#cb1-2"></a>agents =<span class="st">  </span><span class="kw">c</span>(<span class="st">&#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&#39;</span>,</span>
<span id="cb1-3"><a href="scraping.html#cb1-3"></a>            <span class="st">&#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&#39;</span>,</span>
<span id="cb1-4"><a href="scraping.html#cb1-4"></a>            <span class="st">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&#39;</span>,</span>
<span id="cb1-5"><a href="scraping.html#cb1-5"></a>            <span class="st">&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14&#39;</span>,</span>
<span id="cb1-6"><a href="scraping.html#cb1-6"></a>            <span class="st">&#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&#39;</span>,</span>
<span id="cb1-7"><a href="scraping.html#cb1-7"></a>            <span class="st">&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36&#39;</span>,</span>
<span id="cb1-8"><a href="scraping.html#cb1-8"></a>            <span class="st">&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36&#39;</span>,</span>
<span id="cb1-9"><a href="scraping.html#cb1-9"></a>            <span class="st">&#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&#39;</span>,</span>
<span id="cb1-10"><a href="scraping.html#cb1-10"></a>            <span class="st">&#39;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&#39;</span>,</span>
<span id="cb1-11"><a href="scraping.html#cb1-11"></a>            <span class="st">&#39;Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;</span>)</span>
<span id="cb1-12"><a href="scraping.html#cb1-12"></a></span>
<span id="cb1-13"><a href="scraping.html#cb1-13"></a>agents[<span class="kw">sample</span>(<span class="dv">1</span>)]</span></code></pre></div>
<pre><code>## [1] &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&quot;</code></pre>
<p>An improvement to this could be using also rotating proxies. A proxy server acts as a gateway between the user and the server. It’s an intermediary server himself, separating end clients from the websites they are browsing. Proxy servers provide varying layers of functionality, security, and privacy are some of the examples.
While the user is exploiting a proxy server, internet traffic flows through the proxy server on its way to the server you requested. The request then comes back through that same proxy server and then the proxy server forwards the data received from the website to you.
Many proxy servers are offered in a paid version so in this case since security barriers of the target website are not high they will not be implemented. It has to be mentioned that many online services are providing free proxies but the turnaround of this solutions are many, two of them are:
- Proxies to be free are widely shared among people, so as long as someone has used them for illegal purposes the user is inheriting their mistakes when caught.
- Some of those proxies, pretty all the ones coming from low ranked websites, are tracked so there might be a user privacy violation issue.</p>
</div>
<div id="handlers" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Handlers</h3>
<p>During the scraping many things could be going wrong. Starting from the things that have been previously explained at the chapter start (URL structure changes, data are moved in different location so that css query goes empty…), ending with the ones that have just been said a few lines ago. Handlers and trycatch error workarounds are explicitly built in this sense. The continuous testing of the scraping functioning while developing has required the maintainer to track where the error occurs. A few numbers: the “agglomerative” function <code>get.data.catsing()</code> triggers more than 36 scrapping functions that are going to catch 36 different data pieces. If one of them went missing then the other one would be missing too. Then when row-data is binded together one entry column might not exists making the process fail.<br />
Then the solution to that is to call inside the aggolmerative function as much as trycatch as many scrapping functions are involved. The trycatch can leverage the gap by introducing a specified quantity and alerting that something went wrong. On top of that many other handlers are called throughout the procedure:</p>
<ul>
<li><code>get_ua()</code> verifies that the user agent coming from the session request is not the default one</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="scraping.html#cb3-1"></a>get_ua =<span class="st"> </span><span class="cf">function</span>(sess) {</span>
<span id="cb3-2"><a href="scraping.html#cb3-2"></a>  <span class="kw">stopifnot</span>(<span class="kw">is.session</span>(sess))</span>
<span id="cb3-3"><a href="scraping.html#cb3-3"></a>  <span class="kw">stopifnot</span>(<span class="kw">is_url</span>(sess<span class="op">$</span>url))</span>
<span id="cb3-4"><a href="scraping.html#cb3-4"></a>  ua =<span class="st"> </span>sess<span class="op">$</span>response<span class="op">$</span>request<span class="op">$</span>options<span class="op">$</span>useragent</span>
<span id="cb3-5"><a href="scraping.html#cb3-5"></a>  <span class="kw">return</span>(ua)</span>
<span id="cb3-6"><a href="scraping.html#cb3-6"></a>}</span></code></pre></div>
<ul>
<li><code>is_url()</code> verifies that the url input needed has the canonic form. This is done by a REGEX query.</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="scraping.html#cb4-1"></a>is_url =<span class="st"> </span><span class="cf">function</span>(url){</span>
<span id="cb4-2"><a href="scraping.html#cb4-2"></a>  re =<span class="st"> &quot;^(?:(?:http(?:s)?|ftp)://)(?:</span><span class="ch">\\</span><span class="st">S+(?::(?:</span><span class="ch">\\</span><span class="st">S)*)?@)?(?:(?:[a-z0-9\u00a1-\uffff](?:-)*)*(?:[a-z0-9\u00a1-\uffff])+)(?:</span><span class="ch">\\</span><span class="st">.(?:[a-z0-9\u00a1-\uffff](?:-)*)*(?:[a-z0-9\u00a1-\uffff])+)*(?:</span><span class="ch">\\</span><span class="st">.(?:[a-z0-9\u00a1-\uffff]){2,})(?::(?:</span><span class="ch">\\</span><span class="st">d){2,5})?(?:/(?:</span><span class="ch">\\</span><span class="st">S)*)?$&quot;</span></span>
<span id="cb4-3"><a href="scraping.html#cb4-3"></a>  <span class="kw">grepl</span>(re, url)</span>
<span id="cb4-4"><a href="scraping.html#cb4-4"></a>}</span></code></pre></div>
<ul>
<li><code>.get_delay()</code> checks through the robotxt file if a delay between each request is welcomed.</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="scraping.html#cb5-1"></a>.get_delay =<span class="st"> </span><span class="cf">function</span>(domain) {</span>
<span id="cb5-2"><a href="scraping.html#cb5-2"></a>  </span>
<span id="cb5-3"><a href="scraping.html#cb5-3"></a>  <span class="kw">message</span>(<span class="kw">sprintf</span>(<span class="st">&quot;Refreshing robots.txt data for %s...&quot;</span>, domain))</span>
<span id="cb5-4"><a href="scraping.html#cb5-4"></a>  </span>
<span id="cb5-5"><a href="scraping.html#cb5-5"></a>  cd_tmp =<span class="st"> </span>robotstxt<span class="op">::</span><span class="kw">robotstxt</span>(domain)<span class="op">$</span>crawl_delay</span>
<span id="cb5-6"><a href="scraping.html#cb5-6"></a>  </span>
<span id="cb5-7"><a href="scraping.html#cb5-7"></a>  <span class="cf">if</span> (<span class="kw">length</span>(cd_tmp) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb5-8"><a href="scraping.html#cb5-8"></a>    star =<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(cd_tmp, useragent<span class="op">==</span><span class="st">&quot;*&quot;</span>)</span>
<span id="cb5-9"><a href="scraping.html#cb5-9"></a>    <span class="cf">if</span> (<span class="kw">nrow</span>(star) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) star =<span class="st"> </span>cd_tmp[<span class="dv">1</span>,]</span>
<span id="cb5-10"><a href="scraping.html#cb5-10"></a>    <span class="kw">as.numeric</span>(star<span class="op">$</span>value[<span class="dv">1</span>])</span>
<span id="cb5-11"><a href="scraping.html#cb5-11"></a>  } <span class="cf">else</span> {</span>
<span id="cb5-12"><a href="scraping.html#cb5-12"></a>    10L</span>
<span id="cb5-13"><a href="scraping.html#cb5-13"></a>  }</span>
<span id="cb5-14"><a href="scraping.html#cb5-14"></a>  </span>
<span id="cb5-15"><a href="scraping.html#cb5-15"></a>}</span>
<span id="cb5-16"><a href="scraping.html#cb5-16"></a></span>
<span id="cb5-17"><a href="scraping.html#cb5-17"></a>get_delay =<span class="st">  </span>memoise<span class="op">::</span><span class="kw">memoise</span>(.get_delay)</span></code></pre></div>
<ul>
<li><code>checkpermission()</code> checks through the robotxt file if the dom allows the user to scape data.</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="scraping.html#cb6-1"></a>dominio =<span class="st"> &quot;immobiliare.it&quot;</span></span>
<span id="cb6-2"><a href="scraping.html#cb6-2"></a></span>
<span id="cb6-3"><a href="scraping.html#cb6-3"></a>checkpermission =<span class="st"> </span><span class="cf">function</span>(dom) {</span>
<span id="cb6-4"><a href="scraping.html#cb6-4"></a>  </span>
<span id="cb6-5"><a href="scraping.html#cb6-5"></a>  robot =<span class="st"> </span><span class="kw">robotstxt</span>(<span class="dt">domain =</span> dom)</span>
<span id="cb6-6"><a href="scraping.html#cb6-6"></a>  vd =<span class="st"> </span>robot<span class="op">$</span><span class="kw">check</span>()[<span class="dv">1</span>]</span>
<span id="cb6-7"><a href="scraping.html#cb6-7"></a>  <span class="cf">if</span> (vd) {</span>
<span id="cb6-8"><a href="scraping.html#cb6-8"></a>    <span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">robot.txt dice: va bene, puoi!&quot;</span>)</span>
<span id="cb6-9"><a href="scraping.html#cb6-9"></a>  } <span class="cf">else</span> {<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">robot.txt dice: non puoi, smettila&quot;</span>)}</span>
<span id="cb6-10"><a href="scraping.html#cb6-10"></a>}</span></code></pre></div>
</div>
</div>
<div id="how-they-are-designed-with-rvest" class="section level2">
<h2><span class="header-section-number">2.3</span> How they are designed with <code>rvest</code></h2>
<div id="from-generic-and-specific-structure" class="section level3">
<h3><span class="header-section-number">2.3.1</span> From Generic and Specific structure</h3>
<div class="figure">
<img src="images/rvest_infr.png" alt="" />
<p class="caption">functional structure</p>
</div>
</div>
<div id="parallel-computing" class="section level3">
<h3><span class="header-section-number">2.3.2</span> parallel computing</h3>
</div>
<div id="foreach-package-and-runtime-benchmark-with-future" class="section level3">
<h3><span class="header-section-number">2.3.3</span> <code>foreach</code> package and runtime benchmark with future</h3>
</div>
</div>
<div id="what-are-the-advantages-of-workflow" class="section level2">
<h2><span class="header-section-number">2.4</span> What are the advantages of workflow</h2>
</div>
<div id="legal-challenges" class="section level2">
<h2><span class="header-section-number">2.5</span> Legal challenges</h2>
<p>“Data that are online and public are always available” is never a good answer to the question “Can I use that data to my scope”. <a href="https://www.immobiliare.it/">Immobiliare.it</a> is not providing any source of data from its own database neither it is planning to do so in the future.</p>
<ul>
<li>Copia e incolla manuale</li>
<li>Web scraper</li>
<li>HTML parsing</li>
<li>Analisi con Visione computerizzata</li>
<li>DOM parsing</li>
<li>Riconoscimento dell’annotazione semantica</li>
<li>Aggregazione verticale</li>
<li>Text pattern matching</li>
</ul>
<p>funzioni di scraping.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-whoishostingthis.com">
<p>“User Agent: Learn Your Web Browsers User Agent Now.” 2020. <em>WhoIsHostingThis.com</em>. <a href="https://www.whoishostingthis.com/tools/user-agent/">https://www.whoishostingthis.com/tools/user-agent/</a>.</p>
</div>
<div id="ref-wiki:xxx">
<p>Wikipedia. 2020. “Web Scraping — Wikipedia, L’enciclopedia Libera.” <a href="http://it.wikipedia.org/w/index.php?title=Web_scraping&amp;oldid=112540647">http://it.wikipedia.org/w/index.php?title=Web_scraping&amp;oldid=112540647</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Infrastructure.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/Thesis/edit/master/02-scraping.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf", "Niccolo_Salvini_Thesis.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
