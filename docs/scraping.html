<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Scraping | REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA.</title>
  <meta name="description" content="Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Scraping | REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA." />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />
  <meta property="og:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Scraping | REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA." />
  
  <meta name="twitter:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="intro.html"/>
<link rel="next" href="Infrastructure.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"> REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#what-is-web-scraping"><i class="fa fa-check"></i><b>2.1</b> What is Web Scraping</a><ul>
<li class="chapter" data-level="2.1.1" data-path="scraping.html"><a href="scraping.html#webstructure"><i class="fa fa-check"></i><b>2.1.1</b> Immobiliare.it Webscraping website structure</a></li>
<li class="chapter" data-level="2.1.2" data-path="scraping.html"><a href="scraping.html#immobiliare.it-webscraping-content-architecture-with-rvest"><i class="fa fa-check"></i><b>2.1.2</b> Immobiliare.it Webscraping content architecture with <code>rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#best-practices"><i class="fa fa-check"></i><b>2.2</b> Scraping Best Practices and Security provisions</a></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#security-provisions-user-agents-proxies-and-handlers"><i class="fa fa-check"></i><b>2.3</b> Security provisions: User agents, Proxies and Handlers</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scraping.html"><a href="scraping.html#spoofing"><i class="fa fa-check"></i><b>2.3.1</b> User Agents Spoofing</a></li>
<li class="chapter" data-level="2.3.2" data-path="scraping.html"><a href="scraping.html#handlers-and-trycatch"><i class="fa fa-check"></i><b>2.3.2</b> Handlers and Trycatch</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#parallel-computing"><i class="fa fa-check"></i><b>2.4</b> Parallel Computing</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#challenges"><i class="fa fa-check"></i><b>2.5</b> Open Challenges</a></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#legal-profiles-ancora-non-validato"><i class="fa fa-check"></i><b>2.6</b> Legal Profiles (ancora non validato)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> Infrastructure</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>3.2</b> Docker</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-docker"><i class="fa fa-check"></i><b>3.2.1</b> What is Docker</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#why-docker"><i class="fa fa-check"></i><b>3.2.2</b> Why Docker</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.3</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#rest-api"><i class="fa fa-check"></i><b>3.3</b> REST API</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>3.3.1</b> Plumber API</a></li>
<li class="chapter" data-level="3.3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#immobiliare.it-rest-api"><i class="fa fa-check"></i><b>3.3.2</b> Immobiliare.it REST API</a></li>
<li class="chapter" data-level="3.3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#rest-api-source-code"><i class="fa fa-check"></i><b>3.3.3</b> REST API source code</a></li>
<li class="chapter" data-level="3.3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#APIdocs"><i class="fa fa-check"></i><b>3.3.4</b> REST API documentation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>3.4</b> NGINX reverse proxy server</a></li>
<li class="chapter" data-level="3.5" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>3.5</b> AWS EC2 server</a><ul>
<li class="chapter" data-level="3.5.1" data-path="Infrastructure.html"><a href="Infrastructure.html#launch-an-ec2-instance"><i class="fa fa-check"></i><b>3.5.1</b> Launch an EC2 instance</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="Infrastructure.html"><a href="Infrastructure.html#further-integrations"><i class="fa fa-check"></i><b>3.6</b> Further integrations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>4</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="exploratory.html"><a href="exploratory.html#data-preparation"><i class="fa fa-check"></i><b>4.1</b> Data preparation</a><ul>
<li class="chapter" data-level="4.1.1" data-path="exploratory.html"><a href="exploratory.html#na-removal-and-imputation"><i class="fa fa-check"></i><b>4.1.1</b> NA removal and imputation</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="exploratory.html"><a href="exploratory.html#spatial-autocorrelation-assessement"><i class="fa fa-check"></i><b>4.2</b> Spatial Autocorrelation assessement</a></li>
<li class="chapter" data-level="4.3" data-path="exploratory.html"><a href="exploratory.html#model-specification"><i class="fa fa-check"></i><b>4.3</b> Model Specification</a></li>
<li class="chapter" data-level="4.4" data-path="exploratory.html"><a href="exploratory.html#mesh-building"><i class="fa fa-check"></i><b>4.4</b> Mesh building</a><ul>
<li class="chapter" data-level="4.4.1" data-path="exploratory.html"><a href="exploratory.html#building-spde-model-on-mesh"><i class="fa fa-check"></i><b>4.4.1</b> BUilding SPDE model on mesh</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exploratory.html"><a href="exploratory.html#spatial-kriging-prediction"><i class="fa fa-check"></i><b>4.5</b> Spatial Kriging (Prediction)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inla-spde.html"><a href="inla-spde.html"><i class="fa fa-check"></i><b>5</b> INLA computation</a><ul>
<li class="chapter" data-level="5.1" data-path="inla-spde.html"><a href="inla-spde.html#LGM"><i class="fa fa-check"></i><b>5.1</b> Latent Gaussian Models LGM</a></li>
<li class="chapter" data-level="5.2" data-path="inla-spde.html"><a href="inla-spde.html#approx"><i class="fa fa-check"></i><b>5.2</b> Approximation in INLA setting</a><ul>
<li class="chapter" data-level="5.2.1" data-path="inla-spde.html"><a href="inla-spde.html#further-approximations-prolly-do-not-note-include"><i class="fa fa-check"></i><b>5.2.1</b> further approximations (prolly do not note include)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inla-spde.html"><a href="inla-spde.html#rinla"><i class="fa fa-check"></i><b>5.3</b> R-INLA package in a bayesian hierarchical regression perspective</a><ul>
<li class="chapter" data-level="5.3.1" data-path="inla-spde.html"><a href="inla-spde.html#overview"><i class="fa fa-check"></i><b>5.3.1</b> Overview</a></li>
<li class="chapter" data-level="5.3.2" data-path="inla-spde.html"><a href="inla-spde.html#example"><i class="fa fa-check"></i><b>5.3.2</b> Linear Predictor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>6</b> Point Referenced Data Modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>6.1</b> Gaussian Process (GP)</a></li>
<li class="chapter" data-level="6.2" data-path="prdm.html"><a href="prdm.html#spatial-covariance-function"><i class="fa fa-check"></i><b>6.2</b> Spatial Covariance Function</a><ul>
<li class="chapter" data-level="6.2.1" data-path="prdm.html"><a href="prdm.html#Matern"><i class="fa fa-check"></i><b>6.2.1</b> Matérn Covariance Function</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="prdm.html"><a href="prdm.html#hedonic-models-literature-review-and-spatial-hedonic-price-models"><i class="fa fa-check"></i><b>6.3</b> Hedonic models Literature Review and Spatial Hedonic Price Models</a></li>
<li class="chapter" data-level="6.4" data-path="prdm.html"><a href="prdm.html#univariateregr"><i class="fa fa-check"></i><b>6.4</b> Point Referenced Regression for univariate spatial data</a></li>
<li class="chapter" data-level="6.5" data-path="prdm.html"><a href="prdm.html#hiermod"><i class="fa fa-check"></i><b>6.5</b> Hierarchical Bayesian models</a></li>
<li class="chapter" data-level="6.6" data-path="prdm.html"><a href="prdm.html#finalregr"><i class="fa fa-check"></i><b>6.6</b> INLA model through spatial hierarchical regression</a></li>
<li class="chapter" data-level="6.7" data-path="prdm.html"><a href="prdm.html#spatial-kriging"><i class="fa fa-check"></i><b>6.7</b> Spatial Kriging</a></li>
<li class="chapter" data-level="6.8" data-path="prdm.html"><a href="prdm.html#model-checking-and-comparison"><i class="fa fa-check"></i><b>6.8</b> Model Checking and Comparison</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="spde.html"><a href="spde.html"><i class="fa fa-check"></i><b>7</b> SPDE approach</a><ul>
<li class="chapter" data-level="7.1" data-path="spde.html"><a href="spde.html#set-spde-problem"><i class="fa fa-check"></i><b>7.1</b> Set SPDE Problem</a></li>
<li class="chapter" data-level="7.2" data-path="spde.html"><a href="spde.html#spde-within-r-inla"><i class="fa fa-check"></i><b>7.2</b> SPDE within R-INLA</a></li>
<li class="chapter" data-level="7.3" data-path="spde.html"><a href="spde.html#first-point-krainsky-rubio-too-technical"><i class="fa fa-check"></i><b>7.3</b> First Point Krainsky Rubio TOO TECHNICAL</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>8</b> Shiny Web App</a><ul>
<li class="chapter" data-level="8.1" data-path="application.html"><a href="application.html#example-one"><i class="fa fa-check"></i><b>8.1</b> Example one</a></li>
<li class="chapter" data-level="8.2" data-path="application.html"><a href="application.html#example-two"><i class="fa fa-check"></i><b>8.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>9</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">REST API for Real Estate rental data, a spatial bayesian modeling approach with INLA.</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="scraping" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Scraping</h1>
<p>The following chapter covers a gentle introduction and concepts of web scraping centered on immobiliare.it. It starts in general terms by ideally segmenting websites in two meta-concepts: website structure and content architecture, then it provokes this segmentation to the specific immobiliare.it case. The abstract workflow will ease the identification of the first “high level” challenge to be fronting during scraping, which is mainly understanding how the website is structured and how to reverse engineer the url composition. The structure is unrolled so that each part can be singularly and directly accessed and the problem can be passed to “lower levels”. At this point a rooted-tree graph representation is used to map scraping functions into immobiliare content architecture. By means of <code>rvest</code> scraping is possible and the main function involved are presented. A specific function to scrape price (the response var of the analysis) is shown and then also a second function, that takes care of grouping all the scraping functions into a single one. Scraping best practices are applied both on the web server side, kindly requesting permission and delayed sending rate; and from the web client side by granting continuous scraping, avoiding server blocks by User Agent rotation and trycatches / handlers for easy debugging. Then run time issues are fronted presenting two different options for looping construction, <code>furrr</code> and <code>foreach</code>. The latter has displayed interesting results and further improvements are taken into consideration. Then an overview of the open challenges is offered so that this work might be extended or integrated with other technologies. In the end legal profiles are addressed comparing scraping results and difficulties with a counterpart case study.</p>
<div id="what-is-web-scraping" class="section level2">
<h2><span class="header-section-number">2.1</span> What is Web Scraping</h2>
<p>Web Scraping is a technique aimed at extracting data from static or dynamic internet web pages. It can be done automatically and simultaneously but also by a scheduler that plans execution at a given time. Due to the absence of API’s to call regarding Milan Real Estate rental markets, this practice has to be forcly applied. The reason behind that and personal hope is to open source the scraping API so that in the future further analysis will be available without putting effort in gathering data fresh data.
Data/content in webpages is the most of the times well organized and accessible by urls. This is made possible by the effort put into building both the <em>website structure</em> and the <em>content architecture</em>. For website structure it is meant the way urls, pointing to webpages, are arranged throughout the website.
Website structure constitutes a <em>first dimension</em> of hierarchy. Some popular structures examples might regard social-networks where posts can be scrolled down within a single page named the wall. Scrolling down might end due to fewer posts, but the perception is a never-ending webpage associated to a single url. Instead personal profiles are dedicated to a specified unique url and even in profiles posts are allocated into a sub domain and might be scrolled down arranged by time since the day of social subscription. Online newspapers display their articles in the front wall and by accessing to one of them all the related following articles sometimes can be reached by an arrow, pointing right or left. Articles can also be suggested and the more the website is explored the more articles are likely to be seen twice within the same session. It will soon end up into a suggestion loop that it will recursively shows the same contents; recursive structures are popular in newspaper-type websites. Online Retailers as Amazon, based on filters, groups inside a single webpage (i.e. page n° 1) a fixed set of items, having their dedicated personal url attached to them. Furthermore Amazon offers the opportunity to skip to the following page (i.e. page n° 2), searching for another different and fixed set of items and so on until the last page/url. Generally website structures try to reflect both the user expectations with respect to the product on the website and the design expression of the web developer. For these reasons for each websites usage category exists a multitude of content architectures. For each content architectures there are multiple front end languages which are destinated to multiple end users. In the future expectations are tailor made webpages on users with respect to its personal preferences. Moreover web design in scraping plays an important role since the more sophisticated graphical technologies are implied, the harder will be to scrape information.</p>
<div class="figure">
<img src="images/content-vs-html-title.png" alt="" />
<p class="caption">general website structure</p>
</div>
<p>A <em>second dimension</em> of hierarchy is brought by content architecture in the name of the language used for content creation and organization i.e. HTML. HTML stands for Hyper Text Markup Language and …
HTML drives the hierarchy structure that is then generalized to the website structure. According to this point of view the hierarchical website structure is a consequence of the content architecture by means of HTML language ( <em>arborescence</em>: direction from root to leaves).
CSS language stands for Cascading Style Sheets and takes care of the style of the webpage. The combination of HTML and CSS offers a wide flexibility in building web sites, once again expressed by the vast amount of different designs on the web. Some websites’ components also might be tuned by Javascript language, which in the context of scraping adds a further layer of difficulty. As a matter of fact since Javascript components are dynamic within the webpage, scraping requires specialized libraries to enable different parser to get the content. CSS allows the scraper to target a class of objects in the web page that shares same style (e.g. same css query) so that each element that belongs to the class (i.e. share same style) can be gathered. This practice provides enormous advantages since by CSS a set of objects can be obtained within a single function call.
First and Second dimension of the scraping problem imply hierarchy, a simple way to approach the problem is to represent it through already known data structures. One way to imagine hierarchy in both of the two dimensions are graph based data structures named as <strong>Rooted Trees</strong>. By analyzing the first dimension through the lenses of Rooted trees it is possible to compress the whole problem into the general graph based jargon <span class="citation">(Diestel <a href="#ref-Graph_Diestel" role="doc-biblioref">2006</a>)</span>. Rooted trees must start with a root node which is the domain of the web page. Each <em>Node</em> is a url destination and each <em>Edge</em> is the connection between nodes. Connections have been made possible in the website by nesting urls inside webpages so that within a single webpage the user can access to a number of other related links. Furthermore, as an extension to the rooted tree framework a general graph theory component is introduced, i.e. the <em>Weight</em>. Each edge is associated to a weight whose interpretation is the run time cost to walk from a node to its conncted other nodes (e.g. from a url to the other). In addition the content inside each node takes the name of payload, which ultimately is the scope of the scraping processes. The walk from node 17 to node 8 in figure below (even though that is the case of a binary rooted tree) is called path and it represented as an ordered list of nodes connected by edges. In this context each node can have both a fixed and variable outgoing sub-nodes that are called <em>Children</em> . When root trees have a fixed set of children are called <em>k-ary</em> rooted trees. A node is said to be <em>Parent</em> to other nodes when it is connected to them by outgoing edge, in right figure below “head” is the parent of nodes “title” and “meta”. Nodes in the tree that shares the same parent node are said <em>Siblings</em>, “head” and “body” are siblings in figure @ref(tree_html). Moreover <em>Subtrees</em> are a set of nodes and edges comprised of a parent and its descendants e.g. node “body” with all of its descendants might constitute a subtree. The concept of subtree in both of the dimensions plays crucial role in cutting run time scraping processes and fake headers provision (see section <a href="scraping.html#spoofing">2.3.1</a>). If the website strucuture is locally reproducible and the content architecture within webpages tends to be equal, then functions for a single subtree might be extended to the rest of others subtrees that are siblings to the same parent node. Local reproducibility is a property according to which starting from a single url all the related urls can be inferred from a pattern. Equal content architecture throughout different single links means to have sort of standard shared-within advs criteria that each single rental advertisement has to refer. In addition two more metrics have might better describe the tree: <em>level</em> and <em>height</em>. The level of a node <span class="math inline">\(\mathbf{L}\)</span> counts the number of edges on the path from the root node to <span class="math inline">\(\mathbf{L}\)</span>. The height is the maximum level for any node in the tree, from now on <span class="math inline">\(\mathbf{H}\)</span>. What is worth to be anticipating is that functions are not going to be applied directly to siblings in the more general rooted tree. Instead it would be better segmenting the highest level rooted tree into a sequence of single subtrees whose roots are the siblings for reasons explained in section <a href="scraping.html#spoofing">2.3.1</a>.</p>
<div class="figure">
<img src="images/tree_html.PNG" alt="" />
<p class="caption">html_tree</p>
</div>
<div id="webstructure" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Immobiliare.it Webscraping website structure</h3>
<p>The structure of the website resembles the one from the popular online retailer Amazon. According to the query filters selected in a dedicated section (e.g. number of rooms 5, price less than 600 p.m. etc), the url is shaped so that each further filter added is appened at the end to the domain url <code>https://www.immobiliare.it/</code> root node (see figure below). Filters are appended to the url with a given syntax, that is proper of the website and that is configurable in general url composition. Once filters are all applied to the root domain this constitutes the new url root domain node that might have this appereance: <code>https://www.immobiliare.it/affitto-case/milano/?criterio=rilevanza&amp;superficieMinima=60</code>. Since this is true only for page n°1 and contains the first 25 advs all the remaining siblings nodes corresponding to next pages have to be generated. Here comes handy the Local reproducibility property introduced in the previous section. The rest of the siblings, i.e. the ones belonging to page 2 (with the attached 25 advs) and to page 3 can be generated by appending <code>&amp;pag=n</code> at the end of the url, where n is the page number reference (from now on referred as <em>pagination</em>). For page number 4 the exact url that points to the web page, being equal the filters from above, is <code>https://www.immobiliare.it/affitto-case/milano/?criterio=rilevanza&amp;superficieMinima=60&amp;pag=4</code>. Author customary choice is to stop pagination up to 300 pages since spatial data can not be to large due to computational burdens. Moreover thanks to the reverse engineer applied to url building tree height <span class="math inline">\(\mathbf{H}\)</span> is pruned, so the parsing part is cut short. At this point pagination has generated a list of siblings urls whose children number is fixed (i.e. 25 per page). That makes those trees k-ary, where k is 25 indicating the number of children. K-ary trees with equal content structure shared across siblings allow to design a single function to call that could be mapped to all the other siblings. In addition in order to further disassemble the website and making available the whole set of single rental advertisement links for each page (ranging from 1 to 300) a specific function has been made. As a consequence a single function call <code>scrape_href()</code> can grab all the links inside page 1. The function is then iterated along all the previously generated siblings (i.e. pages) obtaining a collection of all the single ads links belonging to the set of pages generated.</p>
<div class="figure">
<img src="images/website_tree.jpg" alt="" />
<p class="caption">website_tree</p>
</div>
<div class="figure">
<img src="images/children_str.jpg" alt="" />
<p class="caption">children_str</p>
</div>
</div>
<div id="immobiliare.it-webscraping-content-architecture-with-rvest" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Immobiliare.it Webscraping content architecture with <code>rvest</code></h3>
<p>To start a general scraping function it is required a target url (i.e. the filtered root node url). Then a <code>html_session</code> nested list object is opened by specifying the url and the request data that the user need to send to the web server (see left part to dashed line image <a href="#fig:workflow"><strong>??</strong></a>). Information to be attached to the web server request will be further explored later, tough they are mainly three: User Agents, emails references and proxy servers. <code>html_session</code> objects contains a number of useful information such as: the url, the response, coockies, session times etc. Once the connection is established (request response 200) all the following operations rely on the opened session, in other words for the time being in the session the user will be authorized with the before-provided characteristics through the request.
The list object contains mostly the html content of the webpage and that is where data needs to be parsed and collected. The list as said can disclose other interesting metadata that might be interesting but is beyond the scope of the analysis.
The workflow below schematize what scraping is doing:</p>
<div class="figure">
<img src="images/workflow.png" alt="" />
<p class="caption">workflow</p>
</div>
<p>In the right part of the dashed vertical line is represented a sequence of <code>rvest</code><span class="citation">(Wickham <a href="#ref-rvest" role="doc-biblioref">2019</a>)</span> functions that follow a general step by step text comprehension rules. <code>rvest</code> first handles parsing the html content of the web page within the session object <code>read_html()</code>, secondly looking for a single node <code>html_nodes()</code> through CSS. CSS is the way to tell <code>rvest</code> to point to a precise node in the webpage and this is brought by query. Thirdly it converts the content into readable text with <code>html_text()</code>. The entire process can be also thought as an autoencoder, where adding decoding layer after layer on top of the starting url, it can be reached the final requested payload data wanted. The reason why the process appear straightforward and simple relies in the blending with <code>rvest</code> by the pipe <code>%&gt;%</code> operator. Below it is shown a function that exemplifies scrapping the price.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="scraping.html#cb1-1"></a>scrapeprice.imm =<span class="st"> </span><span class="cf">function</span>(session) {</span>
<span id="cb1-2"><a href="scraping.html#cb1-2"></a>  </span>
<span id="cb1-3"><a href="scraping.html#cb1-3"></a>  opensess =<span class="st"> </span><span class="kw">read_html</span>(session)</span>
<span id="cb1-4"><a href="scraping.html#cb1-4"></a>  price  =<span class="st"> </span>opensess <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-5"><a href="scraping.html#cb1-5"></a><span class="st">    </span><span class="kw">html_nodes</span>(<span class="dt">css =</span><span class="st">&quot;.im-mainFeatures__title&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-6"><a href="scraping.html#cb1-6"></a><span class="st">    </span><span class="kw">html_text</span>() <span class="op">%&gt;%</span></span>
<span id="cb1-7"><a href="scraping.html#cb1-7"></a><span class="st">    </span><span class="kw">str_trim</span>() </span>
<span id="cb1-8"><a href="scraping.html#cb1-8"></a>  </span>
<span id="cb1-9"><a href="scraping.html#cb1-9"></a>  <span class="cf">if</span>(<span class="kw">is.null</span>(price) <span class="op">||</span><span class="st"> </span><span class="kw">identical</span>(price, <span class="kw">character</span>(<span class="dv">0</span>))) {</span>
<span id="cb1-10"><a href="scraping.html#cb1-10"></a>    price2 =<span class="st"> </span>opensess <span class="op">%&gt;%</span></span>
<span id="cb1-11"><a href="scraping.html#cb1-11"></a><span class="st">      </span><span class="kw">html_nodes</span>(<span class="dt">css =</span><span class="st">&#39;.im-features__value , .im-features__title&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-12"><a href="scraping.html#cb1-12"></a><span class="st">      </span><span class="kw">html_text</span>() <span class="op">%&gt;%</span></span>
<span id="cb1-13"><a href="scraping.html#cb1-13"></a><span class="st">      </span><span class="kw">str_trim</span>()</span>
<span id="cb1-14"><a href="scraping.html#cb1-14"></a>    </span>
<span id="cb1-15"><a href="scraping.html#cb1-15"></a>    <span class="cf">if</span> (<span class="st">&quot;prezzo&quot;</span> <span class="op">%in%</span><span class="st"> </span>price2) {</span>
<span id="cb1-16"><a href="scraping.html#cb1-16"></a>      pos =<span class="st"> </span><span class="kw">match</span>(<span class="st">&quot;prezzo&quot;</span>,price2)</span>
<span id="cb1-17"><a href="scraping.html#cb1-17"></a>      <span class="kw">return</span>(price2[pos<span class="op">+</span><span class="dv">1</span>])  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-18"><a href="scraping.html#cb1-18"></a><span class="st">        </span><span class="kw">str_replace_all</span>(<span class="kw">c</span>(<span class="st">&quot;€&quot;</span>=<span class="st">&quot;&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">.&quot;</span>=<span class="st">&quot;&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-19"><a href="scraping.html#cb1-19"></a><span class="st">        </span><span class="kw">str_extract</span>( <span class="st">&quot;</span><span class="ch">\\</span><span class="st">-*</span><span class="ch">\\</span><span class="st">d+</span><span class="ch">\\</span><span class="st">.*</span><span class="ch">\\</span><span class="st">d*&quot;</span>) <span class="op">%&gt;%</span><span class="st">  </span></span>
<span id="cb1-20"><a href="scraping.html#cb1-20"></a><span class="st">        </span><span class="kw">str_replace_na</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-21"><a href="scraping.html#cb1-21"></a><span class="st">        </span><span class="kw">str_replace</span>(<span class="st">&quot;NA&quot;</span>, <span class="st">&quot;Prezzo Su Richiesta&quot;</span>)</span>
<span id="cb1-22"><a href="scraping.html#cb1-22"></a>    } <span class="cf">else</span> {</span>
<span id="cb1-23"><a href="scraping.html#cb1-23"></a>      <span class="kw">return</span>(<span class="ot">NA_character_</span>)</span>
<span id="cb1-24"><a href="scraping.html#cb1-24"></a>    }</span>
<span id="cb1-25"><a href="scraping.html#cb1-25"></a>  } <span class="cf">else</span> {</span>
<span id="cb1-26"><a href="scraping.html#cb1-26"></a>    <span class="kw">return</span>(price) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-27"><a href="scraping.html#cb1-27"></a><span class="st">      </span><span class="kw">str_replace_all</span>(<span class="kw">c</span>(<span class="st">&quot;€&quot;</span>=<span class="st">&quot;&quot;</span>,<span class="st">&quot;</span><span class="ch">\\</span><span class="st">.&quot;</span>=<span class="st">&quot;&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-28"><a href="scraping.html#cb1-28"></a><span class="st">      </span><span class="kw">str_extract</span>( <span class="st">&quot;</span><span class="ch">\\</span><span class="st">-*</span><span class="ch">\\</span><span class="st">d+</span><span class="ch">\\</span><span class="st">.*</span><span class="ch">\\</span><span class="st">d*&quot;</span>) <span class="op">%&gt;%</span><span class="st">  </span></span>
<span id="cb1-29"><a href="scraping.html#cb1-29"></a><span class="st">      </span><span class="kw">str_replace_na</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-30"><a href="scraping.html#cb1-30"></a><span class="st">      </span><span class="kw">str_replace</span>(<span class="st">&quot;NA&quot;</span>, <span class="st">&quot;Prezzo Su Richiesta&quot;</span>)</span>
<span id="cb1-31"><a href="scraping.html#cb1-31"></a>      </span>
<span id="cb1-32"><a href="scraping.html#cb1-32"></a>  }</span>
<span id="cb1-33"><a href="scraping.html#cb1-33"></a>  </span>
<span id="cb1-34"><a href="scraping.html#cb1-34"></a>}</span></code></pre></div>
<p>The function takes as a single argument a session object which is initialized in one other function. Then It reads the inner html content in the session storing the information into an obj called the <code>opensess</code>. Another obj is created, namely price, right after the pipe operator a css query into the html is called. The css query <code>.im-mainFeatures__title</code> points to a precise data stored in immobiliare web page header, right below the main title. Expectation are that price is a one-element chr vector, containing the price and some other unnecessary characters. Then the algorithm enters into the first <code>if</code> statement. The handler checks if the object <code>price</code> is empty. If it doesn’t the algorithm jumps to the end of the algorithm and returns the cleaned quantity. But If it does it takes again the <code>opensess</code> and redirect to a second css query <code>.im-features__value , .im-features__title</code> where price, once again in the webpage, could be found. Please note that This is all done within the same session, so no more request additional information has to be sent. Since the latter css query points to data stored inside a list, for the time being the newly created obj price2 is a list containing various information. Then the algorithm flow enters into the second <code>if</code> statement that checks whether the <code>"prezzo"</code> is matched the list or not, if it does it returns the +1 position index element with respect to the “prezzo” positioning. This happens because data in price2 list are stored by couples sequentially, e.g. [title, “Appartamento Sempione”, energy class, “G”, “prezzo”, 1200/al mese]. When it returns the element corresponding to +1 position index it applies also some data wrangling with <code>stringr</code> package to keep out overabundant characters. The function then escapes in the else statement by setting <code>price2 = NA_Character_</code> once no css query could be finding the price information. the <em>character-string</em> type has to be imposed due to fact that later they can not be bind. In other words if the function is evaluated for a url and returns the price quantity, but then is evaluated for url2 and outputs NA (no character) then results can not be combined into dataframe due to different types.</p>
<p>Once all the functions have been created they need to be called together and then data coming after them need to be combined. This is done by <code>get,data.catsing()</code> which at first checks the validity of the url, then takes the same url as input and filters it as a session object. Then simultaneously all the functions are called and then combined. All this happens inside a <code>foreach</code> parallel loop called by <code>scrape.all.info()</code></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="scraping.html#cb2-1"></a>scrape.all.info =<span class="st"> </span><span class="cf">function</span>(<span class="dt">url =</span> <span class="st">&quot;https://www.immobiliare.it/affitto-case...&quot;</span>,</span>
<span id="cb2-2"><a href="scraping.html#cb2-2"></a>                           <span class="dt">vedi =</span> <span class="ot">FALSE</span>, </span>
<span id="cb2-3"><a href="scraping.html#cb2-3"></a>                           <span class="dt">scrivi =</span> <span class="ot">FALSE</span>, </span>
<span id="cb2-4"><a href="scraping.html#cb2-4"></a>                           <span class="dt">silent =</span> <span class="ot">FALSE</span>){</span>
<span id="cb2-5"><a href="scraping.html#cb2-5"></a>  <span class="cf">if</span> (silent) {</span>
<span id="cb2-6"><a href="scraping.html#cb2-6"></a>    start =<span class="st"> </span><span class="kw">as_hms</span>(<span class="kw">Sys.time</span>()); <span class="kw">cat</span>(<span class="st">&#39;Starting the process...</span><span class="ch">\n\n</span><span class="st">&#39;</span>)</span>
<span id="cb2-7"><a href="scraping.html#cb2-7"></a>    <span class="kw">message</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">The process has started in&#39;</span>,<span class="kw">format</span>(start,<span class="dt">usetz =</span> <span class="ot">TRUE</span>))  </span>
<span id="cb2-8"><a href="scraping.html#cb2-8"></a>  }</span>
<span id="cb2-9"><a href="scraping.html#cb2-9"></a>  <span class="co"># open parallel multisession</span></span>
<span id="cb2-10"><a href="scraping.html#cb2-10"></a>  cl =<span class="st"> </span><span class="kw">makeCluster</span>(<span class="kw">detectCores</span>()<span class="op">-</span><span class="dv">1</span>) <span class="co">#using max cores - 1 for parallel processing</span></span>
<span id="cb2-11"><a href="scraping.html#cb2-11"></a>  <span class="kw">registerDoParallel</span>(cl)</span>
<span id="cb2-12"><a href="scraping.html#cb2-12"></a>  start =<span class="st"> </span><span class="kw">as_hms</span>(<span class="kw">Sys.time</span>())</span>
<span id="cb2-13"><a href="scraping.html#cb2-13"></a>  </span>
<span id="cb2-14"><a href="scraping.html#cb2-14"></a>  <span class="cf">if</span> (silent) {</span>
<span id="cb2-15"><a href="scraping.html#cb2-15"></a>    <span class="kw">message</span>(<span class="st">&#39;</span><span class="ch">\n\n</span><span class="st">Start all the requests at time:&#39;</span>, <span class="kw">format</span>(start,<span class="dt">usetz =</span> T))  </span>
<span id="cb2-16"><a href="scraping.html#cb2-16"></a>  }</span>
<span id="cb2-17"><a href="scraping.html#cb2-17"></a>  ALL =<span class="st"> </span><span class="kw">foreach</span>(<span class="dt">i =</span> <span class="kw">seq_along</span>(links),</span>
<span id="cb2-18"><a href="scraping.html#cb2-18"></a>                <span class="dt">.packages =</span> lista.pacchetti,</span>
<span id="cb2-19"><a href="scraping.html#cb2-19"></a>                <span class="dt">.combine =</span> <span class="st">&quot;bind_rows&quot;</span>,</span>
<span id="cb2-20"><a href="scraping.html#cb2-20"></a>                <span class="dt">.multicombine =</span> <span class="ot">FALSE</span>,</span>
<span id="cb2-21"><a href="scraping.html#cb2-21"></a>                <span class="dt">.export =</span><span class="st">&quot;links&quot;</span> ,</span>
<span id="cb2-22"><a href="scraping.html#cb2-22"></a>                <span class="dt">.verbose =</span> <span class="ot">TRUE</span>,</span>
<span id="cb2-23"><a href="scraping.html#cb2-23"></a>                <span class="dt">.errorhandling=</span><span class="st">&#39;pass&#39;</span>) <span class="op">%dopar%</span><span class="st"> </span>{</span>
<span id="cb2-24"><a href="scraping.html#cb2-24"></a>                  <span class="kw">source</span>(<span class="st">&quot;utils.R&quot;</span>)</span>
<span id="cb2-25"><a href="scraping.html#cb2-25"></a>                  <span class="kw">sourceEntireFolder</span>(<span class="st">&quot;functions_singolourl&quot;</span>)</span>
<span id="cb2-26"><a href="scraping.html#cb2-26"></a>                  get.data.catsing =<span class="st"> </span><span class="cf">function</span>(singolourl){</span>
<span id="cb2-27"><a href="scraping.html#cb2-27"></a>                    </span>
<span id="cb2-28"><a href="scraping.html#cb2-28"></a>                    <span class="co"># dormi()</span></span>
<span id="cb2-29"><a href="scraping.html#cb2-29"></a>                    <span class="co"># </span></span>
<span id="cb2-30"><a href="scraping.html#cb2-30"></a>                    <span class="cf">if</span>(<span class="op">!</span><span class="kw">is_url</span>(singolourl)){</span>
<span id="cb2-31"><a href="scraping.html#cb2-31"></a>                      <span class="kw">stop</span>(<span class="kw">paste0</span>(<span class="st">&quot;The following url does not seem either to exist or it is invalid&quot;</span>, singolourl))</span>
<span id="cb2-32"><a href="scraping.html#cb2-32"></a>                    }</span>
<span id="cb2-33"><a href="scraping.html#cb2-33"></a>                    </span>
<span id="cb2-34"><a href="scraping.html#cb2-34"></a>                    session =<span class="st"> </span><span class="kw">html_session</span>(singolourl, <span class="kw">user_agent</span>(agents[<span class="kw">sample</span>(<span class="dv">1</span>)]))</span>
<span id="cb2-35"><a href="scraping.html#cb2-35"></a>                    <span class="cf">if</span> (<span class="kw">class</span>(session) <span class="op">==</span><span class="st"> &quot;session&quot;</span>) {</span>
<span id="cb2-36"><a href="scraping.html#cb2-36"></a>                      session =<span class="st"> </span>session<span class="op">$</span>response  </span>
<span id="cb2-37"><a href="scraping.html#cb2-37"></a>                    }</span>
<span id="cb2-38"><a href="scraping.html#cb2-38"></a>                      </span>
<span id="cb2-39"><a href="scraping.html#cb2-39"></a>                    id         =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapehouse.ID</span>(session)},</span>
<span id="cb2-40"><a href="scraping.html#cb2-40"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapehouse.ID&quot;</span>) })</span>
<span id="cb2-41"><a href="scraping.html#cb2-41"></a>                    lat        =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapelat.imm</span>(session)},</span>
<span id="cb2-42"><a href="scraping.html#cb2-42"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapelat.imm&quot;</span>) })</span>
<span id="cb2-43"><a href="scraping.html#cb2-43"></a>                    long       =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapelong.imm</span>(session)},</span>
<span id="cb2-44"><a href="scraping.html#cb2-44"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapelong.imm&quot;</span>) })</span>
<span id="cb2-45"><a href="scraping.html#cb2-45"></a>                    location   =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">take.address</span>(session)},</span>
<span id="cb2-46"><a href="scraping.html#cb2-46"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in take.address&quot;</span>) })</span>
<span id="cb2-47"><a href="scraping.html#cb2-47"></a>                    condom     =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapecondom.imm</span>(session)},</span>
<span id="cb2-48"><a href="scraping.html#cb2-48"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapecondom.imm&quot;</span>) })</span>
<span id="cb2-49"><a href="scraping.html#cb2-49"></a>                    buildage   =<span class="st"> </span><span class="kw">tryCatch</span>({<span class="kw">scrapeagebuild.imm</span>(session)},</span>
<span id="cb2-50"><a href="scraping.html#cb2-50"></a>                                          <span class="dt">error =</span> <span class="cf">function</span>(e){ <span class="kw">message</span>(<span class="st">&quot;some problem occured in scrapeagebuild.imm&quot;</span>) })</span>
<span id="cb2-51"><a href="scraping.html#cb2-51"></a>                    </span>
<span id="cb2-52"><a href="scraping.html#cb2-52"></a>                  ...</span>
<span id="cb2-53"><a href="scraping.html#cb2-53"></a>                  </span>
<span id="cb2-54"><a href="scraping.html#cb2-54"></a>                   combine =<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">ID        =</span> id,</span>
<span id="cb2-55"><a href="scraping.html#cb2-55"></a>                                     <span class="dt">LAT       =</span> lat, </span>
<span id="cb2-56"><a href="scraping.html#cb2-56"></a>                                     <span class="dt">LONG      =</span> long,</span>
<span id="cb2-57"><a href="scraping.html#cb2-57"></a>                                     <span class="dt">LOCATION  =</span> location,</span>
<span id="cb2-58"><a href="scraping.html#cb2-58"></a>                                     <span class="dt">CONDOM    =</span> condom,</span>
<span id="cb2-59"><a href="scraping.html#cb2-59"></a>                                     <span class="dt">BUILDAGE  =</span> buildage,</span>
<span id="cb2-60"><a href="scraping.html#cb2-60"></a>                                    </span>
<span id="cb2-61"><a href="scraping.html#cb2-61"></a>                  ...</span>
<span id="cb2-62"><a href="scraping.html#cb2-62"></a>                  </span>
<span id="cb2-63"><a href="scraping.html#cb2-63"></a>                  </span>
<span id="cb2-64"><a href="scraping.html#cb2-64"></a>                  <span class="kw">return</span>(combine) </span>
<span id="cb2-65"><a href="scraping.html#cb2-65"></a>                  <span class="er">}</span></span>
<span id="cb2-66"><a href="scraping.html#cb2-66"></a>  <span class="kw">stopCluster</span>(cl)</span>
<span id="cb2-67"><a href="scraping.html#cb2-67"></a>  <span class="kw">return</span>(ALL)</span>
<span id="cb2-68"><a href="scraping.html#cb2-68"></a><span class="er">}</span></span></code></pre></div>
<p>The skeleton constitutes a standard format adopted for many other scraping function used in the analysis. Being equal the css query what it changes is the matching term, i.e. “numero camere” instead of “prezzo” to look for how many rooms there are in the house. This is true for all the information contained in the list accessed by the fixed css query. Those that are not they are a few and they do not need to be scraped.
In addition some other functions outputs need to undergo to further heavy cleaning steps in order to be usable As a consequence oh that functions need also to be broken down by pieces into many single .R files whose names correspond to each important information.
Below it is printed the tree structure folder that composes the main elements of the scraping procedure. It can be notices that the two folders, namely functions_singolourl and functions_url enclose all the single functions that allows to grab single information from session. Folders with a customized function are then sourced within the two main functions, scrape.all and scrape.all.info so data can be extracted.</p>
<pre><code> levelName
1  immobiliare.it-WebScraping     
2   ¦--functions_singolourl       
3   ¦   ¦--0scrapesqfeetINS.R     
4   ¦   ¦--0scrapenroomINS.R      
5   ¦   ¦--0scrapepriceINS.R      
6   ¦   ¦--0scrapetitleINS.R      
7   ¦   ¦--ScrapeAdDate.R         
8   ¦   ¦--ScrapeAge.R            
9   ¦   ¦--ScrapeAgeBuilding.R    
10  ¦   ¦--ScrapeAirConditioning.R
11  ¦   ¦--ScrapeAptChar.R        
12  ¦   ¦--ScrapeCatastInfo.R     
13  ¦   ¦--ScrapeCompart.R        
14  ¦   ¦--ScrapeCondom.R         
15  ¦   ¦--ScrapeContr.R          
16  ¦   ¦--ScrapeDisp.R           
17  ¦   ¦--ScrapeEnClass.R        
18  ¦   ¦--ScrapeFloor.R          
19  ¦   ¦--ScrapeHasMulti.R       
20  ¦   ¦--ScrapeHeating.R        
21  ¦   ¦--ScrapeHouseID.R        
22  ¦   ¦--ScrapeHouseTxtDes.R    
23  ¦   ¦--ScrapeLAT.R            
24  ¦   ¦--ScrapeLONG.R           
25  ¦   ¦--ScrapeLoweredPrice.R   
26  ¦   ¦--ScrapeMetrature.R      
27  ¦   ¦--ScrapePhotosNum.R      
28  ¦   ¦--ScrapePostAuto.R       
29  ¦   ¦--ScrapePropType.R       
30  ¦   ¦--ScrapeReaReview.R      
31  ¦   ¦--ScrapeStatus.R         
32  ¦   ¦--ScrapeTotPiani.R       
33  ¦   ¦--ScrapeType.R           
34  ¦   °--take_location.R        
35  ¦--scrapeALL.R                
36  ¦--scrapeALLINFO.R            
37  ¦--functions_url              
38  ¦   ¦--ScrapeHREF.R           
39  ¦   ¦--ScrapePrice.R          
40  ¦   ¦--ScrapePrimaryKey.R     
41  ¦   ¦--ScrapeRooms.R          
42  ¦   ¦--ScrapeSpace.R          
43  ¦   °--ScrapeTitle.R          
44  ¦--libs.R                     
45  ¦--utils.R                    
46  ¦--README.Rmd                 
47  ¦--README.md                  
48  °--simulations                
49      ¦--rt_match_vs_forloop.R  
50      °--runtime_simul.R</code></pre>
</div>
</div>
<div id="best-practices" class="section level2">
<h2><span class="header-section-number">2.2</span> Scraping Best Practices and Security provisions</h2>
<p>Robots.txt files are (rivedi citation) a way to kindly ask webbots, spiders, crawlers, wanderers and the like to access or not access certain parts of a webpage. The de facto ‘standard’ never made it beyond a informal “Network Working Group INTERNET DRAFT”. Nonetheless, the use of robots.txt files is widespread (e.g. <a href="https://en.wikipedia.org/robots.txt" class="uri">https://en.wikipedia.org/robots.txt</a>, <a href="https://www.google.com/robots.txt" class="uri">https://www.google.com/robots.txt</a>) and bots from Google, Yahoo and the like will adhere to the rules defined in robots.txt files, although their <em>interpretation</em> of those rules might differ.</p>
<p>Robots.txt files are plain text and always found at the root of a website’s domain. The syntax of the files in essence follows a fieldname: value scheme with optional preceding user-agent: … lines to indicate the scope of the following rule block. Blocks are separated by blank lines and the omission of a user-agent field (which directly corresponds to the HTTP user-agent field) is seen as referring to all bots. # serves to comment lines and parts of lines. Everything after # until the end of line is regarded a comment. Possible field names are: user-agent, disallow, allow, crawl-delay, sitemap, and host. For further notions <span class="citation">(Meissner and Ren <a href="#ref-robotstxt" role="doc-biblioref">2020</a>, <span class="citation">@google:robottxt</span>)</span></p>
<p>Some interpretation problems:</p>
<ul>
<li>finding no robots.txt file at the server (e.g. HTTP status code 404) implies that everything is permitted</li>
<li>subdomains should have there own robots.txt file if not it is assumed that everything is allowed</li>
<li>redirects involving protocol changes - e.g. upgrading from http to https - are followed and considered no domain or subdomain change - so whatever is found at the end of the redirect is considered to be the - robots.txt file for the original domain</li>
<li>redirects from subdomain www to the doamin is considered no domain change - so whatever is found at the end of the redirect is considered to be the robots.txt file for the subdomain originally requested</li>
</ul>
<p>For the thesis purposes it has been designed a dedicated function to assess whether the domain or the related paths require specific actions or they prevent some activity on the target. The following <code>checkpermission()</code> function has been integrated inside the scraping system and it is called once at the starting point.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="scraping.html#cb4-1"></a>dominio =<span class="st"> &quot;immobiliare.it&quot;</span></span>
<span id="cb4-2"><a href="scraping.html#cb4-2"></a></span>
<span id="cb4-3"><a href="scraping.html#cb4-3"></a>checkpermission =<span class="st"> </span><span class="cf">function</span>(dom) {</span>
<span id="cb4-4"><a href="scraping.html#cb4-4"></a>    </span>
<span id="cb4-5"><a href="scraping.html#cb4-5"></a>    robot =<span class="st"> </span><span class="kw">robotstxt</span>(<span class="dt">domain =</span> dom)</span>
<span id="cb4-6"><a href="scraping.html#cb4-6"></a>    vd =<span class="st"> </span>robot<span class="op">$</span><span class="kw">check</span>()[<span class="dv">1</span>]</span>
<span id="cb4-7"><a href="scraping.html#cb4-7"></a>    <span class="cf">if</span> (vd) {</span>
<span id="cb4-8"><a href="scraping.html#cb4-8"></a>        <span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">robot.txt for&quot;</span>, dom, <span class="st">&quot;is okay with scraping!&quot;</span>)</span>
<span id="cb4-9"><a href="scraping.html#cb4-9"></a>    } <span class="cf">else</span> {</span>
<span id="cb4-10"><a href="scraping.html#cb4-10"></a>        <span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">robot.txt does not like what you&#39;re doing&quot;</span>)</span>
<span id="cb4-11"><a href="scraping.html#cb4-11"></a>        <span class="kw">stop</span>()</span>
<span id="cb4-12"><a href="scraping.html#cb4-12"></a>    }</span>
<span id="cb4-13"><a href="scraping.html#cb4-13"></a>}</span>
<span id="cb4-14"><a href="scraping.html#cb4-14"></a><span class="co">## metti path allowed check</span></span>
<span id="cb4-15"><a href="scraping.html#cb4-15"></a><span class="kw">checkpermission</span>(dominio)</span></code></pre></div>
<pre><code>## 
## robot.txt for immobiliare.it is okay with scraping!</code></pre>
<p>Further improvements in this direction might come with the <code>polite</code> package <span class="citation">(Perepolkin <a href="#ref-polite" role="doc-biblioref">2019</a>)</span> which combines the power of the <code>robotstxt</code>, the <code>ratelimitr</code> <span class="citation">(Shah <a href="#ref-ratelimitr" role="doc-biblioref">2018</a>)</span> to limit sequential requests together with the <code>memoise</code> <span class="citation">(Wickham et al. <a href="#ref-memoise" role="doc-biblioref">2017</a>)</span> for response caching. This package is wrapped up around 3 simple but effective ideas:</p>
<blockquote>
<p>The three pillars of a polite session are seeking permission, taking slowly and never asking twice.</p>
</blockquote>
<p>The three pillars constitute the <em>Ethical</em> web scraping manifesto <span class="citation">(Densmore <a href="#ref-densmore_2019" role="doc-biblioref">2019</a>)</span> which are common shared practises that are aimed to self regularize scrapers. These have to be intended as best practices, not in any case as law enforcements, however many scrapers themselves, as website administrators or analyst, have fought in their daily working tasks with bots. Crawling bots in intensive scraping processes might fake real client navigation logs and as a consequence might induce distorted analytics. Due to this fact comes the need to find a common operating ground and therefore politely asking for permission.</p>
</div>
<div id="security-provisions-user-agents-proxies-and-handlers" class="section level2">
<h2><span class="header-section-number">2.3</span> Security provisions: User agents, Proxies and Handlers</h2>
<p>HTTP requests to the website server by web clients come with some mandatory information packed in it. The process according to which HTTP protocols allow to exchange information can be easily thought with an everyday real world analogy. As a generic person A rings the door’s bell of person B’s house. A comes to B door with its personal information, its name, surname, where he lives etc. At this point B may either answer to A requests by opening the door and let him enter given the set of information he has, or it may not since B is not sure of the real intentions of A. This typical everyday situation in nothing more what happens billions of times on the internet everyday, the user (in the example above A) is interacting with a server website (part B) sending packets of information. If a server does not trust the information provided by the user, if the requests are too many, if the requests seems to be scheduled due to fixed sleeping time, a server can block requests. In certain cases it can even forbid the user log to the website. The language the two parties exchanges are coded in numbers that ranges from 100 to 511, each of which has its own specific significance. A popular case of this type of interaction occurs when users are not connected to internet so the server responds 404, page not found. Servers are built with a immune-system like software that raises barriers and block users to prevent dossing or other illegal practices.</p>
<div class="figure">
<img src="images/how_web_works.png" alt="" />
<p class="caption">How Web Works</p>
</div>
<p>This procedure is a dayly issue to people that are trying to collect information from websites. Google performs it everyday with its spider crawlers, which are very sophisticated bots that scrapes over a enormous range of websites. This challenge can be addressed in multiple ways, there are some specific Python packages that overcome this issue. The are also certain types of scraping as the Selenium web driver automation that simulates browser automation. Selenium allows the user not to be easily detected by the server immune system and peaceful. Precautions have not been taken lightly, and a simple but effective approach is proposed.</p>
<div id="spoofing" class="section level3">
<h3><span class="header-section-number">2.3.1</span> User Agents Spoofing</h3>
<p>A user agent <span class="citation">(WhoIsHostingThis.com <a href="#ref-whoishostingthis.com" role="doc-biblioref">2020</a>)</span> is a string of characters in each web browser that serves as identification card. The user agent permits the web server to be able to identify the user operating system and the browser. Then, the web server uses the exchanged information to determine what content should be presented to particular operating systems and web browsers on a series of devices. The user agent string includes the user application or software, the operating system (and their versions), the web client, the web client’s version, as well as the web engine responsible for the content display (such as AppleWebKit). The user agent string is sent in the form of a HTTP request header. Since User Agents acts as middle man between the client request and the server response, then from a continous scraping point of view it would be better rotating them, so that each time the middle man looks different. The solution adopted builds a vector of user agent strings identified by different specifications, different web client, different operating system and so on, then samples 1 of them
Then whenever a request from a web browser is sent to a web server, 1 random sample string is drawn from the user agents pool. So each time the user is sending the request it appears to be a different User Agent.
Below the user agents rotation pool:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="scraping.html#cb6-1"></a><span class="kw">set.seed</span>(<span class="dv">27</span>)</span>
<span id="cb6-2"><a href="scraping.html#cb6-2"></a>agents =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&quot;</span>, </span>
<span id="cb6-3"><a href="scraping.html#cb6-3"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&quot;</span>, </span>
<span id="cb6-4"><a href="scraping.html#cb6-4"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&quot;</span>, </span>
<span id="cb6-5"><a href="scraping.html#cb6-5"></a>    <span class="st">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14&quot;</span>, </span>
<span id="cb6-6"><a href="scraping.html#cb6-6"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&quot;</span>, </span>
<span id="cb6-7"><a href="scraping.html#cb6-7"></a>    <span class="st">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36&quot;</span>, </span>
<span id="cb6-8"><a href="scraping.html#cb6-8"></a>    <span class="st">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36&quot;</span>, </span>
<span id="cb6-9"><a href="scraping.html#cb6-9"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&quot;</span>, </span>
<span id="cb6-10"><a href="scraping.html#cb6-10"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;</span>, </span>
<span id="cb6-11"><a href="scraping.html#cb6-11"></a>    <span class="st">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0&quot;</span>)</span>
<span id="cb6-12"><a href="scraping.html#cb6-12"></a>agents[<span class="kw">sample</span>(<span class="dv">1</span>)]</span></code></pre></div>
<pre><code>## [1] &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&quot;</code></pre>
<p>A more secure approach might be a further rotation of proxies between the back and forth sending-receving process. A proxy server acts as a gateway between the web user and the web server.
While the user is exploiting a proxy server, internet traffic flows through the proxy server on its way to the server requested. The request then comes back through that same proxy server and then the proxy server forwards the data received from the website back to the client. The final result will be linear combination of User Agents ID and Proxy server for each sending requests, grating a high security level.
Many proxy servers are offered in a paid version, so in this case since security barriers are not that high they will not be implemented. As a further disclaimer many online services are providing free proxies server access, but this comes at a personal security cost due to a couple of reasons:
- Free plan Proxies are shared among a number of different clients, so as long as someone has used them in the past for illegal purposes the client is indirectly inheriting their legal infringements.
- Very cheap proxies, for sure all of the ones free, have the activity redirected on their servers monitored, profiling in some cases a user privacy violation issue.</p>
</div>
<div id="handlers-and-trycatch" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Handlers and Trycatch</h3>
<p>During scraping many difficulties are met. Some of them might come from website structure issues, so that rooted-tree hierarchies are changed as a consequence of a restructuring. Some others might interest content architecture where data is reallocated to some other places in the webpage, then CSS query are no more able to catch data. Handlers in the form of trycatch error workarounds are explicitly built in this sense. The continuous building and testing of the scraping functioning has required the maintainer to have a precise and fast debugging experience. The following consideration might give a sense of the time consumed when debugging handlers are not implied: <code>get.data.catsing()</code> triggers 34 different scrapping functions that are supposed to point to 34 different data pieces. Within a single function call by default pagination generates 10 pages each of which contains at least 25 different single urls to be scrapped. That leads to a number of 8500 single data information, the probability given 8500 associated to something going missing or misparsed is undoubtedly high.
The solution proposed tries to handle fails by implementing as many trycatches as scrapping fucntions inside the a the single <code>get.data.catsing()</code> call. Then inside each single scrapping function are put the handlers. This set up allows to catch (and in some cases prevent) fails starting from the very end of the scraping process. As a consequence of this setting when a scrapping function is not able to gather data an error inside the function is thrown, then the error call is intercepted by the corresponding trycatch, which at the end messages where the error occurs.
Below some of the main handlers implied:</p>
<ul>
<li><code>.get_ua()</code> verifies that the User Agent in the session is not the default one.</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="scraping.html#cb8-1"></a>.get_ua =<span class="st"> </span><span class="cf">function</span>(sess) {</span>
<span id="cb8-2"><a href="scraping.html#cb8-2"></a>    <span class="kw">stopifnot</span>(<span class="kw">is.session</span>(sess))</span>
<span id="cb8-3"><a href="scraping.html#cb8-3"></a>    <span class="kw">stopifnot</span>(<span class="kw">is_url</span>(sess<span class="op">$</span>url))</span>
<span id="cb8-4"><a href="scraping.html#cb8-4"></a>    ua =<span class="st"> </span>sess<span class="op">$</span>response<span class="op">$</span>request<span class="op">$</span>options<span class="op">$</span>useragent</span>
<span id="cb8-5"><a href="scraping.html#cb8-5"></a>    <span class="kw">return</span>(ua)</span>
<span id="cb8-6"><a href="scraping.html#cb8-6"></a>}</span></code></pre></div>
<ul>
<li><code>.is_url()</code> verifies that the url input needed has the canonic form. This is done by a REGEX query.</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="scraping.html#cb9-1"></a>.is_url =<span class="st"> </span><span class="cf">function</span>(url) {</span>
<span id="cb9-2"><a href="scraping.html#cb9-2"></a>    re =<span class="st"> &quot;^(?:(?:http(?:s)?|ftp)://)(?:</span><span class="ch">\\</span><span class="st">S+(?::(?:</span><span class="ch">\\</span><span class="st">S)*)?@)?(?:(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;](?:-)*)*(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;])+)(?:</span><span class="ch">\\</span><span class="st">.(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;](?:-)*)*(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;])+)*(?:</span><span class="ch">\\</span><span class="st">.(?:[a-z0-9¡-&lt;ef&gt;&lt;U+00BF&gt;&lt;U+00BF&gt;]){2,})(?::(?:</span><span class="ch">\\</span><span class="st">d){2,5})?(?:/(?:</span><span class="ch">\\</span><span class="st">S)*)?$&quot;</span></span>
<span id="cb9-3"><a href="scraping.html#cb9-3"></a>    <span class="kw">grepl</span>(re, url)</span>
<span id="cb9-4"><a href="scraping.html#cb9-4"></a>}</span></code></pre></div>
<ul>
<li><code>.get_delay()</code> checks through the robotxt file if a delay between each request is kindly welcomed. When response is NA delay is not required.</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="scraping.html#cb10-1"></a>.get_delay =<span class="st"> </span><span class="cf">function</span>(domain) {</span>
<span id="cb10-2"><a href="scraping.html#cb10-2"></a>  </span>
<span id="cb10-3"><a href="scraping.html#cb10-3"></a>  <span class="kw">message</span>(<span class="kw">sprintf</span>(<span class="st">&quot;Refreshing robots.txt data for %s...&quot;</span>, domain))</span>
<span id="cb10-4"><a href="scraping.html#cb10-4"></a>  </span>
<span id="cb10-5"><a href="scraping.html#cb10-5"></a>  cd_tmp =<span class="st"> </span>robotstxt<span class="op">::</span><span class="kw">robotstxt</span>(domain)<span class="op">$</span>crawl_delay</span>
<span id="cb10-6"><a href="scraping.html#cb10-6"></a>  </span>
<span id="cb10-7"><a href="scraping.html#cb10-7"></a>  <span class="cf">if</span> (<span class="kw">length</span>(cd_tmp) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb10-8"><a href="scraping.html#cb10-8"></a>    star =<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(cd_tmp, useragent<span class="op">==</span><span class="st">&quot;*&quot;</span>)</span>
<span id="cb10-9"><a href="scraping.html#cb10-9"></a>    <span class="cf">if</span> (<span class="kw">nrow</span>(star) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) star =<span class="st"> </span>cd_tmp[<span class="dv">1</span>,]</span>
<span id="cb10-10"><a href="scraping.html#cb10-10"></a>    <span class="kw">as.numeric</span>(star<span class="op">$</span>value[<span class="dv">1</span>])</span>
<span id="cb10-11"><a href="scraping.html#cb10-11"></a>  } <span class="cf">else</span> {</span>
<span id="cb10-12"><a href="scraping.html#cb10-12"></a>    10L</span>
<span id="cb10-13"><a href="scraping.html#cb10-13"></a>  }</span>
<span id="cb10-14"><a href="scraping.html#cb10-14"></a>  </span>
<span id="cb10-15"><a href="scraping.html#cb10-15"></a>}</span>
<span id="cb10-16"><a href="scraping.html#cb10-16"></a>get_delay =<span class="st">  </span>memoise<span class="op">::</span><span class="kw">memoise</span>(.get_delay) <span class="co">## so that .get_delay results are cached</span></span>
<span id="cb10-17"><a href="scraping.html#cb10-17"></a><span class="kw">.get_delay</span>(<span class="dt">domain =</span> dominio)</span></code></pre></div>
<pre><code>## [1] NA</code></pre>
</div>
</div>
<div id="parallel-computing" class="section level2">
<h2><span class="header-section-number">2.4</span> Parallel Computing</h2>
<p>Since are opened as many sessions as single links and since for each link are supposed to be called 34 functions run time computation can take a while. Run time is crucial when dealing with active web pages and time to market in real estate is very important, in here originates the need to have always up-to-date data. Run time optimization involves each level of the scraping process from the “lowest” i.e. inside each single function to the “highest” i.e. the agglomerative function. Inside single scraping functions as a general criteria for loops are avoided due to Rcpp reasons, vectorization is preferred. Within agglomerative function instead the approach was to test two different results. All the following runtime examinations are performed on the <code>scrape()</code> functions which is a lightweight version of the final API function.
The first attempt was using <code>furrr</code> package <span class="citation">(Vaughan and Dancho <a href="#ref-furrr" role="doc-biblioref">2018</a>)</span> which enables mapping through a list with the <code>purrr</code>, along with a <code>future</code> parallel back end. The approach has shown decent preformance, but its run time drastically increases when more requests are sent. This leads to a preventive conclusion about the computational complexity: it has to be at least linear with steep slope. Empirical demonstrations have been made:</p>
<div class="figure">
<img src="images/run_timefurrr.png" alt="" />
<p class="caption">computational complexity analysis with Furrr</p>
</div>
<p>On the x-axis in the figure the number of urls which are evaluated together, on y axis the run time taken measured in seconds. Iteration after iteration the urls considered are cumulated one at at a time. Looking at the blue smoothing curve in between confidence lines the big-O guess might be linear time <span class="math inline">\(\mathcal{O}(n)\)</span>, where n are the links considered.</p>
<p>A second attempt tried to explore the <code>foreach</code> package <span class="citation">(Microsoft and Weston <a href="#ref-foreach" role="doc-biblioref">2020</a>)</span>. This quite recent package enables a new looping construct for executing R code in an iterative way. The core reason for using the <code>foreach</code> package is that it supports <em>parallel execution</em>, that is, it can execute those repeated operations on multiple processors/cores on the computer, or on multiple nodes of a cluster. The construction follows the r-base looping idea, below steps are summarized:</p>
<ul>
<li>start clusters on processors cores</li>
<li>define the iterator, i.e. “i” equal to the number of elements that are going to be looped</li>
<li><code>.packages</code>: Inherits the packages that are used in the tasks define below</li>
<li><code>.combine</code>: Define the combining function that bind results at the end (say cbind, rbind or tidyverse::bind_rows).</li>
<li><code>.errorhandling</code>: specifies how a task evaluation error should be handle.</li>
<li><code>%dopar%</code>: the dopar keyword suggests foreach with parallelization method</li>
<li>then the function within the elements are iterated</li>
<li>close clusters</li>
</ul>
<p>One major concern regards that functions inside the %dopar% should be standalone in order to be executed in parallel. For standalone it is meant that everything that is needed to be executed and to output results should be defined inside the %dopar%, as it would be opened a new empty environment for each iteration. Moreover as a further consequence packages imported into each clusters, the .packages methods takes care of that.</p>
<div class="figure">
<img src="images/run_timeforeach.png" alt="" />
<p class="caption">computational complexity analysis with Furrr</p>
</div>
<p>It can be grasped quite easily that the curve now is flattened and a confident guess migh be logarithmic time <span class="math inline">\(\mathcal{O}(log(n))\)</span>.</p>
<p>A further performance improvement could be obtained using a new package called <code>doAzureParallel</code> which is built on top of the foreach. doAzureParallel enables different Virtual Machines operating parallel computing throughout Microsoft Azure cloud, but this comes at a substantial monetary cost. This would be a perfect match given that parallel methods seen before accelerates the number of requests sent among different processors or cluster, even though actually the goal is to have something that separates different sessions. Unleashing Virtual Machines allows from one hand to further increase computational capabilties, so the number of potential requests, from the other it can partition requests among different proper machines (a pool of agents for each VM) extending even more the combination of IDs and as a consequence masquerading even better the scraping automation.</p>
</div>
<div id="challenges" class="section level2">
<h2><span class="header-section-number">2.5</span> Open Challenges</h2>
<p>The main challenge remains unsolved since each single elements have been finely optimized but scraping function and so REST API must be continuously maintained. Indeed What it can not be a-priori optimized are the future changes that involves the website structure. Content architecture as opposite, with some sophistication can take care of finding exact information within the webpage even if the designed is changed. The idea developed in the package <span class="citation">Khalil (<a href="#ref-Rcrawler" role="doc-biblioref">2018</a>)</span>, even though results are not always acceptable, is to crawl the website and to search for targeted keywords. Once keywords are found the scraping algorithm looks for the related information that should be located throughout the html files locally stored. html are known to be very lightweight so computation of this kind is not bothering run time. For this reasons performances with algorithm of this species are very neat but results, as anticipated, are under the expectation. As a solution an accurate text mining approach can be considered as a further enachement. It should be also pointed out that <span class="citation">Khalil (<a href="#ref-Rcrawler" role="doc-biblioref">2018</a>)</span> is designed to scrape a vast number of websites, as contrary the scraping functions here presented are exclusively designed to be applied on immobliare.it, even though they can be extended to other related website with no effort.
The way the scraping function are designed really facilitates responsive fast debugging but this can not be by any means automatized. The API necessitates frequently to resort to continuous integration (i.e. CI) review to verify the working status. Moreover Error messages can not really be understood sometimes even with handlers, this is due to functions that are called within a parallel beckend that does not allow to print error on console as in this <a href="https://stackoverflow.com/questions/10903787/how-can-i-print-when-using-dopar">stackoverflow reproducible example</a>. So each time an error occurs the “main” functions needs to be taken out of from the parallel back end and separately evaluated. This is time consuming but for the time being no solutions have been found.</p>
</div>
<div id="legal-profiles-ancora-non-validato" class="section level2">
<h2><span class="header-section-number">2.6</span> Legal Profiles (ancora non validato)</h2>
<p>“Data that is online and public is always available for all” is never a good answer to the question “Can I use those web data to my scope”. <a href="https://www.immobiliare.it/">Immobiliare.it</a> is not providing any open source data from its own database neither the perception is that it is planning to do so in the future. Immobiliare has not even provided a paid API through which data might be accessed.
A careful reading of their terms, reviewed with a intellectual property expert, has been done to get this service running without any legal consequence, as a reference the full policy can be seen in their <a href="https://www.immobiliare.it/terms/">specialized section</a>. Nevertheless the golden standard for scraping was respected since the robottxt is neat allowing any actions as demonstrated above. So if it might be the case of misinterpretation of their policy, it will be also the case of lack of communication between servers response and immobiliare.it intent to preserve their own intellectual property.
What it was shockingly surprising are the low barriers to obtain information with respect to other counterpart online players. Best practices are applied and delayed requests (even though not asked) have been sent to normalize traffic congestion. But scraping criteria followed are once again fully based on common shared best practises (see section <a href="scraping.html#best-practices">2.2</a>), and <em>not</em> any sort of general agreements between parties. As a result a plausible approach could be applying scraping procedures without any prevention. It would not surely cause any sort of disservice for the website since budjet constraints are set low, but in the long run it will cause lagging as soon as budjet or subjects will increase. Totally different was the approach proposed by Idealista.com, which is a comparable to immobiliare.it. Idealista does block requests if they are not in compliance with their servers inner rules. User agents in this case must be rotated quite frequently and as soon as a request does not fall within the pool of user agents (i.e. is labled as web bot) it is immediately blocked and 404 response is sent back. Delay is kindly asked and it must be specified, consequnetly this slows down scraping function per se.</p>
<ul>
<li>Idealista content is composed by Javascript so and html parser can no get that.</li>
<li>Idealista blocks also certain web browser that have a demonstrated “career” in scraping procedures.</li>
</ul>
<p>All of this leads to accept that entry barriers to scrape are for sure higher than the one faced for Immobiliare. The reticence to share data could be a reflex on how big idealista is; as a matter of fact it has a heavy market presence in some of the Europe real estate country as Spain and France. So the hidden intention was to raise awareness on scraping procedure that in a certain remote way can hurt their business. This has been validated by the fact that prior filtering houses on their website a checkbox has to be signed. The checkbox make the user sign an agreement on their platform according to which data can not be misused and it belongs their intellectual property.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-densmore_2019">
<p>Densmore, James. 2019. “Ethics in Web Scraping.” <em>Medium</em>. Towards Data Science. <a href="https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01">https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01</a>.</p>
</div>
<div id="ref-Graph_Diestel">
<p>Diestel, Reinhard. 2006. <em>Graph Theory</em>. 3rd ed. Graduate Texts in Mathematics. Springer.</p>
</div>
<div id="ref-Rcrawler">
<p>Khalil, Salim. 2018. <em>Rcrawler: Web Crawler and Scraper</em>. <a href="https://CRAN.R-project.org/package=Rcrawler">https://CRAN.R-project.org/package=Rcrawler</a>.</p>
</div>
<div id="ref-robotstxt">
<p>Meissner, Peter, and Kun Ren. 2020. <em>Robotstxt: A ’Robots.txt’ Parser and ’Webbot’/’Spider’/’Crawler’ Permissions Checker</em>. <a href="https://CRAN.R-project.org/package=robotstxt">https://CRAN.R-project.org/package=robotstxt</a>.</p>
</div>
<div id="ref-foreach">
<p>Microsoft, and Steve Weston. 2020. <em>Foreach: Provides Foreach Looping Construct</em>. <a href="https://CRAN.R-project.org/package=foreach">https://CRAN.R-project.org/package=foreach</a>.</p>
</div>
<div id="ref-polite">
<p>Perepolkin, Dmytro. 2019. <em>Polite: Be Nice on the Web</em>. <a href="https://CRAN.R-project.org/package=polite">https://CRAN.R-project.org/package=polite</a>.</p>
</div>
<div id="ref-ratelimitr">
<p>Shah, Tarak. 2018. <em>Ratelimitr: Rate Limiting for R</em>. <a href="https://CRAN.R-project.org/package=ratelimitr">https://CRAN.R-project.org/package=ratelimitr</a>.</p>
</div>
<div id="ref-furrr">
<p>Vaughan, Davis, and Matt Dancho. 2018. <em>Furrr: Apply Mapping Functions in Parallel Using Futures</em>. <a href="https://CRAN.R-project.org/package=furrr">https://CRAN.R-project.org/package=furrr</a>.</p>
</div>
<div id="ref-whoishostingthis.com">
<p>WhoIsHostingThis.com. 2020. “User Agent: Learn Your Web Browsers User Agent Now.” <em>WhoIsHostingThis.com</em>. <a href="https://www.whoishostingthis.com/tools/user-agent/">https://www.whoishostingthis.com/tools/user-agent/</a>.</p>
</div>
<div id="ref-rvest">
<p>Wickham, Hadley. 2019. <em>Rvest: Easily Harvest (Scrape) Web Pages</em>. <a href="https://CRAN.R-project.org/package=rvest">https://CRAN.R-project.org/package=rvest</a>.</p>
</div>
<div id="ref-memoise">
<p>Wickham, Hadley, Jim Hester, Kirill Müller, and Daniel Cook. 2017. <em>Memoise: Memoisation of Functions</em>. <a href="https://CRAN.R-project.org/package=memoise">https://CRAN.R-project.org/package=memoise</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Infrastructure.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/Thesis/edit/master/02-scraping.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf", "Niccolo_Salvini_Thesis.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
