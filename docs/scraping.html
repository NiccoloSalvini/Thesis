<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Scraping | REST API for Real Estate rental data, a spatial Bayesian modeling approach with INLA.</title>
  <meta name="description" content="Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Scraping | REST API for Real Estate rental data, a spatial Bayesian modeling approach with INLA." />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />
  <meta property="og:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Scraping | REST API for Real Estate rental data, a spatial Bayesian modeling approach with INLA." />
  
  <meta name="twitter:description" content="Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="intro.html"/>
<link rel="next" href="algorithm-1.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"> REST API for Real Estate rental data, a spatial Bayesian modeling approach with INLA.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#web-scraping-and-main-challenges"><i class="fa fa-check"></i><b>2.1</b> Web Scraping and main Challenges</a><ul>
<li class="chapter" data-level="2.1.1" data-path="scraping.html"><a href="scraping.html#webstructure"><i class="fa fa-check"></i><b>2.1.1</b> Immobiliare.it website structure</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="algorithm-1.html"><a href="algorithm-1.html"><i class="fa fa-check"></i><b>3</b> Algorithm 1</a><ul>
<li class="chapter" data-level="3.0.1" data-path="algorithm-1.html"><a href="algorithm-1.html#immobiliare.it-content-architecture-with-rvest"><i class="fa fa-check"></i><b>3.0.1</b> Immobiliare.it content architecture with <code id="ContentArchitecture">rvest</code></a></li>
<li class="chapter" data-level="3.1" data-path="algorithm-1.html"><a href="algorithm-1.html#best-practices"><i class="fa fa-check"></i><b>3.1</b> Scraping Best Practices and Security provisions</a></li>
<li class="chapter" data-level="3.2" data-path="algorithm-1.html"><a href="algorithm-1.html#security-provisions-user-agents-proxies-and-handlers"><i class="fa fa-check"></i><b>3.2</b> Security provisions: User Agents, Proxies and Handlers</a><ul>
<li class="chapter" data-level="3.2.1" data-path="algorithm-1.html"><a href="algorithm-1.html#spoofing"><i class="fa fa-check"></i><b>3.2.1</b> User Agents Spoofing</a></li>
<li class="chapter" data-level="3.2.2" data-path="algorithm-1.html"><a href="algorithm-1.html#handlers"><i class="fa fa-check"></i><b>3.2.2</b> Handlers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="algorithm-1.html"><a href="algorithm-1.html#explicit-parallel-scraping"><i class="fa fa-check"></i><b>3.3</b> Explicit Parallel Scraping</a></li>
<li class="chapter" data-level="3.4" data-path="algorithm-1.html"><a href="algorithm-1.html#challenges"><i class="fa fa-check"></i><b>3.4</b> Open Challenges and Further Improvemements</a></li>
<li class="chapter" data-level="3.5" data-path="algorithm-1.html"><a href="algorithm-1.html#legal-profiles-ancora-non-validato"><i class="fa fa-check"></i><b>3.5</b> Legal Profiles (ancora non validato)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>4</b> REST API Infrastructure</a><ul>
<li class="chapter" data-level="4.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>4.1</b> Scheduler</a><ul>
<li class="chapter" data-level="4.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>4.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="Infrastructure.html"><a href="Infrastructure.html#rest-api"><i class="fa fa-check"></i><b>4.2</b> REST API</a><ul>
<li class="chapter" data-level="4.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>4.2.1</b> Plumber REST API</a></li>
<li class="chapter" data-level="4.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#immobiliare.it-parallel-rest-api"><i class="fa fa-check"></i><b>4.2.2</b> Immobiliare.it <em>Parallel</em> REST API</a></li>
<li class="chapter" data-level="4.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#APIdocs"><i class="fa fa-check"></i><b>4.2.3</b> REST API documentation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>4.3</b> Docker</a><ul>
<li class="chapter" data-level="4.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#why-docker"><i class="fa fa-check"></i><b>4.3.1</b> Why Docker</a></li>
<li class="chapter" data-level="4.3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>4.3.2</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>4.4</b> AWS EC2 server</a><ul>
<li class="chapter" data-level="4.4.1" data-path="Infrastructure.html"><a href="Infrastructure.html#launch-an-ec2-instance"><i class="fa fa-check"></i><b>4.4.1</b> Launch an EC2 instance</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>4.5</b> NGINX reverse proxy server</a></li>
<li class="chapter" data-level="4.6" data-path="Infrastructure.html"><a href="Infrastructure.html#further-integrations"><i class="fa fa-check"></i><b>4.6</b> Further Integrations</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inla.html"><a href="inla.html"><i class="fa fa-check"></i><b>5</b> INLA computation</a><ul>
<li class="chapter" data-level="5.1" data-path="inla.html"><a href="inla.html#LGM"><i class="fa fa-check"></i><b>5.1</b> Latent Gaussian Models LGM</a></li>
<li class="chapter" data-level="5.2" data-path="inla.html"><a href="inla.html#approx"><i class="fa fa-check"></i><b>5.2</b> Approximation in INLA setting</a><ul>
<li class="chapter" data-level="5.2.1" data-path="inla.html"><a href="inla.html#further-approximations-prolly-do-not-note-include"><i class="fa fa-check"></i><b>5.2.1</b> further approximations (prolly do not note include)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inla.html"><a href="inla.html#rinla"><i class="fa fa-check"></i><b>5.3</b> R-INLA package in a bayesian hierarchical regression perspective</a><ul>
<li class="chapter" data-level="5.3.1" data-path="inla.html"><a href="inla.html#overview"><i class="fa fa-check"></i><b>5.3.1</b> Overview</a></li>
<li class="chapter" data-level="5.3.2" data-path="inla.html"><a href="inla.html#example"><i class="fa fa-check"></i><b>5.3.2</b> Linear Predictor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>6</b> Point Referenced Data Modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>6.1</b> Gaussian Process (GP)</a></li>
<li class="chapter" data-level="6.2" data-path="prdm.html"><a href="prdm.html#spatial-covariance-function"><i class="fa fa-check"></i><b>6.2</b> Spatial Covariance Function</a><ul>
<li class="chapter" data-level="6.2.1" data-path="prdm.html"><a href="prdm.html#Matern"><i class="fa fa-check"></i><b>6.2.1</b> Matérn Covariance Function</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="prdm.html"><a href="prdm.html#hedonic-models-literature-review-and-spatial-hedonic-price-models"><i class="fa fa-check"></i><b>6.3</b> Hedonic models Literature Review and Spatial Hedonic Price Models</a></li>
<li class="chapter" data-level="6.4" data-path="prdm.html"><a href="prdm.html#univariateregr"><i class="fa fa-check"></i><b>6.4</b> Point Referenced Regression for univariate spatial data</a></li>
<li class="chapter" data-level="6.5" data-path="prdm.html"><a href="prdm.html#hiermod"><i class="fa fa-check"></i><b>6.5</b> Hierarchical Bayesian models</a></li>
<li class="chapter" data-level="6.6" data-path="prdm.html"><a href="prdm.html#finalregr"><i class="fa fa-check"></i><b>6.6</b> INLA model through spatial hierarchical regression</a></li>
<li class="chapter" data-level="6.7" data-path="prdm.html"><a href="prdm.html#spatial-kriging"><i class="fa fa-check"></i><b>6.7</b> Spatial Kriging</a></li>
<li class="chapter" data-level="6.8" data-path="prdm.html"><a href="prdm.html#model-checking"><i class="fa fa-check"></i><b>6.8</b> Model Checking</a></li>
<li class="chapter" data-level="6.9" data-path="prdm.html"><a href="prdm.html#prior-specification"><i class="fa fa-check"></i><b>6.9</b> Prior Specification</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="spde.html"><a href="spde.html"><i class="fa fa-check"></i><b>7</b> SPDE approach</a><ul>
<li class="chapter" data-level="7.1" data-path="spde.html"><a href="spde.html#set-spde-problem"><i class="fa fa-check"></i><b>7.1</b> Set SPDE Problem</a></li>
<li class="chapter" data-level="7.2" data-path="spde.html"><a href="spde.html#spde-within-r-inla"><i class="fa fa-check"></i><b>7.2</b> SPDE within R-INLA</a></li>
<li class="chapter" data-level="7.3" data-path="spde.html"><a href="spde.html#first-point-krainsky-rubio-too-technical"><i class="fa fa-check"></i><b>7.3</b> First Point Krainsky Rubio TOO TECHNICAL</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>8</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="8.1" data-path="exploratory.html"><a href="exploratory.html#prep"><i class="fa fa-check"></i><b>8.1</b> Data preparation</a><ul>
<li class="chapter" data-level="8.1.1" data-path="exploratory.html"><a href="exploratory.html#maps-and-geo-visualisations"><i class="fa fa-check"></i><b>8.1.1</b> Maps and Geo-Visualisations</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="exploratory.html"><a href="exploratory.html#counts-and-first-orientations"><i class="fa fa-check"></i><b>8.2</b> Counts and First Orientations</a></li>
<li class="chapter" data-level="8.3" data-path="exploratory.html"><a href="exploratory.html#text-mining-in-estate-review"><i class="fa fa-check"></i><b>8.3</b> Text Mining in estate Review</a></li>
<li class="chapter" data-level="8.4" data-path="exploratory.html"><a href="exploratory.html#missing-assessement-and-imputation"><i class="fa fa-check"></i><b>8.4</b> Missing Assessement and Imputation</a><ul>
<li class="chapter" data-level="8.4.1" data-path="exploratory.html"><a href="exploratory.html#missing-assessement"><i class="fa fa-check"></i><b>8.4.1</b> Missing assessement</a></li>
<li class="chapter" data-level="8.4.2" data-path="exploratory.html"><a href="exploratory.html#covariates-imputation"><i class="fa fa-check"></i><b>8.4.2</b> Covariates Imputation</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="exploratory.html"><a href="exploratory.html#model-specification"><i class="fa fa-check"></i><b>8.5</b> Model Specification</a></li>
<li class="chapter" data-level="8.6" data-path="exploratory.html"><a href="exploratory.html#mesh-building"><i class="fa fa-check"></i><b>8.6</b> Mesh building</a><ul>
<li class="chapter" data-level="8.6.1" data-path="exploratory.html"><a href="exploratory.html#shinyapp-for-mesh-assessment"><i class="fa fa-check"></i><b>8.6.1</b> Shinyapp for mesh assessment</a></li>
<li class="chapter" data-level="8.6.2" data-path="exploratory.html"><a href="exploratory.html#building-spde-model-on-mesh"><i class="fa fa-check"></i><b>8.6.2</b> BUilding SPDE model on mesh</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="exploratory.html"><a href="exploratory.html#spatial-kriging-prediction"><i class="fa fa-check"></i><b>8.7</b> Spatial Kriging (Prediction)</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="modelspec.html"><a href="modelspec.html"><i class="fa fa-check"></i><b>9</b> Model Selection &amp; Fitting</a><ul>
<li class="chapter" data-level="9.1" data-path="modelspec.html"><a href="modelspec.html#model-criticism"><i class="fa fa-check"></i><b>9.1</b> Model Criticism</a></li>
<li class="chapter" data-level="9.2" data-path="modelspec.html"><a href="modelspec.html#spatial-kriging-1"><i class="fa fa-check"></i><b>9.2</b> Spatial Kriging</a></li>
<li class="chapter" data-level="9.3" data-path="modelspec.html"><a href="modelspec.html#model-checking-1"><i class="fa fa-check"></i><b>9.3</b> Model Checking</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>10</b> Shiny Application</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="10.1" data-path="appendix.html"><a href="appendix.html#gp-basics"><i class="fa fa-check"></i><b>10.1</b> GP basics</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">REST API for Real Estate rental data, a spatial Bayesian modeling approach with INLA.</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="scraping" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Scraping</h1>
<!--  You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(Scraping). Figure cross referencing follows this syntax: \@ref(fig:plot) when matched in the markdown figure insertion syntax ![a generic plot](images/a_generic_plot.png) -->
<p>The following chapter covers advanced topics for web scraping with a focus on immobiliare.it. A new simplifying scraping workflow is proposed that removes frictions related to complexities as well as offering a faster solution with respect to traditional package oriented scraping methods. The workflow introduces two superficial abstract partitions that composes each websites: website structure and content architecture. As a consequence of the partition issues related to web scraping are logically arranged so that at first are solved the ones related to crawling the links, then it comes the turn of the proper scraping part. The segmentation is then forced onto the specific immobiliare.it case.
Website structure mainly accounts the way individual subpages are linked to one another through urls. The solution proposed is a “inverse crawling” methodology, where at first url anatomy is reverse engineered and secondly links are extracted from the composed urls. Once the structure is “explicit” so that each part can be singularly and directly targeted by scraping functions, then the focus shifts to the content architecture. For content architecture it is meant the way information is arranged within each single webpage. A rooted-tree graph representation of web site content architecture is conveniently used to map scraping functions design into immobiliare.it. By means of <code>rvest</code> <span class="citation">Wickham (<a href="#ref-rvest" role="doc-biblioref">2019</a>)</span> either website structure and content architecture can be separately handled and the main function involved are presented. <code>scrapeprice()</code> outlines the strategy adopted to look for the price inside each of the web pages. The skeleton of the function is then reproduced for all the data needed. Moreover it is shown a main function where parallel execution takes place that combines all the functions into a single call. Scraping best practices are applied from the web server point of view, kindly requesting permission and delayed sending rate. Form the the web client point of view continuous scraping is granted by server blocks through User Agent pool rotation and dealing failure handlers to bypass empty content. Then a run time parallel scraping benchmark is presented for two different back ends <code>future</code> and <code>doMC</code> along with two parallel looping constructions, <code>furrr</code> <span class="citation">Vaughan and Dancho (<a href="#ref-furrr" role="doc-biblioref">2018</a>)</span> and <code>foreach</code> <span class="citation">Microsoft and Weston (<a href="#ref-foreach" role="doc-biblioref">2020</a>)</span>. Both of the two have displayed similar results, nevertheless the former offers a Tidier and future oriented choice. Furthermore an overview of open challenges is offered so that this work might be extended and integrated with newer technologies or paradigms. In the end legal profiles are addressed comparing scraping results and difficulties with a counterpart study.</p>
<div id="web-scraping-and-main-challenges" class="section level2">
<h2><span class="header-section-number">2.1</span> Web Scraping and main Challenges</h2>

<div class="definition">
<span id="def:scraping" class="definition"><strong>Definition 2.1  (Scraping)  </strong></span>Web Scraping is a technique aimed at extracting data from static or dynamic internet web pages. It can be applied simultaneously or automatically by a scheduler that plans execution at a given time.
</div>

<p>Content in web pages is the most of the times well organized and accessible.
This is made possible by the effort put into building both the <em>website structure</em> and the <em>content architecture</em>. For website structure it is meant the way urls, pointing to different web pages, are arranged throughout the website oriented to the best user experience.
Website structure constitutes a <em>first dimension</em> of hierarchy. A practical example might regard the way files are arranged into folders, where files are urls and folders are web pages.
Popular website structure examples are social-networks where posts can be scrolled down within a single url named “wall”. The scrolling option might end due to the end of the feed, but the experience is a never-ending web page associated to a single url. Indeed personal profiles have dedicated unique url and personal photos or friends are allocated into specific personal profile sub-domains. Connection among webpages are complex networks that happens at many levels, photos, friends, places.
As a different example online newspapers display their articles in the front page of their websites. and By accessing to one of them all the pertinent articles sometimes can be reached in the low bottom or in the side part of the webpage. Suggested articles during website exploration can be seen twice, that is the more the website is covered the more is the likelihood of seeing the same article twice. Recursive website structures are popular in newspaper-kind websites since it is part of the expectation on website’s user experience.
Online Retailers as Amazon, based on search filters, groups inside a single web page (i.e. page n° 1) a fixed set of items, having their dedicated single urls attached to them. Furthermore many of the retailers, Amazon’s too, allows the user to search into many pages i.e. page n° 2,3, looking for another fixed set of items. The experience ends when the last page associated to the last url is met.
These are few examples of how websites can be built and infinitely many others are available ranging from the easiest to the most complex, as in figure @ref(fig:html_tree).
Generally speaking website structures try to reflect both the user expectations on the product and the creative design expression of the web developer. This is also constrained to the programming languages chosen and the specific requirements that the website should met. For all the reason said, for each product, whether it is physical product, or service, or information there exists a multitude of website structure. For each website structure there exists multiple content architecture. For each content architecture there exists many front end languages which are ultimately designated to satisfy multiple end users. In the future chances are that websites might display tailor made customization of contents and design based on specific personal preferences. As a further addition web design in scraping plays an important role since the more are implied sophisticated graphical technologies, the harder will be scraping information.</p>
<div class="figure">
<img src="images/netstruc_vs_hierstruc.jpg" alt="" />
<p class="caption">(#fig:html_tree)Linearity in Website Structure vs Audience Education</p>
</div>
<p>A <em>second dimension</em> of hierarchy is brought by content architecture by means of the language used for content creation and organization i.e. HTML. HTML stands for Hyper Text Markup Language and is the standard <em>markup</em> language for documents designed to be showed into a web browser. It can be supported by technologies such as Cascading Style Sheets (CSS) and other scripting languages, as an example JavaScript <span class="citation">(“HTML” <a href="#ref-html_2020" role="doc-biblioref">2020</a>)</span>.
HTML inner language properties brings along the hierarchy that is then inherited from the website structure. According to this point of view the hierarchical website structure is a consequence of the language chosen for building content architecture.
Since a hierarchy structure is present a direction must be chosen, this direction is from root to leaves i.e. <em>arborescence</em>.
CSS language stands for Cascading Style Sheets and is a style sheet language used for modifying the appearance of a document written in a <em>markup</em> language<span class="citation">(“CSS” <a href="#ref-css_2020" role="doc-biblioref">2020</a>)</span>.
The combination of HTML and CSS offers a wide flexibility in building web sites, once again expressed by the vast amount of different websites designs on the web. Some websites’ components also might be tuned by a scripting language as Javascript. JavaScript enables interactive web pages and the vast majority of websites use it for all the operations that are performed by the client in a client-server relationship <span class="citation">(“JavaScript” <a href="#ref-Javascript_2020" role="doc-biblioref">2020</a>)</span>.
In the context of scraping Javascript adds a further layer of difficulty. As a matter of fact Javascript components are dynamic and scraping requires specialized libraries or remote web browser automation (<span class="citation">(Harrison <a href="#ref-RSelenium" role="doc-biblioref">2020</a>)</span> R Bindings for Selenium 2.0 Remote WebDriver) to catch the website content.
CSS instead allows the scraper to target a class of objects in the web page that shares same style (e.g. same CSS query) so that each element that belongs to the class (i.e. share the same style) can be gathered. This practice provides tremendous advantages since by a single CSS query a precise set of objects can be obtained within a unique function call.
First and Second dimension of the scraping problem imply hierarchy. One way to imagine hierarchy in both of the two dimensions are graph based data structures named as <strong>Rooted Trees</strong>. By analyzing the first dimension through the lenses of Rooted trees it is possible to compress the whole setting into tree graph jargon, as a further reference on notation and wordings can be found in <span class="citation">Diestel (<a href="#ref-Graph_Diestel" role="doc-biblioref">2006</a>)</span>. Rooted trees must start with a root node which in this context is the domain of the web page. Each <em>Node</em> is a url destination and <em>Edges</em> are the connections to web pages. Jumps from one page to the others (i.e. connections) are possible in the website by nesting urls inside webpages so that within a single webpage the user can access to a limited number of other links. Each edge is associated to a <em>Weight</em> whose interpretation is the run time cost to walk from one node to its connected others (i.e. from a url to the other). In addition the content inside each node takes the name of payload, which is ultimately the goal of the scraping processes.
The walk from node “body” to node “h2” in figure below is called path and it represented as an ordered list of nodes connected by edges. In this context each node can have both a fixed and variable outgoing sub-nodes that are called <em>Children</em> . When root trees have a fixed set of children are called <em>k-ary</em> rooted trees. A node is said to be <em>Parent</em> to other nodes when it is connected to them by outgoing edges, in the figure below “headre” is the parent of nodes “h1” and “p”. Nodes in the tree that shares the same parent node are said <em>Siblings</em>, “h1” and “p” are siblings in figure @ref(fig:html_tree). Moreover <em>Subtrees</em> are a set of nodes and edges comprised of a parent and its descendants e.g. node “main” with all of its descendants might constitute a subtree. The concept of subtree in both of the problem dimensions plays crucial role in cutting run time scraping processes as well as fake headers provision (see section <a href="algorithm-1.html#spoofing">3.2.1</a>). If the website strucuture is locally reproducible and the content architecture within webpages tends to be equal, then functions for a single subtree might be extended to the rest of others siblings subtrees. Local reproducibility is a property according to which starting from a single url all the related urls can be inferred from a pattern. Equal content architecture throughout different single links means to have a standard shared-within-webpages criteria according to which each single rental advertisement has to refer (e.g. each new advertisement replicates the structure of the existing ones). In addition two more metrics describe the tree: <em>level</em> and <em>height</em>. The level of a node <span class="math inline">\(\mathbf{L}\)</span> counts the number of edges on the path from the root node to <span class="math inline">\(\mathbf{L}\)</span> , e.g. “head” and “body”, are at the same level. The height is the maximum level for any node in the tree, from now on <span class="math inline">\(\mathbf{H}\)</span>, in figure @ref(fig:html_tree). What is worth to be anticipating is that functions are not going to be applied directly to siblings in the “upper” general rooted tree (i.e. from the domain). Instead the approach follwed is segmenting the highest tree into a sequence of single children unit that shares the same level (“nav”, “main”, “header”, “title” and “footer”) for reasons explained in section <a href="algorithm-1.html#spoofing">3.2.1</a>.</p>
<div class="figure">
<img src="images/html_general_representation.jpg" alt="" />
<p class="caption">(#fig:html_tree) html tree structure of a general website, randomly generated online</p>
</div>
<div id="webstructure" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Immobiliare.it website structure</h3>
<p>The website structure of immobiliare can be assumed to be similar to the one of the largest online retailer Amazon. For that reason they both fall into the same website structure category. Sharing the same category might imply that the transition from customized website structure scraping functions (i.e. immobiliare) do not take extraordinary sophistication to be extended to other comparable websites (i.e. Amazon). Assuming that the scraper knows where data is stored (i.e. payloads), the mandatory step is a way to compose and decompose url anatomy. As a matter of fact each time the scraper script visits the website it should not step back from domain root node and then down the longest path reaching the final content node. Instead it should try to shorten the path by minimizing the number of nodes encountered, conditioned to the respective nodes’ weights. This is a first important conclusion since by separating the website strcuture from the content architecture scraping is massively faster and should not no more rely on the website forced root-to-node paths.
immobiliare.it is a <a href="https://en.wikipedia.org/wiki/Clean_URL">clean url</a> <em>miss lit</em> and it can be easily parsed and queried according to some parameters (i.e. filters) selected in their dedicated section (e.g. city, number of rooms 5, square footage less than 60 <span class="math inline">\(m^2\)</span>, macrozone “fiera” and “centro”). The url is shaped so that each further parameters and its respcetive values are appended at the end of the domain url <code>https://www.immobiliare.it/</code>. Parameters and values are appended with a proper semantic, not all the sematics are equal, that is why scraping needs sophostication when applied to other websites. One major adavatge in this context is immobilaire being a <a href="https://en.wikipedia.org/wiki/Clean_URL">clean url</a>, whose sematic is oriented to usability and accessibility.
Once parameters are applied to the root domain this constitutes a newer rooted tree whose url root node is the parametrized.It might have this appearance (params are city of Milan, square footage is less than 60 <span class="math inline">\(m^2\)</span>: domain + filters i.e. <code>affitto-case/milano/?superficieMinima=60</code>. Since for the moment are generated only links related to page n°1 containing the first 25 advs links (see figure <a href="algorithm-1.html#fig:websitetree">3.1</a>) all the remaining siblings nodes corresponding to the subsequent pages have to be initialized. In here resides the utility of Local reproducibility property introduced in the previous section. The remaining siblings, e.g. the ones belonging to page 2 (with the attached 25 links), to page 3 etc. can be generated by adding a further parameter <code>&amp;pag=n</code>, where n is the page number reference (from now on referred as <em>pagination</em>). Author customary choice is to stop pagination up to 300 pages since spatial data can not be to too large due to computational requirements imposed by inla methodology <a href="inla.html#inla">5</a>. The code chunk below has the aim to mimic the url syntax filters building, s given a set of information it can reproduce any related sibling. detaching website structure from content architecture.</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-css_2020">
<p>“CSS.” 2020. <em>Wikipedia</em>. Wikimedia Foundation. <a href="https://en.wikipedia.org/wiki/CSS">https://en.wikipedia.org/wiki/CSS</a>.</p>
</div>
<div id="ref-Graph_Diestel">
<p>Diestel, Reinhard. 2006. <em>Graph Theory</em>. 3rd ed. Graduate Texts in Mathematics. Springer.</p>
</div>
<div id="ref-RSelenium">
<p>Harrison, John. 2020. <em>RSelenium: R Bindings for ’Selenium Webdriver’</em>. <a href="https://CRAN.R-project.org/package=RSelenium">https://CRAN.R-project.org/package=RSelenium</a>.</p>
</div>
<div id="ref-html_2020">
<p>“HTML.” 2020. <em>Wikipedia</em>. Wikimedia Foundation. <a href="https://en.wikipedia.org/wiki/HTML">https://en.wikipedia.org/wiki/HTML</a>.</p>
</div>
<div id="ref-Javascript_2020">
<p>“JavaScript.” 2020. <em>Wikipedia</em>. Wikimedia Foundation. <a href="https://en.wikipedia.org/wiki/JavaScript">https://en.wikipedia.org/wiki/JavaScript</a>.</p>
</div>
<div id="ref-foreach">
<p>Microsoft, and Steve Weston. 2020. <em>Foreach: Provides Foreach Looping Construct</em>. <a href="https://CRAN.R-project.org/package=foreach">https://CRAN.R-project.org/package=foreach</a>.</p>
</div>
<div id="ref-furrr">
<p>Vaughan, Davis, and Matt Dancho. 2018. <em>Furrr: Apply Mapping Functions in Parallel Using Futures</em>. <a href="https://CRAN.R-project.org/package=furrr">https://CRAN.R-project.org/package=furrr</a>.</p>
</div>
<div id="ref-rvest">
<p>Wickham, Hadley. 2019. <em>Rvest: Easily Harvest (Scrape) Web Pages</em>. <a href="https://CRAN.R-project.org/package=rvest">https://CRAN.R-project.org/package=rvest</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="algorithm-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/thesis/edit/master/02-scraping.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
