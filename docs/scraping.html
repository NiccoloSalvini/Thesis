<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Web Scraping | REST Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</title>
  <meta name="description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Web Scraping | REST Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />
  <meta property="og:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Web Scraping | REST Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  
  <meta name="twitter:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />

<meta name="author" content="Niccolò Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/logo/spatial_logo.png" />
  <link rel="shortcut icon" href="images/logo/spatial_logo.ico" type="image/x-icon" />
<link rel="prev" href="intro.html"/>
<link rel="next" href="Infrastructure.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/Proj4Leaflet-1.0.1/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.3/leaflet.js"></script>
<link href="libs/leaflet-minimap-3.3.1/Control.MiniMap.min.css" rel="stylesheet" />
<script src="libs/leaflet-minimap-3.3.1/Control.MiniMap.min.js"></script>
<script src="libs/leaflet-minimap-3.3.1/Minimap-binding.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/logo/spatial_logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Web Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#a-gentle-introduction-on-web-scraping"><i class="fa fa-check"></i><b>2.1</b> A Gentle Introduction on Web Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#inverse-url-compostion"><i class="fa fa-check"></i><b>2.2</b> Inverse url compostion</a></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#graph-representation-of-html"><i class="fa fa-check"></i><b>2.3</b> Graph Representation of HTML</a></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#crawling"><i class="fa fa-check"></i><b>2.4</b> Crawling</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#proper-scraping"><i class="fa fa-check"></i><b>2.5</b> Proper Scraping</a><ul>
<li class="chapter" data-level="2.5.1" data-path="scraping.html"><a href="scraping.html#webstructure"><i class="fa fa-check"></i><b>2.5.1</b> Immobiliare.it website structure</a></li>
<li class="chapter" data-level="2.5.2" data-path="scraping.html"><a href="scraping.html#immobiliare.it-content-architecture-with-rvest"><i class="fa fa-check"></i><b>2.5.2</b> Immobiliare.it content architecture with <code id="ContentArchitecture">rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#ProperScraping"><i class="fa fa-check"></i><b>2.6</b> Proper Scarping</a></li>
<li class="chapter" data-level="2.7" data-path="scraping.html"><a href="scraping.html#best-practices"><i class="fa fa-check"></i><b>2.7</b> Scraping Best Practices and Security provisions</a></li>
<li class="chapter" data-level="2.8" data-path="scraping.html"><a href="scraping.html#web-client-security-provisions-user-agents-proxies-and-fail-dealers"><i class="fa fa-check"></i><b>2.8</b> Web Client Security provisions: User Agents, Proxies and Fail Dealers</a><ul>
<li class="chapter" data-level="2.8.1" data-path="scraping.html"><a href="scraping.html#spoofing"><i class="fa fa-check"></i><b>2.8.1</b> HTTP User Agent and Mail Spoofing</a></li>
<li class="chapter" data-level="2.8.2" data-path="scraping.html"><a href="scraping.html#possibly"><i class="fa fa-check"></i><b>2.8.2</b> Dealing with failure</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="scraping.html"><a href="scraping.html#parallelscraping"><i class="fa fa-check"></i><b>2.9</b> Parallel Scraping</a><ul>
<li class="chapter" data-level="2.9.1" data-path="scraping.html"><a href="scraping.html#parallel-furrrfuture"><i class="fa fa-check"></i><b>2.9.1</b> Parallel furrr+future</a></li>
<li class="chapter" data-level="2.9.2" data-path="scraping.html"><a href="scraping.html#parallel-foreachdofuture"><i class="fa fa-check"></i><b>2.9.2</b> Parallel foreach+doFuture</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="scraping.html"><a href="scraping.html#challenges"><i class="fa fa-check"></i><b>2.10</b> Open Challenges and Further Improvemements</a></li>
<li class="chapter" data-level="2.11" data-path="scraping.html"><a href="scraping.html#legal-profiles"><i class="fa fa-check"></i><b>2.11</b> Legal Profiles</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> API Technology Stack</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#rest-api"><i class="fa fa-check"></i><b>3.2</b> REST API</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>3.2.1</b> Plumber REST API</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#immobiliare.it-parallel-rest-api"><i class="fa fa-check"></i><b>3.2.2</b> Immobiliare.it <em>Parallel</em> REST API</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#APIdocs"><i class="fa fa-check"></i><b>3.2.3</b> REST API documentation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>3.3</b> Docker</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#why-docker"><i class="fa fa-check"></i><b>3.3.1</b> Why Docker</a></li>
<li class="chapter" data-level="3.3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.3.2</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>3.4</b> AWS EC2 instance</a><ul>
<li class="chapter" data-level="3.4.1" data-path="Infrastructure.html"><a href="Infrastructure.html#launch-an-ec2-instance"><i class="fa fa-check"></i><b>3.4.1</b> Launch an EC2 instance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>3.5</b> NGINX reverse proxy server</a></li>
<li class="chapter" data-level="3.6" data-path="Infrastructure.html"><a href="Infrastructure.html#software-development-workflow"><i class="fa fa-check"></i><b>3.6</b> Software development Workflow</a></li>
<li class="chapter" data-level="3.7" data-path="Infrastructure.html"><a href="Infrastructure.html#further-integrations"><i class="fa fa-check"></i><b>3.7</b> Further Integrations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inla.html"><a href="inla.html"><i class="fa fa-check"></i><b>4</b> INLA computation</a><ul>
<li class="chapter" data-level="4.1" data-path="inla.html"><a href="inla.html#LGM"><i class="fa fa-check"></i><b>4.1</b> Latent Gaussian Models LGM</a></li>
<li class="chapter" data-level="4.2" data-path="inla.html"><a href="inla.html#approx"><i class="fa fa-check"></i><b>4.2</b> Approximation in INLA setting</a><ul>
<li class="chapter" data-level="4.2.1" data-path="inla.html"><a href="inla.html#further-approximations-prolly-do-not-note-include"><i class="fa fa-check"></i><b>4.2.1</b> further approximations (prolly do not note include)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inla.html"><a href="inla.html#rinla"><i class="fa fa-check"></i><b>4.3</b> R-INLA package in a bayesian hierarchical regression perspective</a><ul>
<li class="chapter" data-level="4.3.1" data-path="inla.html"><a href="inla.html#overview"><i class="fa fa-check"></i><b>4.3.1</b> Overview</a></li>
<li class="chapter" data-level="4.3.2" data-path="inla.html"><a href="inla.html#example"><i class="fa fa-check"></i><b>4.3.2</b> Linear Predictor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>5</b> Point Referenced Data Modeling</a><ul>
<li class="chapter" data-level="5.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>5.1</b> Gaussian Process (GP)</a></li>
<li class="chapter" data-level="5.2" data-path="prdm.html"><a href="prdm.html#spatial-covariance-function"><i class="fa fa-check"></i><b>5.2</b> Spatial Covariance Function</a><ul>
<li class="chapter" data-level="5.2.1" data-path="prdm.html"><a href="prdm.html#Matern"><i class="fa fa-check"></i><b>5.2.1</b> Matérn Covariance Function</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="prdm.html"><a href="prdm.html#hedonic-models-literature-review-and-spatial-hedonic-price-models"><i class="fa fa-check"></i><b>5.3</b> Hedonic models Literature Review and Spatial Hedonic Price Models</a></li>
<li class="chapter" data-level="5.4" data-path="prdm.html"><a href="prdm.html#univariateregr"><i class="fa fa-check"></i><b>5.4</b> Point Referenced Regression for univariate spatial data</a></li>
<li class="chapter" data-level="5.5" data-path="prdm.html"><a href="prdm.html#hiermod"><i class="fa fa-check"></i><b>5.5</b> Hierarchical Bayesian models</a></li>
<li class="chapter" data-level="5.6" data-path="prdm.html"><a href="prdm.html#finalregr"><i class="fa fa-check"></i><b>5.6</b> INLA model through spatial hierarchical regression</a></li>
<li class="chapter" data-level="5.7" data-path="prdm.html"><a href="prdm.html#spatial-kriging"><i class="fa fa-check"></i><b>5.7</b> Spatial Kriging</a></li>
<li class="chapter" data-level="5.8" data-path="prdm.html"><a href="prdm.html#model-checking"><i class="fa fa-check"></i><b>5.8</b> Model Checking</a></li>
<li class="chapter" data-level="5.9" data-path="prdm.html"><a href="prdm.html#prior-specification"><i class="fa fa-check"></i><b>5.9</b> Prior Specification</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="spde.html"><a href="spde.html"><i class="fa fa-check"></i><b>6</b> SPDE approach</a><ul>
<li class="chapter" data-level="6.1" data-path="spde.html"><a href="spde.html#set-spde-problem"><i class="fa fa-check"></i><b>6.1</b> Set SPDE Problem</a></li>
<li class="chapter" data-level="6.2" data-path="spde.html"><a href="spde.html#spde-within-r-inla"><i class="fa fa-check"></i><b>6.2</b> SPDE within R-INLA</a></li>
<li class="chapter" data-level="6.3" data-path="spde.html"><a href="spde.html#first-point-krainsky-rubio-too-technical"><i class="fa fa-check"></i><b>6.3</b> First Point Krainsky Rubio TOO TECHNICAL</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>7</b> Exploratory Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="exploratory.html"><a href="exploratory.html#prep"><i class="fa fa-check"></i><b>7.1</b> Data preparation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="exploratory.html"><a href="exploratory.html#maps-and-geo-visualisations"><i class="fa fa-check"></i><b>7.1.1</b> Maps and Geo-Visualisations</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="exploratory.html"><a href="exploratory.html#counts-and-first-orientations"><i class="fa fa-check"></i><b>7.2</b> Counts and First Orientations</a></li>
<li class="chapter" data-level="7.3" data-path="exploratory.html"><a href="exploratory.html#text-mining-in-estate-review"><i class="fa fa-check"></i><b>7.3</b> Text Mining in estate Review</a></li>
<li class="chapter" data-level="7.4" data-path="exploratory.html"><a href="exploratory.html#missing-assessement-and-imputation"><i class="fa fa-check"></i><b>7.4</b> Missing Assessement and Imputation</a><ul>
<li class="chapter" data-level="7.4.1" data-path="exploratory.html"><a href="exploratory.html#missing-assessement"><i class="fa fa-check"></i><b>7.4.1</b> Missing assessement</a></li>
<li class="chapter" data-level="7.4.2" data-path="exploratory.html"><a href="exploratory.html#covariates-imputation"><i class="fa fa-check"></i><b>7.4.2</b> Covariates Imputation</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="exploratory.html"><a href="exploratory.html#model-specification"><i class="fa fa-check"></i><b>7.5</b> Model Specification</a></li>
<li class="chapter" data-level="7.6" data-path="exploratory.html"><a href="exploratory.html#mesh-building"><i class="fa fa-check"></i><b>7.6</b> Mesh building</a><ul>
<li class="chapter" data-level="7.6.1" data-path="exploratory.html"><a href="exploratory.html#shinyapp-for-mesh-assessment"><i class="fa fa-check"></i><b>7.6.1</b> Shinyapp for mesh assessment</a></li>
<li class="chapter" data-level="7.6.2" data-path="exploratory.html"><a href="exploratory.html#building-spde-model-on-mesh"><i class="fa fa-check"></i><b>7.6.2</b> BUilding SPDE model on mesh</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="exploratory.html"><a href="exploratory.html#spatial-kriging-prediction"><i class="fa fa-check"></i><b>7.7</b> Spatial Kriging (Prediction)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modelspec.html"><a href="modelspec.html"><i class="fa fa-check"></i><b>8</b> Model Selection &amp; Fitting</a><ul>
<li class="chapter" data-level="8.1" data-path="modelspec.html"><a href="modelspec.html#model-criticism"><i class="fa fa-check"></i><b>8.1</b> Model Criticism</a></li>
<li class="chapter" data-level="8.2" data-path="modelspec.html"><a href="modelspec.html#spatial-kriging-1"><i class="fa fa-check"></i><b>8.2</b> Spatial Kriging</a></li>
<li class="chapter" data-level="8.3" data-path="modelspec.html"><a href="modelspec.html#model-checking-1"><i class="fa fa-check"></i><b>8.3</b> Model Checking</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>9</b> Shiny Application</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="9.1" data-path="appendix.html"><a href="appendix.html#gp-basics"><i class="fa fa-check"></i><b>9.1</b> GP basics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">REST Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="scraping" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Web Scraping</h1>
<!--  You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(Scraping). Figure cross referencing follows this syntax: \@ref(fig:plot) when matched in the markdown figure insertion syntax ![a generic plot](images/a_generic_plot.png) -->
<p>The following chapter covers advanced techniques for web scraping in R and related main challenges with a focus on the immobiliare.it case. A quicker and less sophisticated <em>scraping workflow</em> is proposed that integrates url reverse engineering inside the proper scraping part, this removes frictions related to inner scraping complexities.
That means instead of crawling the entire website and then searching for keywords, it retrieves a cluster of urls in the sitemaps given some parameter arguments and then applies custom made proper scraping on this closed set. By doing that the scraper takes advantage of the url clean structure and adopts an “inverse url composition” methodology. At first url sematic is reverse engineered so that the sitemap is made explicit and urls can be freely manipulated at will. Secondly links that are children of the manipualted urlss are gathered and collected into a list. Up to this point each single link belonging to the list can be directly targeted by a number of scraping functions, with that said the focus can shift to the proper scraping part. An example of scraping function with <code>rvest</code> <span class="citation">Wickham (<a href="#ref-rvest" role="doc-biblioref">2019</a>)</span> is presented outlining the algorithm adopted to search for the content within immobiliare.it HTML/CSS. The skeleton of the algorithm is then reproduced for all the other functions that shares the same CSS query location. For all the functions that necessitate other CSS query as input nodes the search strategy is exclusively built on top of them.
Scraping common shared best practices are applied from the <em>web server</em> point of view, this is taken care by kindly asking for permission and sending delayed requests rate. As well as from the the <em>web client</em> point of view by preventing scraping discontinuity caused by server blocks through <em>User Agent</em> pool <em>rotation</em> and <em>fail dealers</em>.
<em>Parallel</em> execution is carried out since data becomes obsolete very fast, and so happens to the analysis that relied on those data. A run time Parallel scraping benchmark is presented for two different back end options <code>future</code> <span class="citation">Bengtsson (<a href="#ref-future" role="doc-biblioref">2020</a><a href="#ref-future" role="doc-biblioref">b</a>)</span> and <code>doParallel</code> <span class="citation">Corporation and Weston (<a href="#ref-doParallel" role="doc-biblioref">2020</a>)</span>, along with their respective two parallel looping constructors, <code>furrr</code> <span class="citation">Vaughan and Dancho (<a href="#ref-furrr" role="doc-biblioref">2018</a>)</span> and <code>foreach</code> <span class="citation">Microsoft and Weston (<a href="#ref-foreach" role="doc-biblioref">2020</a>)</span>. Both of the two combination have showed similar results, nevertheless the former offers has a more {Tidiverse} orientation and a more comfortable debugging experience. Furthermore an overview of the still open challenges and improvements is given with the hope that the effort put into this project might be extended or integrated. In the end legal profiles are addressed comparing results and popular legal case studies.</p>
<div id="a-gentle-introduction-on-web-scraping" class="section level2">
<h2><span class="header-section-number">2.1</span> A Gentle Introduction on Web Scraping</h2>

<div class="definition">
<span id="def:scraping" class="definition"><strong>Definition 2.1  (Scraping)  </strong></span>Web Scraping is a technique aimed at extracting unstructured data from static or dynamic internet web pages and collecting it in a structured way.
Automated data collection, web extraction, web crawling, or web data mining are often used as synonyms to web scraping.
</div>

<p>The World Wide Web (WWW or just the web") data accessible today is calculated in zettabytes (Cisco Systems, 2017 <em>miss lit</em>) (1 zettabyte = <span class="math inline">\(10^{21}\)</span> bytes). This huge volume of data provides a wealth of resources for researchers and practitioners to obtain new insights in real time regarding individuals, organizations, or even macro-level socio-technical phenomena <a href="https://www.researchgate.net/publication/328513839_Tutorial_Web_Scraping_in_the_R_Language">miss lit</a>. Unsurprisingly, researchers of Information Systems are increasingly turning to the internet for data that can address their research questions.
Taking advantage of the immense web data also requires a programmatic approach <a href="https://www.researchgate.net/publication/328513839_Tutorial_Web_Scraping_in_the_R_Language">miss lit</a> and a strong foundation in different web technologies.
Besides the large amount of web access and data to be analyzed, there are three rather common problems related to big web data: variety, velocity, and veracity (Goes,z 2014; Krotov &amp; Silva, 2018 <em>miss lit</em>), each of which exposes a singular aspect of scraping and constitutes some of the challenges fronted in later chapters.
<em>Variety</em> mainly accounts the most commonly used mark-up languages on the web used for content creation and organization such as such as HTML <span class="citation">(“HTML” <a href="#ref-html_2020" role="doc-biblioref">2020</a>)</span>, CSS <span class="citation">(“CSS” <a href="#ref-css_2020" role="doc-biblioref">2020</a>)</span>, and XML <em>miss lit</em>. Sometimes Javascript <em>miss lit</em> components are also embedded into websites, dedicated parsers are required in these contexts. Starting from that scraping requires at least to know the ropes of these technologies which are also assumed in this analysis.
<em>Velocity</em>: web data are in continuous flow status: it is created in real time, modified and changed continuously. This massively impacts the analysis that relies on those data which, as times passes, becomes obsolete. However it also suggests the speed and pace at which data should be collected. From one hand data should be gathered as quicker as possible so that analysis or softwares are up-to-date, section <a href="scraping.html#parallelscraping">2.9</a>. From the other this should happen in any case by constraining speed to common shared scraping best practices, which require occasional rests in requesting information. The latter issue is faced later in section <a href="scraping.html#best-practices">2.7</a>.
<em>Veracity</em>: Internet data quality and usability are still surrounded by confusion. A researcher can never entirely be sure if the data he wants are available on the internet and if the data are sufficiently accurate to be used in analysis. While for the former point data can be, from a theoretical perspective, accurately selected it can not be by any means predicted to exist. This is a crucial aspects of scraping, it should take care of dealing with the possibility to fail, in other words to be lost. The latter point that points to data quality is also crucial and it is assessed by a thoughtful market analysis that mostly assures the importance of immobiliare.it as a top player in italian real estate. As a consequence data coming from reliable source is also assumed to be reliable.
Furthermore web scraping can be mainly applied in two common forms as in <a href="https://www.tandfonline.com/doi/full/10.1080/10691898.2020.1787116">miss lit</a>: the first is browser scraping, where extractions takes place through HTML/XML parser with regular expression matching from the website’s source code. The second uses programming interfaces for applications, usually referred to APIs. The main goal of this chapter is to combine the two by shaping a HTML parser and make the code portable into an RESTful API whose software structure is found at <a href="#infrastructure"><strong>??</strong></a>.
Regardless of how difficult it is the process of scraping, the circle introduced in <span class="citation">(<span class="citeproc-not-found" data-reference-id="automateddata"><strong>???</strong></span>)</span> and here revised, is almost always the same. Most scraping activities include the following tasks:</p>
<ul>
<li>Identification of data</li>
<li>Algorithm search Strategy selection</li>
<li>Data collection</li>
<li>Data preprocess</li>
<li>Data conversion</li>
<li>Debugging and maintenance</li>
<li>Portability</li>
</ul>
<p>Scraping essentially is a clever combination and iteration of the previous tasks which should be heavily shaped on the target website or websites.</p>
</div>
<div id="inverse-url-compostion" class="section level2">
<h2><span class="header-section-number">2.2</span> Inverse url compostion</h2>
<ul>
<li><p>A website contains a collection of standardized HTTP requests that return JSON or XML files. . Popular options are scraping APIs, proprietary crawling softwares, browser integration, in the end open source libraries. Challenges in scraping mainly regards <em>security</em>, <em>exception handling</em> and <em>run-time</em>, therefore those will be the points touched during the dissertation.</p></li>
<li><p>RSelenium library:Docker. Open Docker Terminal and run docker pull- selenium/standalone-chrome. Replace chrome with firefox if you’re a Firefox user. Then docker run -d -p 4445:4444 selenium/standalone-chrome.If above two codes are successful, run docker-machine ip and note the IP address to be used in the R code (nrowse automation) <a href="https://www.pluralsight.com/guides/advanced-web-scraping-with-r">miss lit</a></p></li>
<li><p>To access content on the Web, we are used to typing URLs into our browser or to simply
clicking on links to get from one place to another, to check our mails, to read news, or
to download files. Behind this program layer that is designed for user interaction there are
several more layers—techniques, standards, and protocols—that make the whole thing work.
Together they are called the Internet Protocol Suite (IPS). Two of the most prominent players
of this Protocol Suite are TCP (Transmission Control Protocol) and IP (Internet Protocol).
They represent the Internet layer (IP) and the transportation layer (TCP). The inner workings
of these techniques are beyond the scope of this book, but fortunately there is no need to
manually manipulate contents of either of these protocols to conduct successful web scraping.
What is worth mentioning, however, is that TCP and IP take care of reliable data transfer
between computers in the network</p></li>
</ul>
<p>Scraping for convenience can be mainly decomposed into 2 separable collectively exhaustive tasks: and <em>Proper Scraping</em> which they need to happen in the order they are presented. As a matter of fact nearby all the scraper are also crawler since at first they neex to collect urls from sitemaps. Then they dive into the proper scraping where data is extracted beased on those urls.
The previous approach in the analysis context would be time consuming and inappropriate since data should be</p>
<p>As a matter of fact given the nature of scraping libraries and the programming languages used for website content creation as a first step urls need to be collected based on search parameters. Then on those urls proper scraping needs to be pplied to actually extracts data. What is commonly done among open source libraries</p>
<p>The forced aspects regards essentially the way websites are made and the language used for content creation and organization. HTML stands for Hyper Text Markup Language and is the standard <em>markup</em> language for documents designed to be showed into a web browser. It can be supported by technologies such as Cascading Style Sheets (CSS) and other scripting languages, such as JavaScript <span class="citation">(“HTML” <a href="#ref-html_2020" role="doc-biblioref">2020</a>)</span>.
CSS is a style sheet language used for modifying the appearance of a document written in a <em>markup</em> language<span class="citation">(“CSS” <a href="#ref-css_2020" role="doc-biblioref">2020</a>)</span>.
Generally speaking website try to reflect both the user expectations on the product and the creative design expression of the web developer. This is also constrained to the programming languages chosen which defines the capability to shape the website based on the specific requirements. For all the reason said, for each product to sell online, whether it is physical product or a service, there exists a multitude of website designs. For each design there exists many front-end languages which may ultimately satisfy multiple end users. A projection in a very near future may depict a scenario where websites will be displaying tailor made appearances based on personal preferences, device options etc.</p>
<!-- Since naturally the scraper has to deal with those languages scraping should be starting from understanding them.  -->
<!-- Some examples of websites might be social-networks where posts are scrolled down within a single url named "wall". The scrolling option might end due to the end of the feed, but the experience is a never-ending web page associated to a single url. Indeed personal profiles have dedicated unique url and personal photos or friends are allocated into specific personal profile sub-domains. Connection among webpages are complex networks that happens at many levels, photos, friends, places. -->
<!-- As a different example online newspapers display their articles in the front page of their websites. and By accessing to one of them all the pertinent articles sometimes can be reached in the low bottom or in the side part of the webpage. Suggested articles during website exploration can be seen twice, that is the more the website is covered the more is the likelihood of seeing the same article twice. Recursive website structures are popular in newspaper-kind websites since it is part of the expectation on website's user experience. -->
<!-- Online Retailers as Amazon, based on search filters, groups inside a single web page (i.e. page n° 1) a fixed set of items, having their dedicated single urls attached to them. Furthermore many of the retailers, Amazon's too, allows the user to search into many pages i.e. page n° 2,3, looking for another fixed set of items. The experience ends when the last page associated to the last url is met. -->
<!-- These are few examples of how websites can be built and infinitely many others are available ranging from the easiest to the most complex, as in figure \@ref(fig:html_tree).  -->
<!-- Generally speaking website structures try to reflect both the user expectations on the product and the creative design expression of the web developer. This is also constrained to the programming languages chosen and the specific requirements that the website should met. For all the reason said, for each product to sell, whether it is physical product, or a service there exists a multitude of website structure. For each website structure there exists multiple content architecture. For each content architecture there exists many front end languages which are ultimately designated to satisfy multiple end users. In the future chances are that websites might display tailor made customization of contents and design based on specific personal preferences. As a further addition web design in scraping plays an important role since the more are implied sophisticated graphical technologies, the harder will be scraping information.  -->
<p>As a further addition web design in scraping plays an important role since the more are implied sophisticated graphical technologies, the harder will be scraping information.</p>
</div>
<div id="graph-representation-of-html" class="section level2">
<h2><span class="header-section-number">2.3</span> Graph Representation of HTML</h2>
<p>Graph based data structures named as <strong>Rooted Trees</strong>. By analyzing the first dimension through the lenses of Rooted trees it is possible to compress the whole setting into tree graph jargon, as a further reference on notation and wordings can be found in <span class="citation">Diestel (<a href="#ref-Graph_Diestel" role="doc-biblioref">2006</a>)</span>. Rooted trees must start with a root node which in this context is the domain of the web page. Each <em>Node</em> is a url destination and <em>Edges</em> are the connections to web pages. Jumps from one page to the others (i.e. connections) are possible in the website by nesting urls inside webpages so that within a single webpage the user can access to a limited number of other links. Each edge is associated to a <em>Weight</em> whose interpretation is the run time cost to walk from one node to its connected others (i.e. from a url to the other). In addition the content inside each node takes the name of payload, which is ultimately the goal of the scraping processes.
The walk from node “body” to node “h2” in figure below is called path and it represented as an ordered list of nodes connected by edges. In this context each node can have both a fixed and variable outgoing sub-nodes that are called <em>Children</em> . When root trees have a fixed set of children are called <em>k-ary</em> rooted trees. A node is said to be <em>Parent</em> to other nodes when it is connected to them by outgoing edges, in the figure below “headre” is the parent of nodes “h1” and “p”. Nodes in the tree that shares the same parent node are said <em>Siblings</em>, “h1” and “p” are siblings in figure @ref(fig:html_tree). Moreover <em>Subtrees</em> are a set of nodes and edges comprised of a parent and its descendants e.g. node “main” with all of its descendants might constitute a subtree. The concept of subtree in both of the problem dimensions plays crucial role in cutting run time scraping processes as well as fake headers provision (see section <a href="scraping.html#spoofing">2.8.1</a>). If the website strucuture is locally reproducible and the content architecture within webpages tends to be equal, then functions for a single subtree might be extended to the rest of others siblings subtrees. Local reproducibility is a property according to which starting from a single url all the related urls can be inferred from a pattern. Equal content architecture throughout different single links means to have a standard shared-within-webpages criteria according to which each single rental advertisement has to refer (e.g. each new advertisement replicates the structure of the existing ones). In addition two more metrics describe the tree: <em>level</em> and <em>height</em>. The level of a node <span class="math inline">\(\mathbf{L}\)</span> counts the number of edges on the path from the root node to <span class="math inline">\(\mathbf{L}\)</span> , e.g. “head” and “body”, are at the same level. The height is the maximum level for any node in the tree, from now on <span class="math inline">\(\mathbf{H}\)</span>, in figure @ref(fig:html_tree). What is worth to be anticipating is that functions are not going to be applied directly to siblings in the “upper” general rooted tree (i.e. from the domain). Instead the approach follwed is segmenting the highest tree into a sequence of single children unit that shares the same level (“nav”, “main”, “header”, “title” and “footer”) for reasons explained in section <a href="scraping.html#spoofing">2.8.1</a>.</p>
<p>Some websites’ components also might be tuned by a scripting language as Javascript. JavaScript enables interactive web pages and the vast majority of websites use it for all the operations that are performed by the client in a client-server relationship <span class="citation">(“JavaScript” <a href="#ref-Javascript_2020" role="doc-biblioref">2020</a>)</span>.
In the context of scraping Javascript adds a further layer of difficulty. As a matter of fact Javascript components are dynamic and scraping requires specialized libraries or remote web browser automation (<span class="citation">(Harrison <a href="#ref-RSelenium" role="doc-biblioref">2020</a>)</span> R Bindings for Selenium 2.0 Remote WebDriver) to catch the website content.</p>
</div>
<div id="crawling" class="section level2">
<h2><span class="header-section-number">2.4</span> Crawling</h2>
<ul>
<li>general idea, definition (with latex def. component)</li>
<li>urllib</li>
<li>clean url</li>
<li>representation of crawling</li>
<li></li>
</ul>
</div>
<div id="proper-scraping" class="section level2">
<h2><span class="header-section-number">2.5</span> Proper Scraping</h2>
<div class="figure">
<img src="images/netstruc_vs_hierstruc.jpg" alt="" />
<p class="caption">(#fig:html_tree)Linearity in Website Structure vs Audience Education</p>
</div>
<p>A <em>second dimension</em> of hierarchy is brought by content architecture by means of the language used for content creation and organization i.e. HTML. HTML stands for Hyper Text Markup Language and is the standard <em>markup</em> language for documents designed to be showed into a web browser. It can be supported by technologies such as Cascading Style Sheets (CSS) and other scripting languages, as an example JavaScript <span class="citation">(“HTML” <a href="#ref-html_2020" role="doc-biblioref">2020</a>)</span>.
HTML inner language properties brings along the hierarchy that is then inherited from the website structure. According to this point of view the hierarchical website structure is a consequence of the language chosen for building content architecture.
Since a hierarchy structure is present a direction must be chosen, this direction is from root to leaves i.e. <em>arborescence</em>.
CSS language stands for Cascading Style Sheets and is a style sheet language used for modifying the appearance of a document written in a <em>markup</em> language<span class="citation">(“CSS” <a href="#ref-css_2020" role="doc-biblioref">2020</a>)</span>.
The combination of HTML and CSS offers a wide flexibility in building web sites, once again expressed by the vast amount of different websites designs on the web. Some websites’ components also might be tuned by a scripting language as Javascript. JavaScript enables interactive web pages and the vast majority of websites use it for all the operations that are performed by the client in a client-server relationship <span class="citation">(“JavaScript” <a href="#ref-Javascript_2020" role="doc-biblioref">2020</a>)</span>.
In the context of scraping Javascript adds a further layer of difficulty. As a matter of fact Javascript components are dynamic and scraping requires specialized libraries or remote web browser automation (<span class="citation">(Harrison <a href="#ref-RSelenium" role="doc-biblioref">2020</a>)</span> R Bindings for Selenium 2.0 Remote WebDriver) to catch the website content.
CSS instead allows the scraper to target a class of objects in the web page that shares same style (e.g. same CSS query) so that each element that belongs to the class (i.e. share the same style) can be gathered. This practice provides tremendous advantages since by a single CSS query a precise set of objects can be obtained within a unique function call.
First and Second dimension of the scraping problem imply hierarchy. One way to imagine hierarchy in both of the two dimensions are</p>
<div class="figure">
<img src="images/html_general_representation.jpg" alt="" />
<p class="caption">(#fig:html_tree) html tree structure of a general website, randomly generated online</p>
</div>
<div id="webstructure" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Immobiliare.it website structure</h3>
<p>The website structure of immobiliare can be assumed to be similar to the one of the largest online retailer Amazon. For that reason they both fall into the same website structure category. Sharing the same category might imply that the transition from customized website structure scraping functions (i.e. immobiliare) do not take extraordinary sophistication to be extended to other comparable websites (i.e. Amazon). Assuming that the scraper knows where data is stored (i.e. payloads), the mandatory step is a way to compose and decompose url anatomy. As a matter of fact each time the scraper script visits the website it should not step back from domain root node and then down the longest path reaching the final content node. Instead it should try to shorten the path by minimizing the number of nodes encountered, conditioned to the respective nodes’ weights. This is a first important conclusion since by separating the website strcuture from the content architecture scraping is massively faster and should not no more rely on the website forced root-to-node paths.
immobiliare.it is a <a href="https://en.wikipedia.org/wiki/Clean_URL">clean url</a> <em>miss lit</em> and it can be easily parsed and queried according to some parameters (i.e. filters) selected in their dedicated section (e.g. city, number of rooms 5, square footage less than 60 <span class="math inline">\(m^2\)</span>, macrozone “fiera” and “centro”). The url is shaped so that each further parameters and its respcetive values are appended at the end of the domain url <code>https://www.immobiliare.it/</code>. Parameters and values are appended with a proper semantic, not all the sematics are equal, that is why scraping needs sophostication when applied to other websites. One major adavatge in this context is immobilaire being a <a href="https://en.wikipedia.org/wiki/Clean_URL">clean url</a>, whose sematic is oriented to usability and accessibility.
Once parameters are applied to the root domain this constitutes a newer rooted tree whose url root node is the parametrized.It might have this appearance (params are city of Milan, square footage is less than 60 <span class="math inline">\(m^2\)</span>: domain + filters i.e. <code>affitto-case/milano/?superficieMinima=60</code>. Since for the moment are generated only links related to page n°1 containing the first 25 advs links (see figure <a href="scraping.html#fig:websitetree">2.2</a>) all the remaining siblings nodes corresponding to the subsequent pages have to be initialized. In here resides the utility of Local reproducibility property introduced in the previous section. The remaining siblings, e.g. the ones belonging to page 2 (with the attached 25 links), to page 3 etc. can be generated by adding a further parameter <code>&amp;pag=n</code>, where n is the page number reference (from now on referred as <em>pagination</em>). Author customary choice is to stop pagination up to 300 pages since spatial data can not be to too large due to computational requirements imposed by inla methodology <a href="inla.html#inla">4</a>. The code chunk below has the aim to mimic the url syntax filters building, s given a set of information it can reproduce any related sibling. detaching website structure from content architecture.</p>
<p><em>pseudo code get_link</em></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="scraping.html#cb1-1"></a><span class="cf">if</span> (<span class="op">!</span><span class="kw">missing</span>(<span class="st">&quot;macrozone&quot;</span>)) {</span>
<span id="cb1-2"><a href="scraping.html#cb1-2"></a>    macrozone =<span class="st"> </span><span class="kw">tolower</span>(macrozone) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">iconv</span>(<span class="dt">to =</span> <span class="st">&quot;ASCII//TRANSLIT&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">str_trim</span>()</span>
<span id="cb1-3"><a href="scraping.html#cb1-3"></a>    idzone =<span class="st"> </span><span class="kw">list</span>()</span>
<span id="cb1-4"><a href="scraping.html#cb1-4"></a>    zone =<span class="st"> </span><span class="kw">fromJSON</span>(here<span class="op">::</span><span class="kw">here</span>(<span class="st">&quot;ALLzone.json&quot;</span>))</span>
<span id="cb1-5"><a href="scraping.html#cb1-5"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(macrozone)) {</span>
<span id="cb1-6"><a href="scraping.html#cb1-6"></a>        zone<span class="op">$</span>name =<span class="st"> </span>zone<span class="op">$</span>name <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tolower</span>()</span>
<span id="cb1-7"><a href="scraping.html#cb1-7"></a>        <span class="cf">if</span> (<span class="kw">grepl</span>(macrozone[i], zone)[<span class="dv">2</span>]) {</span>
<span id="cb1-8"><a href="scraping.html#cb1-8"></a>            pos =<span class="st"> </span><span class="kw">grepl</span>(macrozone[i], zone<span class="op">$</span>name, <span class="dt">ignore.case =</span> T)</span>
<span id="cb1-9"><a href="scraping.html#cb1-9"></a>            idzone[i] =<span class="st"> </span>zone[pos, ] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(id)</span>
<span id="cb1-10"><a href="scraping.html#cb1-10"></a>        } <span class="cf">else</span> {</span>
<span id="cb1-11"><a href="scraping.html#cb1-11"></a>            <span class="kw">stop</span>(<span class="kw">paste0</span>(<span class="st">&quot;zone:&quot;</span>, macrozone[i], <span class="st">&quot; is not recognized&quot;</span>))</span>
<span id="cb1-12"><a href="scraping.html#cb1-12"></a>        }</span>
<span id="cb1-13"><a href="scraping.html#cb1-13"></a>    }</span>
<span id="cb1-14"><a href="scraping.html#cb1-14"></a>    idzone =<span class="st"> </span>idzone <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unlist</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unique</span>()</span>
<span id="cb1-15"><a href="scraping.html#cb1-15"></a>    mzones =<span class="st"> </span>glue<span class="op">::</span><span class="kw">glue_collapse</span>(<span class="dt">x =</span> idzone, <span class="st">&quot;&amp;idMZona[]=&quot;</span>)</span>
<span id="cb1-16"><a href="scraping.html#cb1-16"></a>    </span>
<span id="cb1-17"><a href="scraping.html#cb1-17"></a>    dom =<span class="st"> &quot;https://www.immobiliare.it/&quot;</span></span>
<span id="cb1-18"><a href="scraping.html#cb1-18"></a>    stringa =<span class="st"> </span><span class="kw">paste0</span>(dom, tipo, <span class="st">&quot;-case/&quot;</span>, citta, <span class="st">&quot;/?&quot;</span>, mzones)</span>
<span id="cb1-19"><a href="scraping.html#cb1-19"></a>    npages_vec =<span class="st"> </span><span class="kw">str_c</span>(stringa, <span class="st">&quot;&amp;pag=&quot;</span>, <span class="dv">2</span><span class="op">:</span>npages) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">append</span>(stringa, <span class="dt">after =</span> <span class="dv">0</span>)</span>
<span id="cb1-20"><a href="scraping.html#cb1-20"></a>    </span>
<span id="cb1-21"><a href="scraping.html#cb1-21"></a>} <span class="cf">else</span> {</span>
<span id="cb1-22"><a href="scraping.html#cb1-22"></a>    dom =<span class="st"> &quot;https://www.immobiliare.it/&quot;</span></span>
<span id="cb1-23"><a href="scraping.html#cb1-23"></a>    stringa =<span class="st"> </span><span class="kw">paste0</span>(dom, tipo, <span class="st">&quot;-case/&quot;</span>, citta, <span class="st">&quot;/&quot;</span>)  <span class="co"># mzones</span></span>
<span id="cb1-24"><a href="scraping.html#cb1-24"></a>    npages_vec =<span class="st"> </span><span class="kw">glue</span>(<span class="st">&quot;{stringa}?pag={2:npages}&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">append</span>(stringa, <span class="dt">after =</span> <span class="dv">0</span>)</span>
<span id="cb1-25"><a href="scraping.html#cb1-25"></a>    </span>
<span id="cb1-26"><a href="scraping.html#cb1-26"></a>}</span></code></pre></div>
<p>Up to this point pagination has generated a vector of siblings nodes whose children elements number is fixed (i.e. 25 links per page <a href="scraping.html#fig:websitetree">2.2</a> lower part). That makes those trees <em>k-ary</em>, where k is 25 indicating the number of children leaves. K-ary trees are rooted trees in which each node has no more than k children, in this particular case final leaves. The well known binary rooted tree is actually a special case of k-ary when <span class="math inline">\(k = 2\)</span>. parameters reverse engineering process and 25-ary trees with equal content structure across siblings allow to design a single function to call that could be mapped for all the other siblings. In addition in order to further unroll the website a specific scraping function grabs the whole set of 25 links per page. As a result a single function call of <code>scrape_href()</code> can grab the links corresponding to page 1. Then the function is mapped for all the generated siblings nodes (i.e. up to 300) obtaining a collection of all links belonging to the set of pages. Ultimately the complete set of links corresponds to every single advertisement posted on immobiliare.it at a given time.</p>
<div class="figure"><span id="fig:websitetree"></span>
<img src="images/website_tree1.jpg" alt="" />
<p class="caption">Figure 2.2: immobiliare.it website structure, author’s source</p>
</div>
</div>
<div id="immobiliare.it-content-architecture-with-rvest" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Immobiliare.it content architecture with <code id="ContentArchitecture">rvest</code></h3>
<p>To start a general scraping function the only requirements are a target url (i.e. the filtered root node url) and a way to compose url (i.e. pagination ). Then a session class object <code>html_session</code> is opened by specifying the url and the request data that the user needs to send to the web server, see left part to dashed line in figure <a href="scraping.html#fig:workflow">2.3</a>. Information to be attached to the web server request will be further explored later, tough they are mainly three: User Agents, emails references and proxy servers. <code>html_session</code> class objects contains a list number of useful data such as: the url, the response, cookies, session times etc. Once the connection is established (response request 200) all the following operations rely on the opened session, in other words for the time being in the session the user will be authorized with the already provided request data. The list object contains the xml/html content response of the webpage and that is where data needs to be parsed and class converted. The list can disclose as well other interesting meta information related to the session but in this context are not collected. The light blue wavy line follows the steps required to get the content parsed from the beginning to the end.</p>
<div class="figure"><span id="fig:workflow"></span>
<img src="images/workflow.png" alt="" />
<p class="caption">Figure 2.3: rvest general flow chart, author’s source</p>
</div>
<p>To the right of dashed line in the flow chart <a href="scraping.html#fig:workflow">2.3</a> are painted a sequence of <code>rvest</code><span class="citation">(Wickham <a href="#ref-rvest" role="doc-biblioref">2019</a>)</span> functions that follow a general step by step text comprehension rules. <code>rvest</code> first handles parsing the html respose content of the web page within the session through <code>read_html()</code>. Secondly, as in figure <a href="scraping.html#fig:ContentStructure">2.4</a>, it looks for a single node <code>html_nodes()</code> through a specified CSS query. CSS is a way to route <code>rvest</code> to consider a precise node or set of nodes in the web page. For each information contained in each of the web page a different CSS query has to be called.
Thirdly it converts the content (i.e. payload) into a human readable text with <code>html_text()</code>. A simplified version of the important contents to be scraped in each single link is sketched in figure <a href="scraping.html#fig:ContentStructure">2.4</a></p>
<div class="figure"><span id="fig:ContentStructure"></span>
<img src="images/content_structure.jpg" alt="" />
<p class="caption">Figure 2.4: immobiliare.it important content structure, author’s source</p>
</div>
<p>The code chunk below exemplifies a function that can scrape the price. The function explicitly covers only the right part to the dashed line (figure <a href="scraping.html#fig:workflow">2.3</a>) of the whole scraping process. The initial part (left dashed in same figure), where session is opened and response is converted is handles inside the second code chunk where <code>get.data.catsing()</code> is.</p>
</div>
</div>
<div id="ProperScraping" class="section level2">
<h2><span class="header-section-number">2.6</span> Proper Scarping</h2>
<p>The present algorithm imposes a nest sequential search strategy gravitating around 2 main criterias: shortest paths and insistent search. At the starting point it is initialized, providing a url, a single session object <code>opensess</code>. The object opensess constitutes a check point obj because it is reused more than once along the algorithm flow. The object contains session data as well as HTML content. Immediately after another object <code>price</code> parses the sessions and points to a CSS query through a set of HTML nodes. The CSS location <code>.im-mainFeatures__title</code> addresses a precise group of data which are found right below the main title. Expectations are that monthly price amount in that location is a single character vector string, containing price along with unnecessary non-UTF characters. Then the algorithm bumps into the first <code>if</code> statement. The logical condition checks whether the object <code>price</code> first CSS search went lost. If it does not the algorithm directly jumps to the end of the algorithm and returns a preprocessed singl quantity. Indeed if it does it considers again the check up <code>opensess</code> and hits with a second css query <code>.im-features__value , .im-features__title</code>, pointing to second data location. Note that the whole search is done within the same session (i.e. reusing the same session object), so no more additional request headers <a href="scraping.html#spoofing">2.8.1</a> has to be sent). Since the second CSS query points to data sequentially stored into a list object, the newly initialized <code>price2</code> is a type list object containing various information. Then the algorithm flows through a second <code>if</code> statement that checks whether <code>"prezzo"</code> is matched in the list, if it does the algorithm returns the +1 position index element with respect to the “prezzo” position. This happens because data in the list is stored by couples sequentially (as a flattened list), e.g. list(title, “Appartamento Sempione”, energy class, “G”, “prezzo”, 1200/al mese). Then in the end a third CSS query is called and a further nested if statement checks the emptiness of the latest CSS query. <code>price3</code> points to a hidden JSON object within the HTML content. If even the last search is lost then the algorithm escapes in the else statement by setting <code>NA_Character_</code>, ending with any CSS query is able to find price data.
The search skeleton used for price scraping constitutes a standard reusable search method in the analysis for all the scraping functions. However for some of the information not all the CSS location points are available and the algorithm is forced to be following only certain paths, e.g. condizionatore can not be found under main title and so on.</p>
<div class="figure"><span id="fig:pseudocode1"></span>
<img src="images/pseudocode_latex/pseudocode_price.jpg" style="width:70.0%" alt="" />
<p class="caption">Figure 2.5: pseudo code algorithm for price search, author’s source</p>
</div>
<p>Once all the functions have been designed and optmized with respect to their scraping target they need to be grouped into a single function. This is done into the API endpoint which also Which also checks the validity of the url, and registers the parallel back end.</p>
</div>
<div id="best-practices" class="section level2">
<h2><span class="header-section-number">2.7</span> Scraping Best Practices and Security provisions</h2>
<ul>
<li>http introduction presa dal libro sezione 5</li>
</ul>
<p>Web scraping have to naturally interact multiple times with both the <em>client</em> and <em>server side</em> and as a result many precautions must be seriously taken into consideration. From the server side a scraper can forward as many requests as it could (in the form of sessions opened) which might cause a traffic bottleneck (DOS attack <span class="citation">contributors (<a href="#ref-wiki:DOS" role="doc-biblioref">2020</a><a href="#ref-wiki:DOS" role="doc-biblioref">a</a>)</span>) impacting the overall server capacity. As a further side effect it can confuse the nature of traffic due to fake user agents <a href="scraping.html#spoofing">2.8.1</a> and proxy servers, consequently analytics reports might be driven off track.
Those are a small portion of the reasons why most of the servers have their dedicated Robots.txt files. Robots.txt <span class="citation">Meissner (<a href="#ref-meissner_2020" role="doc-biblioref">2020</a>)</span> are a way to kindly ask webbots, spiders, crawlers to access or not access certain parts of a webpage. The de facto “standard” never made it beyond a <em>informal</em> “Network Working Group INTERNET DRAFT”. Nonetheless, the use of robots.txt files is widespread due to the vast number of web crawlers (e.g. <a href="https://en.wikipedia.org/robots.txt">Wikipedia robot</a>, <a href="https://www.google.com/robots.txt">Google robot</a>). Bots from the own Google, Yahoo adhere to the rules defined in robots.txt files, although their <em>interpretation</em> might differ.</p>
<p>Robots.txt files <span class="citation">(Meissner and Ren <a href="#ref-robotstxt" role="doc-biblioref">2020</a>)</span> essentially are plain text and always found at the root of a website’s domain. The syntax of the files follows a field-name value scheme with optional preceding user-agent. Blocks are separated by blank lines and the omission of a user-agent field (which directly corresponds to the HTTP user-agent field, cleared later in <a href="scraping.html#spoofing">2.8.1</a>) is seen as referring to all bots. The whole set of possible field names are pinpointed in <span class="citation">Google (<a href="#ref-google:robottxt" role="doc-biblioref">2020</a>)</span>, some important are: user-agent, disallow, allow, crawl-delay, sitemap and host. A standard set of shared interpretation is:</p>
<ul>
<li>Finding no robots.txt file at the server (e.g. HTTP status code 404) implies full permission.</li>
<li>Sub-domains should have their own robots.txt file, if not it is assumed full permission.</li>
<li>Redirects from subdomain www to the domain is considered no domain change - so whatever is found at the end of the redirect is considered to be the robots.txt file for the subdomain originally requested.</li>
</ul>
<p>A scraping is explored in the <code>polite</code> <span class="citation">Perepolkin (<a href="#ref-polite" role="doc-biblioref">2019</a>)</span> package which combines the effects of <code>robotstxt</code>, <code>ratelimitr</code> <span class="citation">(<a href="#ref-ratelimitr" role="doc-biblioref">2018</a>)</span> to limit sequential session requests together with the <code>memoise</code> <span class="citation">Wickham et al. (<a href="#ref-memoise" role="doc-biblioref">2017</a>)</span> for robotstxt response caching. Even though the solution meets the requirements (from server and client side) ratelimitr is not designed to run in parallel as documented in the vignette <span class="citation">Shah (<a href="#ref-ratelimitr" role="doc-biblioref">2018</a>)</span>, so it is not involved in the final outcome. However the 3 simple and effective ideas wrapped up in the package describes what a “polite” session should look like and by doing principles are kept fixed during the scraping:</p>
<blockquote>
<p>The three pillars of a polite session are seeking permission, taking slowly and never asking twice.</p>
<footer>
— Polite
</footer>
</blockquote>
<p>The three pillars constitute the <em>Ethical</em> web scraping manifesto <span class="citation">(Densmore <a href="#ref-densmore_2019" role="doc-biblioref">2019</a>)</span> which are common shared <em>best practices</em> that are aimed to self regularize scrapers. Still these have to be intended as practices and by no means as law enforcements. However many scrapers themselves, as website administrators or analyst, have fought in daily working tasks with bots and product derivatives. Intensive Crawling might fake out real client navigation log messages and digital footprint and as a consequence might induce distorted analytics.
With that said a custom function that permanently checks the validity of the session request is called once. It has to be invoked prior any scraping function execution and then immediately cached into a variable. In the result below the function applied to the domain of immobiliare.it returns a boolean approving or disallowing the permission.</p>
<pre><code>## Memoised Function:
## [1] TRUE</code></pre>
<p>Furthermore a custom function based on robotxtst cached results initially checks if the bot can search the inputted address through <code>polite_permission</code>. Then It observes the suggested delay date, in this particular context no delays are kindly asked. As a polite author choice delay request rate is set equal to 5 seconds. Delayed requests rate are managed through the <code>purrr</code> stack. At first a <code>rate</code> object is initialized based on polite_permission, therefore a <code>rate_sleep</code> delay is called within scraping as in <span class="citation">Lionel Henry RStudio (<a href="#ref-rate_delay" role="doc-biblioref">2020</a><a href="#ref-rate_delay" role="doc-biblioref">b</a>)</span>.</p>
<pre><code>## [1] &quot;immobiliare.it&quot;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="scraping.html#cb4-1"></a>get_delay =<span class="st"> </span><span class="cf">function</span>(memoised_robot, domain) {</span>
<span id="cb4-2"><a href="scraping.html#cb4-2"></a>    </span>
<span id="cb4-3"><a href="scraping.html#cb4-3"></a>    <span class="kw">message</span>(<span class="kw">glue</span>(<span class="st">&quot;Refreshing robots.txt data for %s... {domain}&quot;</span>))</span>
<span id="cb4-4"><a href="scraping.html#cb4-4"></a>    temp =<span class="st"> </span>memoised_robot<span class="op">$</span>crawl_delay</span>
<span id="cb4-5"><a href="scraping.html#cb4-5"></a>    </span>
<span id="cb4-6"><a href="scraping.html#cb4-6"></a>    <span class="cf">if</span> (<span class="kw">length</span>(temp) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(temp[<span class="dv">1</span>, ]<span class="op">$</span>value)) {</span>
<span id="cb4-7"><a href="scraping.html#cb4-7"></a>        star =<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">filter</span>(temp, useragent <span class="op">==</span><span class="st"> &quot;*&quot;</span>)</span>
<span id="cb4-8"><a href="scraping.html#cb4-8"></a>        <span class="cf">if</span> (<span class="kw">nrow</span>(star) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) </span>
<span id="cb4-9"><a href="scraping.html#cb4-9"></a>            star =<span class="st"> </span>temp[<span class="dv">1</span>, ]</span>
<span id="cb4-10"><a href="scraping.html#cb4-10"></a>        <span class="kw">as.numeric</span>(star<span class="op">$</span>value[<span class="dv">1</span>])</span>
<span id="cb4-11"><a href="scraping.html#cb4-11"></a>    } <span class="cf">else</span> {</span>
<span id="cb4-12"><a href="scraping.html#cb4-12"></a>        5L</span>
<span id="cb4-13"><a href="scraping.html#cb4-13"></a>    }</span>
<span id="cb4-14"><a href="scraping.html#cb4-14"></a>    </span>
<span id="cb4-15"><a href="scraping.html#cb4-15"></a>}</span>
<span id="cb4-16"><a href="scraping.html#cb4-16"></a><span class="kw">get_delay</span>(rbtxt_memoised, <span class="dt">domain =</span> dom)</span></code></pre></div>
<pre><code>## [1] 5</code></pre>
</div>
<div id="web-client-security-provisions-user-agents-proxies-and-fail-dealers" class="section level2">
<h2><span class="header-section-number">2.8</span> Web Client Security provisions: User Agents, Proxies and Fail Dealers</h2>
<p>HTTP headers are sent via HTTP protocol transactions and allow the client and the server to pass additional information with the request or the response. Some of most important request header fields are User agent, proxies, urls and e-mails addresses. From a very general point of view the process according to which HTTP protocols allow to exchange information can be easily figured out by an everyday real life world analogy. As a generic person A rings to the door’s bell of person B. Then A is coming to B door with its personal information, i.e. name, surname, where he lives etc. Since now B may either positively answer to A requests by opening the door given the set of information he has, or it may not since B is not sure of the real intentions of A. The situation can be transposed on the internet where the user browser (in the example above A) is interacting with a website server (part B) sending packets of information, figure <a href="scraping.html#fig:webworks">2.6</a>. If a server does not trust the information provided by the user, if the requests are too many, if the requests seems to be scheduled due to fixed sleeping time, a server can block requests. In certain cases it can even forbid the user to open a session to the website. Servers are built with a immune-system like software that raises barriers and block users to prevent dossing or other illegal acts.</p>
<div class="figure"><span id="fig:webworks"></span>
<img src="images/how_web_works.png" alt="" />
<p class="caption">Figure 2.6: How the web interacts between broswer and server</p>
</div>
<div id="spoofing" class="section level3">
<h3><span class="header-section-number">2.8.1</span> HTTP User Agent and Mail Spoofing</h3>

<div class="definition">
<span id="def:useragents" class="definition"><strong>Definition 2.2  (User Agents)  </strong></span>The user agent (from now refered as UA) “retrieves, renders and facilitates end-user interaction with Web content” <span class="citation">User:Jallan (<a href="#ref-UaDef" role="doc-biblioref">2011</a>)</span>.
</div>

<p>In HTTP, the UA string is often considered as <em>content negotiator</em> <span class="citation">(contributors <a href="#ref-wiki:UserAgent" role="doc-biblioref">2020</a><a href="#ref-wiki:UserAgent" role="doc-biblioref">d</a>)</span>. The requested server in the form of code embedded into the hosted website selects the most appropriate content on the basis of operating parameters for the response. Therefore according to the UA, the web server can load different CSS based on the outcome, deliver custom JavaScript, automatically send the correct translation due to UA language preferences <span class="citation">(WhoIsHostingThis.com <a href="#ref-whoishostingthis.com" role="doc-biblioref">2020</a>)</span>.
However UA fieldcname has been recently sentenced as superseded in favor of a newer (2020) proactive content negotiator named <em>Hints</em> <span class="citation">contributors (<a href="#ref-wiki:UserAgentHints" role="doc-biblioref">2020</a><a href="#ref-wiki:UserAgentHints" role="doc-biblioref">c</a>)</span>.
UA is a dense content string that includes many user details: the user application or software, the operating system (and versions), the web client, the web client’s version, as well as the web engine responsible for the content display (such as AppleWebKit).
A full components breakdown of UA example might be:</p>
<p><code>Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36</code></p>
<ul>
<li>The user agent application is Mozilla version 5.0.</li>
<li>The operating system is Windows NT 6.3; WOW64, running on Windows</li>
<li>The client is Chrome version 45.0.2454.85.</li>
<li>The client is based on Safari version 537.36.</li>
<li>The engine responsible for displaying content on this device is AppleWebKit version 537.36 (and KHTML, an open-source layout engine, is present too).</li>
</ul>
<p>The UA string is also one of the main responsible according to which Web crawlers and scrapers through a dedicated name field in robotstxt <a href="scraping.html#best-practices">2.7</a> may be ousted from accessing certain parts of a website. Since many requests are sent the server may encounter insistently the same UA and as consequence it may block requests associated to the same UA. In order to avoid server block this scraping technique adopts a rotation of a pool of UAs. Each time requests are sent a different set of headers are drawn from the pool and then combined. The more the pool is populated the larger are the UA combinations. The solution proposed tries in addition to resample periodically the pool as soon as the website from which Agents ID are extracted updates newer UA strings.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="scraping.html#cb6-1"></a><span class="kw">set.seed</span>(<span class="dv">27</span>)</span>
<span id="cb6-2"><a href="scraping.html#cb6-2"></a>url =<span class="st"> &quot;https://user-agents.net/&quot;</span></span>
<span id="cb6-3"><a href="scraping.html#cb6-3"></a>agents =<span class="st"> </span><span class="kw">read_html</span>(url) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">html_nodes</span>(<span class="dt">css =</span> <span class="st">&quot;.agents_list li&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">html_text</span>()</span>
<span id="cb6-4"><a href="scraping.html#cb6-4"></a></span>
<span id="cb6-5"><a href="scraping.html#cb6-5"></a>agents[<span class="kw">sample</span>(<span class="dv">1</span>)]</span></code></pre></div>
<pre><code>## [1] &quot;Mozilla/5.0 (Linux; Android 10; BLA-L29; HMSCore 5.0.5.300; GMSCore 20.47.13) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 HuaweiBrowser/11.0.3.304 Mobile Safari/537.36&quot;</code></pre>
<p>The same procedure has been applied to mails attached to the request headers. E-mails, that are randomly generated from a website, are scraped and subsequently stored into a variable. The A further way to see what it has been done for both UA and mails is considering low level API calls to dedicated servers nested into a more general higher level API.</p>
<p>An even more secure approach may be accomplished rotating proxy servers between the back and forth sending-receiving process. A proxy server acts as a gateway between the web user and the web server. While the user is exploiting a proxy server, internet traffic flows through the proxy server on its way to the server requested. The request then comes back through that same proxy server and then the proxy server forwards the data received from the website back to the client. The final combination would give birth to a more complex linear combination, adding a further layer of masking.
Many proxy servers are offered as paid version. In this particular case security barriers are not that high and this suggests to not apply them. As a further disclaimer many online services are providing free proxies server access, but this comes at a personal security cost due to a couple of reasons:</p>
<ul>
<li>Free plan Proxies are shared among a number of different clients, so as long as someone has used them in the past for illegal purposes the client is indirectly inheriting their legal infringements.</li>
<li>Very cheap proxies, for sure all of the ones free, have the activity redirected on their servers monitored, profiling in some cases a user privacy violation issue.</li>
</ul>
</div>
<div id="possibly" class="section level3">
<h3><span class="header-section-number">2.8.2</span> Dealing with failure</h3>
<p>During scraping many difficulties coming from different sources are met. Some of them may come from the website’s layout changes (<a href="scraping.html#ProperScraping">2.6</a>), some of them may regard internet connection, some other may have been caused by security breaches (section <a href="scraping.html#spoofing">2.8.1</a>).
One of the most inefficient event it can happen is an unexpected error thrown while sending requests that causes all the data previously acquired going lost. In this particular context is even more worrying since scraping “main” functions is able to call 34 different functions each of which points to a different data location. Within a single function invocation, pagination contributes to initialize 10 pages. Each single page includes 25 different single links (<a href="scraping.html#crawling">2.4</a>) leading to a number of 8500 single data pieces. Unfortunately the probability given 8500 associated to one piece being lost, unparsed is frankly high.
For all the reasons said scraping functions needs to deal with the possibility to fail. This is carried out by the implementation of <code>purrr</code> vectorization function <code>map</code> (and its derivatives) and the adverb <code>possibly</code> <span class="citation">Lionel Henry RStudio (<a href="#ref-possibly" role="doc-biblioref">2020</a><a href="#ref-possibly" role="doc-biblioref">a</a>)</span>. <em>Possibly</em> takes as argument a function (map iteration over a list) and returns a modified version. In this case, the modified function returns an empty dataframe regardless of the error thrown. The approach is strongly encouraged when functions need to be mapped over large objects and time consuming processes as outlined in <span class="citation">Hadley Wickham (<a href="#ref-Rdatascience" role="doc-biblioref">2017</a>)</span> section 21.6. Moreover vecrotizaion is not only applied to a vector of urls, but also to a set of functions defined in the environemnt.</p>
<div class="figure"><span id="fig:pseudocode2"></span>
<img src="images/pseudocode_latex/pseudocode_possibly.jpg" style="width:70.0%" alt="" />
<p class="caption">Figure 2.7: pseudo code for a generic set of functions applied with possibly fail dealers , author’s source</p>
</div>
<!-- Furthermore inside each single scrapping function are bundled input checkers that may protect both the web server and the end user from what are called unpolite sessions. This set up allows to catch (and in some cases prevent) web sites policy misuses from the very beginning of the scraping process and be sured that high standard security requirements set up in between requests are respected. -->
<!-- - `get_ua()` verifies that the User Agent in the session is not the default one. -->
<!-- ```{r GetUa, eval=FALSE, echo=TRUE} -->
<!-- get_ua = function(sess) { -->
<!--   stopifnot(is.session(sess)) -->
<!--   stopifnot(is_url(sess$url)) -->
<!--   ua = sess$response$request$options$useragent -->
<!--   return(ua) -->
<!-- } -->
<!-- ``` -->
<!-- - `is_url()` verifies that the url input needed has the canonic form. This is done by a REGEX query. -->
<!-- ```{r IsUrl, tidy=FALSE, echo=TRUE, eval=FALSE} -->
<!-- is_url = function(url){ -->
<!--   re = "^(?:(?:http(?:s)?|ftp)://)(?:\\S+(?::(?:\\S)*)?@) -->
<!--   ?(?:(?:[a-z0-9\u00a1-\uffff](?:-)*)*(?:[a-z0-9\u00a1-\u -->
<!--   ffff])+)(?:\\.(?:[a-z0-9\u00a1-\uffff](?:-)*)*(?:[a-z0- -->
<!--   9\u00a1-\uffff])+)*(?:\\.(?:[a-z0-9\u00a1-\uffff]){2,}) -->
<!--   (?::(?:\\d){2,5})?(?:/(?:\\S)*)?$" -->
<!--   grepl(re, url) -->
<!-- } -->
<!-- ``` -->
</div>
</div>
<div id="parallelscraping" class="section level2">
<h2><span class="header-section-number">2.9</span> Parallel Scraping</h2>
<p>Scraping run time is crucial when dealing with dynamic web pages. This assumption is stronger in Real Estate rental markets where time to market is a massive competitive advantage.
From a run time perspective the dimension of the problem requires as many html session opened as single links crawled (refer to previous section <a href="scraping.html#possibly">2.8.2</a>). As a result computation needs to be <em>parallelized</em> in order to be feasible.
The extraordinary amount of time taken in a non-parallel environment is caused by R executing scraping on a single processor <em>sequentially</em> url-by-url in a queue, left part of figure <a href="scraping.html#fig:singlethreaded">2.8</a> (i.e. single threaded computing).</p>

<div class="definition">
<span id="def:parallel" class="definition"><strong>Definition 2.3  (parallel)  </strong></span><em>Parallel execution</em> is characterized as multiple operations taking place over overlapping time periods. <span class="citation">(Eddelbuettel <a href="#ref-eddelbuettel2020parallel" role="doc-biblioref">2020</a>)</span>
</div>

<p>This requires multiple execution units and modern processors architecture provide multiple cores on a single processor and a way to redistribute computation (i.e. multi threaded computing). As a result tasks can be split into smaller chunks over processors and then multiple cores for each processor, right part of figure <a href="scraping.html#fig:singlethreaded">2.8</a>.
Therefore Parallel scraping (sometimes improperly called <a href="https://medium.com/@cummingsi1993/the-difference-between-asynchronous-and-parallel-6400729fa897">asynchronous</a>) functions are proposed, so that computation do not employ vast cpu time (i.e. cpu-bound) and space.</p>
<div class="figure"><span id="fig:singlethreaded"></span>
<img src="images/parallel_problem.jpg" alt="" />
<p class="caption">Figure 2.8: single threaded computing vs parallel computing, <span class="citation">Barney (<a href="#ref-barney" role="doc-biblioref">2020</a>)</span> source</p>
</div>
<p>Parallel execution heavily depends on hardware and software choice. Linux environments offers multi-core computation through <em>forking</em> <span class="citation">(contributors <a href="#ref-wiki:forking" role="doc-biblioref">2020</a><a href="#ref-wiki:forking" role="doc-biblioref">b</a>)</span> (only on Linux) so that global variables are directly inherited by child processes. As a matter of fact when computation are split over cores they need to import whatever it takes to be carried out, such as libraries, variables, functions. From a certain angle they need to be treated as a containerized stand-alone environments. This can not happen in Windows (local machine) since it does not support multicore.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="scraping.html#cb8-1"></a>future<span class="op">::</span><span class="kw">supportsMulticore</span>()</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p><em>Cluster processing</em> is an alternative to multi-core processing, where parallelization takes place through a collection of separate processes running in the background. The parent R session instructs the dependencies that needs to be sent to the children sessions.
This is done by registering the parallel back end. Arguments to be supplied mainly regards the strategy (i.e. multi-core cluster, also said multisession) and the <em>working group</em>. The working group is a software concept <span class="citation">(“What Is Parallel Computing” <a href="#ref-parallelr" role="doc-biblioref">2020</a>)</span>, that points out the number of processes and their relative computing power/memory allocation according to which the task is going to be split. Moreover from a strictly theoretic perspective the <em>workers</em> (i.e. working group single units) can be greater than the number of physical cores detected. Although parallel libraries as a default choice (and choice for this analysis) initializes <em>as many workers as</em> physical HT (i.e. Hyper Threaded) <em>cores</em>.
Parallel looping constructor libraries generally pops up as a direct cause of new parallel packages. The latest research activity by Bengtsson <span class="citation">Bengtsson (<a href="#ref-doFuture" role="doc-biblioref">2020</a><a href="#ref-doFuture" role="doc-biblioref">c</a>)</span> indeed tries to unify all the previous back ends under the same umbrella of <code>doFuture</code>. The latter library allows to register many back ends for the most popular parallel looping options solving both the dependency inheritance problem and the OS agnostic challenge.
The two alternatives proposed for going parallel are <code>Future</code> <span class="citation">Bengtsson (<a href="#ref-future" role="doc-biblioref">2020</a><a href="#ref-future" role="doc-biblioref">b</a>)</span> with <code>furrr</code> <span class="citation">Vaughan and Dancho (<a href="#ref-furrr" role="doc-biblioref">2018</a>)</span> and <code>doFuture</code> <span class="citation">(<a href="#ref-doFuture" role="doc-biblioref">2020</a><a href="#ref-doFuture" role="doc-biblioref">c</a>)</span> along with the <code>foreach</code> <span class="citation">Microsoft and Weston (<a href="#ref-foreach" role="doc-biblioref">2020</a>)</span> loop constructor. The former is a generic, low-level API for parallel processing as in <span class="citation">Bengtsson (<a href="#ref-bengtsson_2017" role="doc-biblioref">2020</a><a href="#ref-bengtsson_2017" role="doc-biblioref">d</a>)</span>. The latter takes inspiration by the previous work and it provides a back-end agnostic version of <code>doParallel</code> <span class="citation">Corporation and Weston (<a href="#ref-doParallel" role="doc-biblioref">2020</a>)</span>.
<!-- depends both on the parallel library and the looping constructor. Although tangible efforts have been recently put into interoperability through `doFuture` @doFuture library, which allows to register many back ends for the most popular looping options. -->
<!-- Generally parallel libraries require to specify at first both the _computing group_ and the parallel _strategy_.  -->
<!-- One reason resides in default optimal load balancing strategies computation over workers proper of operating systems. As a matter of fact OSs are also responsible for enabling different strategies when they are forced to go parallel (forking @wiki:forking only on Linux). -->
<!-- strategies on the other hand the way workers have to evaluate the tasks.  -->
<!-- In the R ecosystem parallel libraries are a few and are mostly built on top of looping constructions. Two popular choices are `doFuture` -@doFuture along with the `foreach` @foreach loop constructor and `Future` @future with `furrr` @furrr (`purrr` and `future`). The former is a back-end agnostic version of `doParallel` @doParallel, which automatically identifies function, packages and variables to pass to the stand alone workers. The latter is a generic, low-level API for parallel processing as in @bengtsson_2017.  -->
Further concepts on parallel computing are beyond the scope of the analysis. However they can be explored in <span class="citation">Barney (<a href="#ref-barney" role="doc-biblioref">2020</a>)</span>, which may offers a comprehensive perspective on Parallel theory both on hardware and software. Indeed for a full reference on the R parallel ecosystem, run time simulations and advanced algorithm back end design strategies, the authorities are <span class="citation">(“What Is Parallel Computing” <a href="#ref-parallelr" role="doc-biblioref">2020</a>)</span>. If the interest is to cut short theory and directly put existing R code into parallel, a valuable resource is covered in <a href="https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html">blog</a>, which also investigate the main debugging aspects.</p>
<!-- **plot with this  https://github.com/HenrikBengtsson/future/issues/231** -->
<!-- **more on this ** -->
<!-- ![single threaded computing vs parallel computing, @barney source](images/simulations/prova_parallel_distri.png) -->
<!-- ```{r detectCore, eval=FALSE} -->
<!-- cl_hyper = as.integer(detectCores()) -->
<!-- cl_not_hyper = as.integer(detectCores(logical = FALSE)) -->
<!-- print(glue("total n° of Hyper-Threading cores on the machine is {cl_hyper}\n -->
<!--             total n° of NOT Hyper-Threading cores {cl_not_hyper}")) -->
<!-- ``` -->
<!-- ![parallel Execution schema into the R ecosystem, @parallelr source](images/parallel_mapping.png) -->
<div id="parallel-furrrfuture" class="section level3">
<h3><span class="header-section-number">2.9.1</span> Parallel furrr+future</h3>
<p><strong>cerca di centrare di più su scraping</strong></p>
<p>Simulations are conducted on a not-rate-delayed (section <a href="scraping.html#best-practices">2.7</a>) and restricted set of functions which may be considered as a “lightweight” version of the final API scraping endpoint.
As a disclaimer run time simulations may not be really representative to the problem since they are performed on a windows 10, Intel(R) Core(TM) i7-8750H 12 cores RAM 16.0 GB local machine. Indeed the API is served on a Linux Ubuntu distro t3.micro 2 cores RAM 1.0 GB server which may adopt forking. Simulations for the reasons said can only offer a run time performance approximation for both of the parallel + looping constructor combinations.</p>
<p>The first simulation considers <code>furrr</code> which enables mapping (i.e. vectorization with <code>map</code>) through a list of urls with <code>purrr</code> and parallelization with <code>Future</code>. Future gravitates around a programming concept called “future”, initially introduced in late 70’s by Baker <span class="citation">(Baker and Hewitt <a href="#ref-BakerFuture" role="doc-biblioref">1977</a>)</span>. Futures are abstractions for values that may be available at a certain time point later <span class="citation">(<a href="#ref-future" role="doc-biblioref">2020</a><a href="#ref-future" role="doc-biblioref">b</a>)</span>.
These values are result of an evaluated expression, this allows to actually divide the assignment operation from the proper result computation. Futures have two stages <em>unresolved</em> or <em>resolved</em>. If the value is queried while the future is still unresolved, the current process is blocked until the stage is resolved. The time and the way futures are resolved massively relies on which strategy is used to evaluate them. For instance, a future can be resolved using a <em>sequential</em> strategy, which means it is resolved in the current R session. Other strategies registered with <code>plan()</code>, such as <em>multi-core</em> (on Linux) and <em>multisession</em>, may resolve futures in parallel, as already pointed out, by evaluating expressions on the current machine in forked processes or concurrently on a cluster of R background sessions.
With parallel futures the current/main R process does not get “bottlenecked”, which means it is available for further processing while the futures are being resolved in separate processes running in the background. Therefore with a “multisession” plan are opened as many R background sessions as workers/cores on which chunks of futures (urls) are split and resolved in parallel. From an algorithmic point of view It can be compared to <em>a divide and conquer</em> strategy where the target urls are at first redistributed among workers/cores (unresolved) through background sessions and then are scraped in equally distributed chunks (resolved).
Furthermore furrr has also a convenient tuning option which can interfere with the redistribution scheduling of urls’ chunks over workers. The argument scheduling can adjust the average number of chunks per worker. Setting it equal to 2 brings <em>dinamicity</em> <span class="citation">(<a href="#ref-furrr" role="doc-biblioref">2018</a>)</span> to the scheduling so that if at some point a worker is busier then chunks are sent to the more free ones.</p>
<p>(migliore rappresentazione)</p>
<div class="figure"><span id="fig:divideconquer"></span>
<img src="images/divideconquer.png" alt="" />
<p class="caption">Figure 2.9: futures reimagined as divide and conquer, author’s source</p>
</div>
<p>The upper plot in figure <a href="scraping.html#fig:furrrfuture">2.10</a> are 20 simulations of 100 url (2500 data points) performed by the lightweight scraping. On the x-axis are plotted the 20 simulations and on the y-axis are represented the respective elapsed times. One major point to breakdown is the first simulation run time measurement which is considerably higher with respect to the others i.e. 15 sec vs mean 7.72 sec. Empirical demonstrations traces this behavior back to the opening time for all the background sessions. As a result the more are the back ground sessions/workers, the more it would be the time occupied to pop up all the sessions. As opposite whence many sessions are opened the mean execution time for each simulation is slightly less.
The lower plot in in figure <a href="scraping.html#fig:furrrfuture">2.10</a> tries to capture the run time slope behavior of the scraping function when urls (1 to 80) are cumulated one by one. The first iteration scrapes 1 single url, the second iteration 2, the third 3 etc. Three replications of the experiment has been made, evidenced by three colours. The former urls are more time consuming confirming the hypothesis casted before. Variability within the first 40 urls for the three repetitions does not show diversion. However It slightly increases when the 40 threshold is trespassed. Two outliers in the yellow line are visible in the nearby of 50 and 60. It can be assumed that workers in that urls neighbor may be overloaded but no evidences are witnessed on cores activity as in plot <a href="scraping.html#fig:cpumonitor">2.11</a>. The measured computational complexity of scraping when <span class="math inline">\(n\)</span> is number of urls seems to be much more less than linear <span class="math inline">\(\mathcal{O}(0.06n)\)</span>.</p>
<div class="figure"><span id="fig:furrrfuture"></span>
<img src="images/simulations/final_furrr_future.png" alt="" />
<p class="caption">Figure 2.10: computational complexity analysis with Furrr</p>
</div>
<div class="figure"><span id="fig:cpumonitor"></span>
<img src="images/parallel_computing.jpg" alt="" />
<p class="caption">Figure 2.11: local machine monitoring of cores during parallel scraping</p>
</div>
</div>
<div id="parallel-foreachdofuture" class="section level3">
<h3><span class="header-section-number">2.9.2</span> Parallel foreach+doFuture</h3>
<p>A second attempt tries to encapsulate <code>foreach</code> <span class="citation">(Microsoft and Weston <a href="#ref-foreach" role="doc-biblioref">2020</a>)</span> originally developed by Microsoft R, being a very fast loop alternative, parallelized with <code>doFuture</code>. The package registered with older back ends required rigorous effort to specify exact dependencies for child process inside foreach arguments <code>.export</code>. From a certain angle the approach could led to an indirect benefit from memory optimization. If global variables needs to be stated than the developer might be forced to focus on limiting packages exporting. Indeed since doFuture implements optimized auto dependency search this problem may be considered solved as in <span class="citation">Bengtsson (<a href="#ref-doFuture" role="doc-biblioref">2020</a><a href="#ref-doFuture" role="doc-biblioref">c</a>)</span>. Two major looping related speed improvements may come from <code>.inorder</code> and <code>.multicombine</code> arguments which both take advantage of parallel split disorder a subsequent recombination of results. In the context where data collection order matters this is extremely wrong, but since in this case order is defined through url composition based on criteria expressed inside nodes contents this can be totally applied. A drawback of enabling .multicombine is a worst debugging experience since errors are thrown at the end when results are reunited and no traceback of the error is given.</p>
<p>The upper part in <a href="scraping.html#fig:foreachdofuture">2.12</a> displays lower initialization lag from R sessions opening and parallel execution that also lead to a lower mean execution time of 6.42 seconds. No other interesting behavior are detected.
THe lower plot displays high similarities with the curves in <a href="scraping.html#fig:furrrfuture">2.10</a> highlighting an outlier in the same proximities of 45/50 urls. The blue simulation repetition shows an uncommon pattern that is not seen in the other plot. Segmented variability from 40 to 80 suggests a higher value which may be addressed do instability. As a result the approach is discarded in favor of furrr + future which also offers both a comfortable {Tidyverse} oriented framework and offers and easy debugging experience.</p>
<div class="figure"><span id="fig:foreachdofuture"></span>
<img src="images/simulations/final_foreach_dofuture_1&amp;2.png" alt="" />
<p class="caption">Figure 2.12: computational complexity analysis with Furrr</p>
</div>
</div>
</div>
<div id="challenges" class="section level2">
<h2><span class="header-section-number">2.10</span> Open Challenges and Further Improvemements</h2>
<p>Although results are quite encouraging still one of the main challenges remains unsolved. In fact optimization has involved each scraping layer but scraping function must be continuously kept up with the immobiliare.it changes, particularly the crawling part. Indeed the proper scraping part, with some further adjustments can take care of auto-search for exact information within the web page even if the design changes. This idea is massively applied in the package Rcrawler <span class="citation">Khalil (<a href="#ref-Rcrawler" role="doc-biblioref">2018</a>)</span>, which doesn’t apply segmentation in crawling, instead it downloads the entire website and then inspects targeted keywords. The major benefit relies in crawling HTML/XML that are agreed to be generally lightweight in this way the process does not weigh down the run time.
Whence all files are saved locally the algorithm applies search methodologies within the HTML files. Run time performance with algorithm of this kind with respect to the amount of data gathered are very efficient, nevertheless results are not always effective due to keywords disambiguation.
Afterall a way safer and time saving approach to general scraping may be including complex theme specific regular expressions on the HTML text which univocally identify CSS data location. With that said the idea would be an unsupervised algorithm that combines the traditional browser search + a selector gadget for CSS conversion.
As a disclaimer Rcrawler is designed to be flexible to scrape a vast number of websites. As opposite the scraping functions here presented are exclusively built on top of immobiliare.it, even though they can be extended with a small effort to other category related website <a href="scraping.html#webstructure">2.5.1</a>.
On the parallel computing side a further boosts might be added with parallel distributed processing through HPC (high-performance computing) clusters by <code>future.batchtools</code> <span class="citation">Bengtsson (<a href="#ref-futurebatchtools" role="doc-biblioref">2020</a><a href="#ref-futurebatchtools" role="doc-biblioref">a</a>)</span>. The package implements a generic future wrapper around batchtools with job scheduler like Torque, Slurm, Sge and many more. A higher level API built on top of the Future framework that exploits <a href="https://cloudyr.github.io/googleComputeEngineR/articles/massive-parallel.html">Google Cloud Engine Clusters</a> i.e.<code>cloudyR</code> allows to distribute computation on Google machines.
<!-- Moreover error messages can not sometimes be printed out in console and be undesrtood while in parallel beckend. as it is shown the [stackoverflow reproducible example](https://stackoverflow.com/questions/10903787/how-can-i-print-when-using-dopar). As a consequence to that each time an error occurs the "main" functions needs to be taken out of from the parallel back end and separately evaluated. This is time consuming but for the time being no solutions have been found.  --></p>
</div>
<div id="legal-profiles" class="section level2">
<h2><span class="header-section-number">2.11</span> Legal Profiles</h2>
<p><strong>rivedere meglio</strong></p>
<p>“Data that is online and public is always available for all” is never a good answer to the question “Can I use those web data to my scope?”. Immobiliare.it does not provide any open source data neither it disposes any paid API.
A careful reading of immobiliare terms, reviewed with a intellectual property expert has been done to get the service running without any legal consequence, as a reference the full policy can be seen in their <a href="https://www.immobiliare.it/terms/">specialized section</a>. Nevertheless the golden standard for scraping was respected since the robotstxt is neat allowing any actions as demonstrated above in section <a href="scraping.html#best-practices">2.7</a>. So if it may be the case of misinterpretation of their policy, it will be also the case of lack of communication between servers response and immobiliare.it intent to preserve their own intellectual property.</p>
<ul>
<li>To the headers are also attached a direct user back tracking and a url pointing to a dedicated address</li>
</ul>
<p>What it was shockingly surprising were the low entry barriers to scrape information with respect to other counterpart online players. Best practices are in any case applied and kind requests (even though politeness was not asked) have been sent to normalize traffic congestion. But scraping criteria followed are once again fully based on common shared best practices (see section <a href="scraping.html#best-practices">2.7</a>), and <em>not</em> any sort of general agreements between parties. As a result a plausible approach could be applying scraping procedures without any prevention. It would not surely cause any sort of disservice for the website since budget constraints are set low, but in the long run it will cause lagging as soon as requests rate would increase. Totally different was the approach proposed bya coiounterpart market Idealista.com. Idealista does block requests if they are not in compliance with their servers inner rules. User agents in this case must be rotated quite frequently and proxies are necessary. Delay is kindly asked and it must be specified, consequnetly this slows down scraping function per se.</p>
<ul>
<li>Idealista content is composed by Javascript so and html parser can no get that.</li>
<li>Idealista blocks also certain web browser that have a demonstrated “career” in scraping procedures.</li>
</ul>
<p>All of this leads to accept that entry barriers to scrape are for sure higher than the one faced for Immobiliare. The reticence to share data could be a reflex on how big idealista is; as a matter of fact it has a heavy market presence in some of the Europe real estate country as Spain and France. So the hidden intention was to raise awareness on scraping procedure that in a certain remote way can hurt their business. This has been validated by the fact that prior filtering houses on their website a checkbox has to be signed. The checkbox make the user sign an agreement on their platform according to which data can not be misused and it belongs their intellectual property.
<a href="https://blawgsearch.justia.com/">prova qui a vedere</a></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-BakerFuture">
<p>Baker, Henry C., and Carl Hewitt. 1977. “The Incremental Garbage Collection of Processes.” <em>SIGPLAN Not.</em> 12 (8): 55–59. <a href="https://doi.org/10.1145/872734.806932">https://doi.org/10.1145/872734.806932</a>.</p>
</div>
<div id="ref-barney">
<p>Barney, Blaise. 2020. “Introduction to Parallel Computing.” <em>Introduction to Parallel Computing</em>. U.S. Department of Energy by Lawrence Livermore National Laborator. <a href="https://computing.llnl.gov/tutorials/parallel_comp/">https://computing.llnl.gov/tutorials/parallel_comp/</a>.</p>
</div>
<div id="ref-futurebatchtools">
<p>Bengtsson, Henrik. 2020a. <em>Future.batchtools: A Future Api for Parallel and Distributed Processing Using ’Batchtools’</em>. <a href="https://CRAN.R-project.org/package=future.batchtools">https://CRAN.R-project.org/package=future.batchtools</a>.</p>
</div>
<div id="ref-future">
<p>Bengtsson, Henrik. 2020b. “A Unifying Framework for Parallel and Distributed Processing in R Using Futures.” <a href="https://arxiv.org/abs/2008.00553">https://arxiv.org/abs/2008.00553</a>.</p>
</div>
<div id="ref-doFuture">
<p>Bengtsson, Henrik. 2020c. “A Unifying Framework for Parallel and Distributed Processing in R Using Futures.” <a href="https://arxiv.org/abs/2008.00553">https://arxiv.org/abs/2008.00553</a>.</p>
</div>
<div id="ref-bengtsson_2017">
<p>Bengtsson, Henrik. 2020d. “A Unifying Framework for Parallel and Distributed Processing in R Using Futures.” <a href="https://arxiv.org/abs/2008.00553">https://arxiv.org/abs/2008.00553</a>.</p>
</div>
<div id="ref-wiki:DOS">
<p>contributors, Wikipedia. 2020a. “Denial-of-Service Attack — Wikipedia, the Free Encyclopedia.” <a href="https://en.wikipedia.org/w/index.php?title=Denial-of-service_attack&amp;oldid=991730165">https://en.wikipedia.org/w/index.php?title=Denial-of-service_attack&amp;oldid=991730165</a>.</p>
</div>
<div id="ref-wiki:forking">
<p>contributors, Wikipedia. 2020b. “Fork (System Call) — Wikipedia, the Free Encyclopedia.” <a href="https://en.wikipedia.org/w/index.php?title=Fork_(system_call)&amp;oldid=989449307">https://en.wikipedia.org/w/index.php?title=Fork_(system_call)&amp;oldid=989449307</a>.</p>
</div>
<div id="ref-wiki:UserAgentHints">
<p>contributors, Wikipedia. 2020c. “HTTP Client Hints — Wikipedia, the Free Encyclopedia.” <a href="https://en.wikipedia.org/w/index.php?title=HTTP_Client_Hints&amp;oldid=972093401">https://en.wikipedia.org/w/index.php?title=HTTP_Client_Hints&amp;oldid=972093401</a>.</p>
</div>
<div id="ref-wiki:UserAgent">
<p>contributors, Wikipedia. 2020d. “User Agent — Wikipedia, the Free Encyclopedia.” <a href="https://en.wikipedia.org/w/index.php?title=User_agent&amp;oldid=992080962">https://en.wikipedia.org/w/index.php?title=User_agent&amp;oldid=992080962</a>.</p>
</div>
<div id="ref-doParallel">
<p>Corporation, Microsoft, and Steve Weston. 2020. <em>DoParallel: Foreach Parallel Adaptor for the ’Parallel’ Package</em>. <a href="https://CRAN.R-project.org/package=doParallel">https://CRAN.R-project.org/package=doParallel</a>.</p>
</div>
<div id="ref-css_2020">
<p>“CSS.” 2020. <em>Wikipedia</em>. Wikimedia Foundation. <a href="https://en.wikipedia.org/wiki/CSS">https://en.wikipedia.org/wiki/CSS</a>.</p>
</div>
<div id="ref-densmore_2019">
<p>Densmore, James. 2019. “Ethics in Web Scraping.” <em>Medium</em>. Towards Data Science. <a href="https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01">https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01</a>.</p>
</div>
<div id="ref-Graph_Diestel">
<p>Diestel, Reinhard. 2006. <em>Graph Theory</em>. 3rd ed. Graduate Texts in Mathematics. Springer.</p>
</div>
<div id="ref-eddelbuettel2020parallel">
<p>Eddelbuettel, Dirk. 2020. “Parallel Computing with R: A Brief Review.” <a href="http://arxiv.org/abs/1912.11144">http://arxiv.org/abs/1912.11144</a>.</p>
</div>
<div id="ref-google:robottxt">
<p>Google. 2020. “Presentazione Dei File Robots.txt - Guida Di Search Console.” <em>Google</em>. Google. <a href="https://support.google.com/webmasters/answer/6062608?hl=it">https://support.google.com/webmasters/answer/6062608?hl=it</a>.</p>
</div>
<div id="ref-Rdatascience">
<p>Hadley Wickham, Garrett Grolemund. 2017. <em>R for Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. 1st ed. O’Reilly Media.</p>
</div>
<div id="ref-RSelenium">
<p>Harrison, John. 2020. <em>RSelenium: R Bindings for ’Selenium Webdriver’</em>. <a href="https://CRAN.R-project.org/package=RSelenium">https://CRAN.R-project.org/package=RSelenium</a>.</p>
</div>
<div id="ref-html_2020">
<p>“HTML.” 2020. <em>Wikipedia</em>. Wikimedia Foundation. <a href="https://en.wikipedia.org/wiki/HTML">https://en.wikipedia.org/wiki/HTML</a>.</p>
</div>
<div id="ref-Javascript_2020">
<p>“JavaScript.” 2020. <em>Wikipedia</em>. Wikimedia Foundation. <a href="https://en.wikipedia.org/wiki/JavaScript">https://en.wikipedia.org/wiki/JavaScript</a>.</p>
</div>
<div id="ref-Rcrawler">
<p>Khalil, Salim. 2018. <em>Rcrawler: Web Crawler and Scraper</em>. <a href="https://CRAN.R-project.org/package=Rcrawler">https://CRAN.R-project.org/package=Rcrawler</a>.</p>
</div>
<div id="ref-possibly">
<p>Lionel Henry RStudio, Hadley Wickham RStudio. 2020a. “Capture Side Effects. — Safely • Purrr.” <a href="https://purrr.tidyverse.org/reference/safely.html">https://purrr.tidyverse.org/reference/safely.html</a>.</p>
</div>
<div id="ref-rate_delay">
<p>Lionel Henry RStudio, Hadley Wickham RStudio. 2020b. “Create Delaying Rate Settings - Rate-Helpers.” <em>Rate-Helpers, Purrr</em>. Lionel Henry, Hadley Wickham. <a href="https://purrr.tidyverse.org/refeence/rate-helpers.html">https://purrr.tidyverse.org/refeence/rate-helpers.html</a>.</p>
</div>
<div id="ref-meissner_2020">
<p>Meissner, Peter. 2020. <em>Using Robotstxt</em>. <a href="https://cran.r-project.org/web/packages/robotstxt/vignettes/using_robotstxt.html">https://cran.r-project.org/web/packages/robotstxt/vignettes/using_robotstxt.html</a>.</p>
</div>
<div id="ref-robotstxt">
<p>Meissner, Peter, and Kun Ren. 2020. <em>Robotstxt: A ’Robots.txt’ Parser and ’Webbot’/’Spider’/’Crawler’ Permissions Checker</em>. <a href="https://CRAN.R-project.org/package=robotstxt">https://CRAN.R-project.org/package=robotstxt</a>.</p>
</div>
<div id="ref-foreach">
<p>Microsoft, and Steve Weston. 2020. <em>Foreach: Provides Foreach Looping Construct</em>. <a href="https://CRAN.R-project.org/package=foreach">https://CRAN.R-project.org/package=foreach</a>.</p>
</div>
<div id="ref-polite">
<p>Perepolkin, Dmytro. 2019. <em>Polite: Be Nice on the Web</em>. <a href="https://CRAN.R-project.org/package=polite">https://CRAN.R-project.org/package=polite</a>.</p>
</div>
<div id="ref-ratelimitr">
<p>Shah, Tarak. 2018. <em>Ratelimitr: Rate Limiting for R</em>. <a href="https://CRAN.R-project.org/package=ratelimitr">https://CRAN.R-project.org/package=ratelimitr</a>.</p>
</div>
<div id="ref-UaDef">
<p>User:Jallan. 2011. “Definition of User Agent - Wai Ua Wiki.” <a href="https://www.w3.org/WAI/UA/work/wiki/Definition_of_User_Agent">https://www.w3.org/WAI/UA/work/wiki/Definition_of_User_Agent</a>.</p>
</div>
<div id="ref-furrr">
<p>Vaughan, Davis, and Matt Dancho. 2018. <em>Furrr: Apply Mapping Functions in Parallel Using Futures</em>. <a href="https://CRAN.R-project.org/package=furrr">https://CRAN.R-project.org/package=furrr</a>.</p>
</div>
<div id="ref-parallelr">
<p>“What Is Parallel Computing.” 2020. <em>ParallelR</em>. parallelr. <a href="http://www.parallelr.com/r-with-parallel-computing/">http://www.parallelr.com/r-with-parallel-computing/</a>.</p>
</div>
<div id="ref-whoishostingthis.com">
<p>WhoIsHostingThis.com. 2020. “User Agent: Learn Your Web Browsers User Agent Now.” <em>WhoIsHostingThis.com</em>. <a href="https://www.whoishostingthis.com/tools/user-agent/">https://www.whoishostingthis.com/tools/user-agent/</a>.</p>
</div>
<div id="ref-rvest">
<p>Wickham, Hadley. 2019. <em>Rvest: Easily Harvest (Scrape) Web Pages</em>. <a href="https://CRAN.R-project.org/package=rvest">https://CRAN.R-project.org/package=rvest</a>.</p>
</div>
<div id="ref-memoise">
<p>Wickham, Hadley, Jim Hester, Kirill Müller, and Daniel Cook. 2017. <em>Memoise: Memoisation of Functions</em>. <a href="https://CRAN.R-project.org/package=memoise">https://CRAN.R-project.org/package=memoise</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Infrastructure.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/thesis/edit/master/02-scraping.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
