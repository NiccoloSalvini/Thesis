# Introduction {#intro}

<!--  You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).-->

The data currently available on the World Wide Web is measured with zettabytes ($1$ zettabyte = $10^~$21)) and it is still expanding, it is also assumed to be exploited for less than the 1%. Equivalently the ocean floors, which scholars have been studying for decades, are only mapped for a 5% of the global surface area. [...] Most of this data becomes obsolete, some other gets canceled, some more gets properly stored. Due to the responsiveness of the Real Estate market, housing data is updated more than daily. In Milan many advertisers posts their property, some other catch a deal and remove their ad. The consumer is then entitled to pay a monthly sum in accordance with the current available proposals and not the "fair value" for a given apartment. Fair value is a rational and unbiased estimate of the market cost of products, services or properties in accounting. In addition to that, given the market circumstances, there are plenty of asymmetries of information between the tenant side and the landlord side. Asymmetries may account hidden costs such as condominium in which are joined some utilities or even the absence of crockery, in the end expenditures related to the fee due to the intermediary. Asymmetries are also found in the nature of the rental economic action and the context in which they are happening, that means rents are transient accommodation options in a context where it happens quite often to change accommodation, and this generates urgency and hurry. The economic conditions in which deals are closed are very unbalanced, a landlord can take some time and consider its opportunities, on the contrary the tenant has the exigency to settle down and start to work/study. These conditions are exacerbated by toxic macroeconomics where demand is much steeper than offer and salaries are not sufficient to make ends meet, this is also truer for young people and juniors. A couple of facts that support these considerations are seen in these COVID19 days (2021-01-21) where mean monthly prices are not decreased and for those that are already renting no discount has been offered, both commercial and private. A further point that raises the stakes is the percentage amount of rents expenditure with respect to the monthly total budget - 17% - on a national scale, from personal experience it impacts almost the half of the total budget.
The motivating purpose of the following work resides on reducing the asymmetries affecting on the client side by providing a "fair value" estimation where houses are not yet posted on a various set of covariates including the spatial ones. 
The first problem arised in chronological order, for any research analysis, come with acquiring data. Since data is either not available for Milan nor updated, as well as for any other city, it has been designed scraping functions to capture this data. This is a common trait of highly biased markets, as a onsequence adopting open data may in a certain sense help closing the gap. The research focuses in one of the major italian players in online real estate, i.e. Immobiliare.it. Scraping functions are genuinely websites' source code interpreters that collects data in a structured way. Urls are the way websites arranges contents and also the only argument required to these functions. That means if there is a way, i.e. a function, to compose urls at will such that only certain contents are displayed then scraping function can be called on these urls and extract desired data based on some parameters. Scraping is handled in R by `rvest` which is wrapped around a popular Python scraping librabry i.e. Beautifulsoup. rvset follows a custom workflow according to which html source code is at first parsed then based on a css query can gather demanded data falling into the css. Since some of the important information are nested into hidden json objects or are absent in one part but present in the others then custom search strategies are adopted. Scraping can damage websites because of insistent requests made to the servers, this is not wanted since benefiting from open source data can not come at the cost of damaging the scraping target. As a consequence a web _etiquette_ is observed taking inspiration from the `Polite` package a revisiting the concepts in more consumer side terms. This is done by putting into place delayed request rates, rotating User Agents (web ID) and through fail dealers (functions that can handle failuree, e.g. trycatch) by the `purrr` stack. Scraping function can take a while and parallelism is required i.e. scraping asynchronously. The selected back-end embodies a programming concept i.e. future by `Future` that enables to split tasks into a "unresolved" and "resolved" stage. Tasks are sent to workers in the form of back ground sessions in an unresolved stage, then are resolved simultaneously. Since now this may be sufficient for personal usage but the idea is to propose a structured open source sofware interacting with different stakeholders. _Desiderata_ also lists: a service that shares information without having to know how it is implemented i.e. API, a service realtively cheap, portable but highly functional and authentication with credentials.  As a result the stack proposed serves a RESTful API with `Plumber` framework generating 2 endpoints each of which calls Parallel scraping functions settled down in section \@ref(scraping). Precautions taken concerns sanitization of user inputs, anti-Dossing strategies and logs monitoring. The software environment is containerized with Docker and then it is  _Composed_  by Docker Compose with a further container housing NGINX proxy server. Orchestration of services is managed through docker compose. NGINX with SSL certificates bring HTTPS communication and authentication. An AWS free tier EC2 server hosts the whole system and the IP is made Elastic. Furthermore the software CI/CD is made automatic by simple connecting cloud services. Each single technology cited is attacked singularly in the dedicated chapter.
The fact that residential locations impact house prices as well as rents is generally accepted and it is usually summed up with a common catch phrase "location location location". Hedonic Price modeling constitutes the economic theoretical foundations that relates the property value/rental to each of the single house characteristics. Literature has widely displayed that various models incorporating spatial location in various settings are only beaten by spatio-temporal ones. 
Hedonic Price Models (HPM) might help through theory set up the linear predictor, indeed the critical part of these model types is always the _estimation_. [...]


<!--  Consumers are increasingly choosing to live in limited space in favor of central locations and proximity to the city center -->

<!-- . This models usually use regressors for residential buildings that capture both the property and the characteristics of the environment in which the property is situated."Can" discusses two results that describe this phenomenon: 1) "neighborhood impacts" and 2) "adjacent effects" . The first is the exchange of a collection of places and public resources, while the second is a form of waste. Moreover, some economists have tried to provide theoretical reasons building suitable econometric property appraisal models. In parallel. Researchers should measure 'neigh-,' using hedonic price methods (HPMs) and view them as the marginal will to pay for effects" And if these qualities are not evaluated explicitly, the related attributes. Economists will currently do their best with the aid of space econometrics[1]. Mate's effects adjacent. Another argument, however, suggests that if data is collected over time, researchers may neglect the effect of the temporal dimension[19]. In fact, in the majority of cases, researchers do not provide knowledge about recurring transactions. As such, housing transaction data refer to several cross parts, meaning, according to chronological order, the data consists of the various findings of the given populations. The simplest approach to processing such data is to construct a broad clustered cross section and to implement pooled OLS regression with separate variables set in space or time. But there is an explicit limit to this strategy. In space and over time it struggles to capture the correlation. As such, estimates of approximate coefficients can be unrealistic and incomplete. This report thus has three goals. Initially there is the implementation of a Bayesian hierarchical spatiotime model. The latent random effect variable in the model tests spatiotemporal correlation. This model has two new methods, combined LAPA (INLA) [39] nested approximations and a stochastic partial differential equation (SPDE) solution, which is demonstrated. INLA is developed for Gaussian latent models based on direct numerical integration. Furthermore, the SPDE method uses the covariance structures of the mother and the Delaunay triangles to generate a random Gaussian Markov (GMRF), strong proxy for the random Gaussian sector (GRF) Finally, the Corsican apartments sector is analyzed using a number of hierarchical models in Bayesia. In all candidate models, the predictive accuracy is comparable, involving individual and joint spatial and temporal random components. A robust model is chosen to detect the position of accommodation, geographical hot spots and reliable forecasts on the valuation of the market. The analysis has the following form. In Section 2, we analyze briefly several recent approaches to calculating spatial and temporal associations in property values, in particular. In Section 3, we define the spatiotemporal hierarchical model and present the INLA system and SPDE approach briefly. In Section 4, we provide the findings of the analysis with evidence, empirical strategies and estimates. -->



<!-- Hedonic modeling refers to the well recognized goal of defining regression models to expound property sale values. There is a rich literature on these models, which we will look at below. This models usually use regressors for residential buildings that capture both the property and the characteristics of the environment in which the property is situated. -->
<!-- In addition to this, awareness has been added of the old maximum spatial random effects for property transactions - "location, location and location." Such modelling is typically complex, with sales of properties aggregated in quarters or years to see the temporal growth of the sales market.. These random effects are characteristic of Gaussian processes and can be called substitution predictors for unmet or unusual predictors that hold spatial and/or temporal information.The random results, however, provide local model modification and Gaussian processes' versatility contributes to better sale price forecasts. The fact that places and times of housing purchases are random is a crucial feature overlooked in such hedonistic modelling. In other terms, sale prices are modeled on a place and time basis; randomness is only added in the regression given locality and time predictors. In comparison, random effects are correlated with fixed positions and periods that these effects are added. Paci et al. (2017) acknowledged in this connection that the compilation of real estate transactions requires the application, for a given market and a given duration, of a random point trend over time and space. That is, there is a random number of transactions and the positions and times of the transactions are random, provided the number. Investigation into the selling price of properties in the sense of a sales route leads to the so-called preferential sampling environment. Preferential sampling (Diggle et al., 2010) is a relatively newly recognized problem, which is discussed below. The underlying principle is that. If the sampling sites exist on a "biased" basis, this will impact our ability to interpolate the surrounding terrain correctly in an effort to learn about a geographic surface over an area by using data from a certain number of locations. -->





<!-- <!-- ABSTRACTTTTTTT --> 

<!-- This thesis has the aim to extract, infer and predict monthly rental price on Milan Real Estate market exploiting _Bayesian geostatistics_ through a convenient computing alternative called INLA. -->
<!-- Data originates from immobiliare.it database and it is extracted through a _scraper_ built on top of the website. The scraper is _optimized_ with respect to both the _server side_ kindly requesting permission and imposing delayed-request rates, and the _client side_ by granting continuity through fail handlers and request's headers rotation. Scraping functions exploits a custom workflow that combines url reverse engineering and optimal searching strategies for scraping. This dramatically speeds up scraping since a dedicated function can recompose urls starting from functional parameter arguments and call scraping on those urls. Speed comes from the fact that differently from spiders and derivatives which operate a full crawling down of the web site, the workflow concentrates only on a restricted set of urls. A further critical speed boost is offered by parallelism through the latest _Future_ back-end, and a run time benchmark demonstrates the scraper rapidity for two recent parallel with two configurations. -->
<!-- The scraper is then wrapped into a http API through an opinionated R framework, namely _Plumber_. Security is a major focus and anti dossing strategies, HTTPS and sanitization are singularly treated. The whole API service _Docker_ containers (built upon custom Dockerfiles) is then orchestrated with _Compose_  through a .yml file. The system is consequently served into a free tier _EC2_ machine, and load balanced with _NGINX_ reverse proxy server. The architecture principles stacked on top of the http API elevates it to being RESTful. Software CI/CD is managed through automatic workflow that exploits GitHub and DockerHub, which ultimately allows containers to be pulled into the EC2. Once the RESTful API endpoint is invoked, data, in this case Milan rental market within the municipality borders, is asynchronously scraped and collected into a JSON format. -->
<!-- Traditional spatial bayesian methods are generally slow since covariance matrices are dense and matrix inversions scale to a cubic order. Therefore Integrated Nested Laplace approximation (INLA) is applied constituting a faster computational alternative on a special type of models called Latent Gaussian models (LGM). _INLA_ shorten computations through analytics approximations with Laplace and numerical methods for space matrices with the aim to obtain an approximated posterior distribution of the parameters. -->
<!-- Hedonic Price Models (HPM) constitutes the economic theoretical foundation of the model according to which the linear predictor is set. As a matter of fact house prices are related to the value of the property by their demand-offer price equilibra for each single characteristic (including the spatial ones). A further aspects addresses the fact that prices are considered as a proxy value for rents since they are both interchangeable economic actions satisfying the same need. However the critical part of studying house characteristics in geostatistics is the _estimation_ for the reason already anticipates. -->
<!-- LGMs are defined into a hierarchical bayesian modeling framework, distinguishing three nested hierarchy levels: the likelihood of the data (generally an exponential family), the latent Gaussian Markov Random Field GRMF (where the linear predictor is) and the hyper parameter distribution for which priors are specified. GMRF are suitable since they provide a sparse precision matrix due to conditional assumption, marking matrices tridiagonal. The spatial component of the data is considered as a discrete realization of an underlying unobserved and continuous Gaussian Process (GP) to be estimated, completely characterized by a mean structure and a covariance matrix. For the Gaussian Process are made two major assumptions: stationarity and isotropy, which let specifying a flexible covariance function i.e. Matérn. -->
<!-- The Stochastic Partial Differential Equations (SPDE) solutions can provide a GMRF representation of the GP whose covariance matrix is Matérn. This happens through a triangulation of the domain of the study said mesh. The model is then fitted and cross validated with R-INLA and inference on parameter posterior distribution is given. -->

<!-- <!-- ABSTRACTTTTTTT END --> 

<!-- Trento:  -->

<!-- * Argomento -->
<!-- * Problema -->
<!-- * Obiettivi -->
<!-- * Metodo -->
<!-- * Struttura della tesi -->


<!-- Main themes:  -->

<!-- - Research Question -->
<!-- - Milan Real Estate Controversies in relation to research question -->
<!-- - why the API (perchè mi mancano i dati e perchè è il futuro) -->
<!-- - Open Data discussion personal hope of data sharing and benefits from oper source -->
<!-- - Why a Bayesian approach -->
<!-- - Why INLA  -->


<!-- <!-- paper visto per leagl ma roba buona--> 

<!-- According to Google research devoted to the analysis of USA real estate -->
<!-- buyers’ habits (National Association of Realtors, Google 2012), out of 10 % -->
<!-- homebuyers use the Internet as one of the primary research tools, and 52 % of -->
<!-- customers start their search using the Internet. According to the information -->
<!-- presented by Google, the number of real estate searches grows up yearly by 22 %.  -->
<!-- The main reason for customers is to find information which can reduce purchasing -->
<!-- related risks (Peterson & Merino, 2003). -->
<!-- In Latvia, 79.7 % of the population now goes online regularly (Latvijas -->
<!-- Interneta Asociācija, 2015). Latvian Internet users have the same habits as the -->
<!-- Internet users of the USA or developed Western countries. 7 out of 10 Internet -->
<!-- users (70 %) look for information on web pages (CSP, 2015). Real estate buyers -->
<!-- read general information about a real estate object and compare prices of various -->
<!-- real estate objects -->



<!-- <!-- estratto dall'introduzione dellpinfrastrutture, ha senso messo qua  --> 
<!-- As a general discussion technologies implied can be thought as the distance between a service running locally on a laptop and something that it can actually be put into production, shared among company stakeholders, solving business related problems. When such technologies are applied data scientist and interlocutors gradually close up the gap. Insights are better communicated, data is up-to-date and automation can save time. Nonetheless when the infrastructure is designed with vision then integrating or substituting existing technologies is not trivial. Anyway technologies can not be always embedded because they might be exclusively designed to work only on certain back ends, therefore some choices are not into discussion. With foresight RStudio by setting future-oriented guidelines has spent a lot of effort giving its users an easy, integrated and interconnected environment. By that it is meant that the RStudio community has tried to either integrate or open the possibility to a number of technologies that fill the blanks in their weaker parts. On top of many, an entire package has been dedicated to democratize REST APIs (Plumber [@plumber]). As a further example developers in RStudio have created an entire new paradigm i.e. Shiny [@shiny], a popular web app development package, that enforces the developer to have front-end and back-end technologies tied up in the same IDE. They also added performance monitoring and optimization packages that are fitted into shiny such as shinytest [metti tag] and shinyloadtest [metti tag] to simulate sessions and verify network traffic congestion. -->
