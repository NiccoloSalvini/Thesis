# INLA computation {#inla-spde}

INLA [@Rue2009] stands for Integrated Nested Laplace approximation and constitutes a integral computational alternative to traditional MCMC methods. INLA does approximate bayesian inference on special type of models called LGM (Latent Gaussian Models) due to the fact that they are _computationally_ convenient. The benefits are many, some among the other are:

- Low computational costs, even for large models.
- It provides high accuracy.
- Can define very complex models within that framework.
- Most important statistical models are LGM.
- Very good support for spatial models.
- Implementation of spatio-temporal model enabled.

INLA uses a combination of analytics approximations and numerical integration to obtain an approximated posterior distribution of the parameters in a shorter time period.
Notation is imported from @Blangiardo-Cameletti and @Bayesian_INLA_Rubio, and quite differ from the one in the original paper by Rue, Chopin and Martino -@Rue2009. The chrono-logic steps in the methodology presentation follows @Moraga2019 and author choices. As further notation remarks: bold symbols are considered as vectors, so each time they occur they have to be considered like the _ensamble_ of their values. Furthermore $\tilde\pi$ are the Laplace approximation of integrals. Inner functioning of Laplace approximation and its special usage in the INLA setting is beyond the scope, an easy shortcut focused on the INLA application

## Latent Gaussian Models LGM{#LGM}

INLA can fit only LG type of models so the following work tries to encapsulate the main properties of a LGM, so that a problem can be shaped into the LGM framework with the purpose to exploit its benefits. Given some observations $y_{i \ldots n}$ in order to define a Latent Gaussain Model within the bayesian framework it is convenient to specify at first an  _exponential family_ (Gaussian, Poisson, Exponential...) distribution function characterized by some parameters $\phi_{i}$ (usually expressed by the mean $\left.E\left(y_{i}\right)\right)$) and some other hyper-parameters $\psi_{k} ,\forall k \in \ 1\ldots K$. The parameter $\phi_{i}$ can be defined as an additive _latent linear predictor_ $\eta_{i}$, as pointed out by Krainski and Rubio (-@Krainski-Rubio) through a link function $g(\cdot)$, i.e. $g\left(\phi_{i}\right)=\eta_{i}$. A comprehensive expression of the linear predictor takes into account all the possible effects on covariates

$$
\eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)
$$

where $\beta_{0}$ is the intercept, $\boldsymbol{\beta}=\left\{\beta_{1}, \ldots, \beta_{M}\right\}$ are the coefficient that quantifies the linear effects on covariates $\boldsymbol{x}=\left({x}_{1}, \ldots, {x}_{M}\right)$ and $f_{l}(\cdot), \forall l \in 1 \ldots L$ are a set of random effects defined in terms of a set of covariates $\boldsymbol{z}=\left(z_{1}, \ldots, z_{L}\right)$ (e.g. rw, ar1). Because of the last assumption LGM models can receive a wide range of models e.g. GLM, GAM, GLMM, linear models and spatio-temporal models. This facilitates INLA 
All the latent components can be conveniently grouped into a variable denoted with $\boldsymbol{\theta}$ such that: $\boldsymbol{\theta}=\left\{\beta_{0}, \beta, f\right\}$ and the same can de done for hyper parameters $\boldsymbol{\psi} = \left\{\psi_{1}, \ldots, \psi_{K}\right\}$. 
Then the probability distribution conditioned to parameters and hyper parameters is then:

$$
y_{i} \mid \boldsymbol{\theta}, \boldsymbol{\psi} \sim \pi\left(y_{i} \mid \boldsymbol{\theta},\boldsymbol{\psi}\right)
$$

Since data $\left(y_{1}, \ldots, y_{n}\right)$ is drawn by the same distribution family but it is conditioned to parameters and hyper parameters (i.e. conditional independence) then joint distribution is given by the likelihood. Moreover the Product operator index $i$ ranges from 1 to $n$, i.e.  $\mathbf{I} = \left\{1 \ldots n \right\}$. When an observation is missing so the corresponding i is not in $\mathbf{I}$ INLA automatically will not include it in the model avoiding errors -@Bayesian_INLA_Rubio. As a consequence the likelihood expression is:

\begin{equation}
\pi(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i \in \mathbb{I}} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)
(\#eq:likelihood)
\end{equation}

Each data point is connected to a linear combination of each element in $\boldsymbol{\theta}$ said _latent field_. The latent aspect of the field regards the  undergoing existence of many parameter combination alternatives. Each component of the latent field is supposed to be conditionally independent (i.e. $\pi\left(\theta_{i}, \theta_{j} \mid \theta_{-i, j}\right)=\pi\left(\theta_{i} \mid \theta_{-i, j}\right) \pi\left(\theta_{j} \mid \theta_{-i, j}\right)$) which constitutes a very important assumption. Hyper parameters are by definition independent, in other words $\boldsymbol{\psi}$ will be the product of many univariate priors [@Bayesian_INLA_Rubio]. A Multivariate Normal distribution is imposed on the latent field $\boldsymbol{\theta}$ such that it is centered in 0 with precision matrix $\boldsymbol{Q(\psi)}$ depending only on $\boldsymbol{\psi}$ hyper parameter vector i.e., $\boldsymbol{\theta} \sim \operatorname{Normal}\left(\mathbf{0}, \boldsymbol{Q}^{-1}(\boldsymbol{\psi})\right)$. As side note some authors choose to approach the precision matrix as a covariance function in the setting before. This is strongly not encouraged since it complicates notation. However This has to be forcibly done in next chapter since a special covariance function is used.
The exponential family density function is then expressed through: 

\begin{equation}
\pi(\boldsymbol{\theta} \mid \boldsymbol{\psi})=(2 \pi)^{-n / 2}| \boldsymbol{Q(\psi)}|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta} \boldsymbol{Q(\psi)} \boldsymbol{\theta}\right)
(\#eq:gmrf)
\end{equation}

The conditional independence assumption on the latent field $\boldsymbol{\theta}$ leads $\boldsymbol{Q(\psi)}$ to be a sparse precision matrix, and the probability distribution function can be framed as _Gaussian Markov random field_ (**GMRF**). From here it comes the source of saving in run time computation inherited using GMRF for inference. As a consequence of GMRF representation of the latent field, matrices are sparse so numerical methods can be exploited [@Blangiardo-Cameletti]. Moreover when stochastic process (e.g. Gaussian Process) are driven as GMRF through SPDE equations (Stochastic Partial Differential Equations) INLA can be applied too. This last assumption will be discussed and verified in dedicated chapter \@ref(spde).
Once priors are specified for $\boldsymbol{\psi}$ then the following holds:

$$
\pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y)\propto  \underbrace{\pi(\boldsymbol{\psi})}_{\text {prior }} \times \underbrace{\pi(\theta \mid \psi)}_{\text {GMRF }} \times \underbrace{\prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)}_{\text {likelihood }}
$$

Last expression is said to be **LGM** if the whole set of assumptions imposed since now are met. Therefore all models that can be reduced to a LGM representation are able to host INLA methodology.
At this point it is possible to write the joint posterior distribution by plugging in the _likelihood_ \@ref(eq:likelihood) and _GMRF_ \@ref(eq:gmrf) expression


$$
\begin{aligned}
\pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y) & \propto \pi(\boldsymbol{\psi}) \times \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \pi(y \mid \boldsymbol{\theta}, \boldsymbol{\psi}) \\
& \propto \pi(\boldsymbol{\psi}) \times \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right) \\
& \propto \pi(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}\right) \times \prod_{i}^{n} \exp \left(\log \left(\pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right)
\end{aligned}
$$

Joining the exponentials thanks to exponents multiplicative property:

\begin{equation}
\pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y) \propto \pi(\psi) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}+\sum^{n} \log \left(\pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right)
(\#eq:jointpostdistr)
\end{equation}


## Approximation in INLA setting

INLA is not going to try to estimate the whole posterior distribution from expression \@ref(eq:jointpostdistr). Instead it will try to estimate the posterior marginal distribution effects for each latent parameter $\boldsymbol{\theta}$ given the hyper parameter priors specification $\psi_{k}$. Proper estimation methods however are beyond the scope of the analysis, further excellent references are suggested in their respective part by Rubio -@Bayesian_INLA_Rubio in section 2.2.2  and the cited Blangiardo & Cameletti -@Blangiardo-Cameletti in section 4.7.2. 
The integral for latent parameter $\boldsymbol{\theta}$ is

\begin{equation}
  \pi(\boldsymbol{\theta} \mid \boldsymbol{y})=\int \pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \mathbf{y}) \pi(\boldsymbol{\psi} \mid \mathbf{y}) d \psi
(\#eq:latentparam)
\end{equation}

The posterior marginal integral for the hyper parameter is $\boldsymbol{\psi}$ since it is composed by $K$ elements, i.e. $\boldsymbol{\psi}=\left\{\psi_{1}, \ldots, \psi_{K}\right\}$ is:


$$
\pi\left(\psi_{k} \mid y\right)=\int \pi(\boldsymbol{\psi} \mid y) d \psi_{-k}
$$

where the notation $\psi_{-k}$ is a vector of hyper parameters $\psi$ without the considere element $\psi_{k}$.

Once again the objective is to have approximated solution for latent parameter posterior distribution. To this purpose A *hierarchical structure* is now imposed since the "lower "hyper parameter integral, whose approximation for the moment does not exist, is nested inside the "upper" parameter integral that take hyper param as integrand. Hierarchical structures are welcomed very warmly since it is convenient later for hierarchical bayesian regression approached in the next chapter \@ref(hierreg)). 
By unnesting the structure the two steps are mandatory:

- step 1: compute Laplace approximation marginals for the hyper parameters: $\tilde\pi\left(\psi_{k} \mid \boldsymbol{y}\right)$
- step 2: compute Laplace approximation marginals for the parameters given the hyper parameter approximation: $\tilde{\pi}\left(\theta_{i} \mid \boldsymbol{y}\right) \approx \int \tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) \underbrace{\tilde{\pi}(\boldsymbol{\psi} \mid \boldsymbol{y})}_{\text {Estim. in step 1 }} \mathrm{d} \psi$

Then plugging approximation in the integral observed in \@ref(eq:latentparam) it is obtained:

$$
\tilde{\pi}\left(\theta_{i} \mid y\right) \approx \int \tilde{\pi}\left(\theta_{i} \mid  \boldsymbol{\psi}, y\right) \tilde{\pi}(\boldsymbol{\psi} \mid y) \mathrm{d} \psi
$$

In the end INLA uses the following approximation to compute marginals: 

$$
\tilde{\pi}\left(\theta_{i} \mid y\right) \approx \sum_{j} \tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}^{(j)}, y\right) \tilde{\pi}\left(\boldsymbol{\psi}^{(j)} \mid y\right) \Delta_{j}
$$

{$\boldsymbol{\psi}^{(j)}$} are a set of values of the hyper param $\psi$ grid used for numerical integration, each of which associated to a specific weight $\Delta_{j}$. INLA focus on this specific integration points by setting up a regular grid about the posterior mode of $\psi$ with CCD (central composite design) centered in the mode [@Bayesian_INLA_Rubio].

![Blangiardo-Cameletti source](images/CCDapplied.PNG)

The approximation $\tilde{\pi}\left(\theta_{i} \mid y\right)$ can take different forms and be computed in different ways. @Rue2009 also discuss how this approximation should be in order to reduce the numerical error [@Krainski-Rubio].


### further approximations (prolly do not note include)

Following @Bayesian_INLA_Rubio,  approximations of the joint posterior for the hyper paramer $\tilde\pi\left(\psi_{k} \mid \boldsymbol{y}\right)$  is used to compute the marginals for the latent effects anf hyper parameters in this way: 

$$
\left.\tilde{\pi}(\boldsymbol{\psi} \mid \mathbf{y}) \propto \frac{\pi(\boldsymbol{\theta}, \boldsymbol{\psi}, y)}{\tilde{\pi}_{G}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, y)}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{*}(\boldsymbol{\psi})}
$$
In the previous equation $\tilde{\pi}_{G}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, y)$ is a gaussian approximation to the full condition of the latent effect ${\theta}^{*}(\boldsymbol{\psi})$ is the mode for a given value of the hyper param vector $\boldsymbol{\psi}$ 

At this point there exists three types of approximations for $\pi\left(\boldsymbol{\theta} \mid \boldsymbol{\psi}, y\right)$

- first with a gaussian approximation, estimating mean $\mu_{i}(\boldsymbol{\psi})$ and variance $\sigma_{i}^{2}(\boldsymbol{\psi})$. 
- second using the _Laplace Approximation._ 
- third using _simplified Laplace Approximation_

(rivedere meglio)

## R-INLA package in a bayesian regression perspective

INLA computations and methodology is developed in the R-INLA project whose package is available on their [website](http://www.r-inla.org). Download is not available on CRAN (the Comprehensive R Archive Network) so a special source, which is maintained by authors and collaborators through a bitbucket repo, has to be specified. However download information are neatly showed in a dedicated section in the project website. The website offers also a forum where threads can be proposed and an active community is keen to answer. Furthermore the project is gaining importance due to its vast application a cases that recently are exponentiallly growing, examples are [italiano citato nel video onlie di RUE 2020, altro tizio citato da Rue]
The core function of the package is `inla()`and works as many other regression functions like `glm()` , `lm()` or `gam()`. Inla takes as arguments the formula together with the data (expects a data.frame obj) on which estimation is desired. Many other more alternative methods inside the function can be specifird and are later presented in a table. Inla objects are inla.dataframe summary-type list object containing the results from model fitting. Results contained in the object are specified in the table below, even though some of them requires special method to be seen: (meglio se metto direttamente tabella generate in kable())

![summary table list object, source: @Krainski-Rubio ](images/summarytable.PNG)


### Linear Predictor

Following Krainski and Rubio -@Krainski-Rubio observations $y(s_{1}), \ldots, y(s_{n})$ come from a Gaussian variable with $\mu_{i}$ and precision $\tau$. The mean moment for the gaussian distribution (i.e. ) $\mu_{i}$  has to be specified as _linear predictor_ through a link function as in section \@ref)(genregvec),i.e $\eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)$. 
Simplifying the composition of the linear predictor and using spatial dataset SPDEtoy: Spatial covariates as long and lat have to be included in the linear predictor as well as the others, so that linear predictor has this form: $\beta_{0}+\beta_{1} s_{1 i}+\beta_{2} s_{2 i}$, where once again $\beta_{0}$ is the intercept and $\beta_{j}$ are the linear effect on covariates. It is possible to include also non-linear effects with the `f()` specifying the model The prior choice for intercept is uniform. Prior for Gaussian latent field are vague such that have 0 mean and 0.001 precision, then the prior in the precision $\tau$ is gamma with parameters 1 and 0.00005. Prior choice can then be changes.

The summary of the model already described is:

$$
\begin{aligned}
y_{i} & \sim N\left(\mu_{i}, \tau^{-1}\right), i=1, \ldots, 200 \\
\mu_{i} &=\beta_{0}+\beta_{1} s_{1 i}+\beta_{2} s_{2 i} \\
\beta_{0} & \sim \text { Uniform } \\
\beta_{j} & \sim N\left(0,0.001^{-1}\right), j=1,2 \\
\tau & \sim G a(1,0.00005)
\end{aligned}
$$


```{r, cache=TRUE}
library(INLA)
data("SPDEtoy")
m0 = inla(y ~ s1 + s2, data = SPDEtoy)
summary(m0)
```








































