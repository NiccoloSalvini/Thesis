# Introduction {#intro}

<!--  You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).-->

The data currently available on the World Wide Web is measured with zettabytes ($1$ zettabyte = $10^~$21)) and it is still expanding, it is also assumed to be exploited for less than the 1%. Equivalently the ocean floors, which scholars have been studying for decades, are only mapped for a 5% of the global surface area. [...] Most of this data becomes obsolete, some other gets canceled, indeed some more gets properly stored. Due to the responsiveness of the Real Estate market, housing data is updated more than daily. In Milan many advertisers posts their property, some other catch a deal and remove their ad. The consumer is then entitled to pay a monthly sum in accordance with the current available proposals and not the "fair value" for a given apartment. Fair value is a rational and unbiased estimate of the market cost of products, services or properties in accounting. In addition to that, given the market circumstances, there are plenty of asymmetries of information between the tenant side and the landlord side. Asymmetries may account hidden costs such as condominium in which are joined some utilities or even the absence of crockery, in the end expenditures related to the fee due to the intermediary. Asymmetries are also found in the nature of the rental economic action and the context in which they are happening, that means rents are transient accommodation options in a context where it happens quite often to change accommodation, and this generates urgency and hurry. The economic conditions in which deals are closed are very unbalanced, a landlord can take some time and consider its opportunities, on the contrary the tenant has the exigency to settle down and start to work/study. These conditions are exacerbated by toxic macroeconomics where demand is much steeper than offer and salaries are not sufficient to make ends meet, this is also truer for young people and juniors. A couple of facts that support these considerations are seen in these COVID19 days (`r noquote(Sys.Date())`) where mean monthly prices are not decreased and for those that are already renting no discount has been offered, both commercial and private. A further point that raises the stakes is the percentage amount of rents expenditure with respect to the monthly total budget - 17% - on a national scale, from personal experience it impacts almost the half of the total budget.
The motivating purpose of the following work resides on reducing the asymmetries affecting the client side by providing the data and tools to assess a "fair value" estimation where houses are not yet posted on a various set of covariates, including the spatial ones. The first problem arised in chronological order, for any research analysis, come with acquiring data. Since data is neither available for Milan nor updated, as well as for any other city, it has been designed scraping functions (\@ref(scraping)) to capture this data. This is a common trait of highly biased markets, as a consequence adopting open data may in a certain sense help closing the gap. The research focuses in one of the major italian players in online real estate, i.e. Immobiliare.it. Scraping functions are genuinely websites' source code interpreters that collects data in a structured way. Urls are the way websites arranges contents and also the only argument required to these functions. That means if there is a way, i.e. a function, to compose urls at will such that only certain contents are displayed then scraping function can be called on these urls and extract desired data based on some parameters. Scraping is handled in R by `rvest` which is wrapped around a popular Python scraping librabry i.e. Beautifulsoup. rvset follows a custom workflow according to which html source code is at first parsed then based on a css query can gather demanded data falling into the css. Since some of the important information are nested into hidden json objects or are absent in one part but present in the others then custom search strategies are adopted. Scraping can damage websites because of insistent requests made to the servers, this is not wanted since benefiting from open source data can not come at the cost of damaging the scraping target. As a consequence a web _etiquette_ is observed taking inspiration from the `Polite` package and revisiting the concepts in a more consumer side orientation. This is done by putting into place delayed request rates, rotating User Agents (web ID) and through fail dealers (functions that can handle failuree, e.g. trycatch) by the `purrr` stack. Scraping function can take a while and parallelism is required i.e. scraping asynchronously. The selected back-end embodies a programming concept i.e. future by `Future` that enables to split tasks into a "unresolved" and "resolved" stage. Tasks are sent to workers in the form of back ground sessions in an unresolved stage, then are resolved simultaneously. Since now this may be sufficient for personal usage but the idea is to propose a structured open source sofware interacting with different stakeholders. The _desiderata_ list also accounts: a service that shares information without having to know how it is implemented i.e. API \@ref(Infrastructure), a service realtively cheap, portable but highly functional and authentication with credentials. As a result the stack proposed serves a RESTful API with `Plumber` framework generating 2 endpoints each of which calls Parallel scraping functions settled down in section \@ref(scraping). Precautions taken concerns sanitization of user inputs, anti-Dossing strategies and logs monitoring. The software environment is containerized with Docker and then it is  _Composed_  by Docker Compose with a further container housing NGINX proxy server. Orchestration of services is managed through docker compose. NGINX with SSL certificates bring HTTPS communication and authentication. An AWS free tier EC2 server hosts the whole system and the IP is made Elastic. Furthermore the software CI/CD is made automatic by simple connecting cloud services. Each single technology cited is attacked singularly in the dedicated chapter.
The fact that residential locations impact house prices as well as rents is generally accepted and it is usually summed up with a common catch phrase "location location location". Hedonic Price modeling constitutes the economic theoretical foundations that relates the property value/rental to each of the single house characteristics. Literature has widely displayed that various models incorporating spatial location in various settings are only beaten by spatio-temporal ones. 
Hedonic Price Models (HPM) might help through theory to set up the linear predictor, indeed the critical part of these models is always _estimation_.
As a matter of fact traditional spatial bayesian methods are generally computing intensive and slow. The computational aspect refers in particular to the ineffectiveness of linear algebra operations with large dense covariance matrices that scale to the order of $\mathcal{O}(n^3)$. Unfortunately this is even worse in the spatial contexts where House prices dynamics may be reagrded as stochastic process indexed on a continuous surface  i.e.Gaussian Process. whose covariance matrices are generally $n \times n$ observations. 
Integrated Nested Laplace approximation (INLA \@ref(inla)) lends a hand constituting an alternative and faster deterministic algorithm on a special type of models called Latent Gaussian models (LGM). In few words _INLA_ makes use of optimal approximations with Laplace and numerical methods for space matrices to fasten computation. The under the hood work in INLA gravitates around three main statistical and mathematical concepts: LGM, Gaussian Markov Rrandom Field and Laplace Approximation. Latent Gaussian Models are a class of models for which are need to be specified three further elements: the likelihood for a given set of observation, the Gaussian Markov Random field (GRMF where parameters are) and priors distributions. Two of the three concepts are familiar, indeed GMRF is a pretty simple structure which distributes its arguments (in this case the all latent effects) according to a multivariate Normal density with 0 mean a given precision matrix, additionally with a further conditional independence assumption (here the word “Markov"). The Markov property of GMRFs are encoded in its precision matrices leading them to be very sparse, here resides the most of the calculus saved. Once the model is set up the Baeysian ultimate goal is to find the posterior distributions of parameters and this is not is not going happen estimating the whole joint posterior distribution of the three elements aforementioned. Instead INLA will try to approximate them singularly starting from the easiest and most coherent Gaussian approximation and then quickly executing all of the others according to special gridded strategies.
The spatial component in the extracted data is assumed to be a discrete realization of an unobserved and continuous Gaussian Process (GP), fully characterized by a mean structure and a covariance matrix. There are two main assumptions made for the Gaussian process: stationarity and isotropy, enabling to use a versatile covariance function to be defined i.e. Matern. A GMRF representation of the GP with a Matérn covariance matrix can be produced by the Stochastic Partial Differential Equations (SPDE) solutions through a functional basis representation. This is achieved by triangulating the discrete realizations of the process in the spatial domain. The benefit, once again, of switching from GP to GMRF is that the latter enjoys strong analyitical properties. The model is then mounted and tested with R-INLA and the resulting posterior distribution parameter are served. Prior choices are a critical step into bayesian decision process since they inject subjectivity into the analysis. Penalized complexity priors are a set of guidelines that are wrapped up around building proper prior, in this setting are demonstrated to behave well. In the end the model is fitted and cross validated.


<!--  Consumers are increasingly choosing to live in limited space in favor of central locations and proximity to the city center -->

<!-- . This models usually use regressors for residential buildings that capture both the property and the characteristics of the environment in which the property is situated."Can" discusses two results that describe this phenomenon: 1) "neighborhood impacts" and 2) "adjacent effects" . The first is the exchange of a collection of places and public resources, while the second is a form of waste. Moreover, some economists have tried to provide theoretical reasons building suitable econometric property appraisal models. In parallel. Researchers should measure 'neigh-,' using hedonic price methods (HPMs) and view them as the marginal will to pay for effects" And if these qualities are not evaluated explicitly, the related attributes. Economists will currently do their best with the aid of space econometrics[1]. Mate's effects adjacent. Another argument, however, suggests that if data is collected over time, researchers may neglect the effect of the temporal dimension[19]. In fact, in the majority of cases, researchers do not provide knowledge about recurring transactions. As such, housing transaction data refer to several cross parts, meaning, according to chronological order, the data consists of the various findings of the given populations. The simplest approach to processing such data is to construct a broad clustered cross section and to implement pooled OLS regression with separate variables set in space or time. But there is an explicit limit to this strategy. In space and over time it struggles to capture the correlation. As such, estimates of approximate coefficients can be unrealistic and incomplete. This report thus has three goals. Initially there is the implementation of a Bayesian hierarchical spatiotime model. The latent random effect variable in the model tests spatiotemporal correlation. This model has two new methods, combined LAPA (INLA) [39] nested approximations and a stochastic partial differential equation (SPDE) solution, which is demonstrated. INLA is developed for Gaussian latent models based on direct numerical integration. Furthermore, the SPDE method uses the covariance structures of the mother and the Delaunay triangles to generate a random Gaussian Markov (GMRF), strong proxy for the random Gaussian sector (GRF) Finally, the Corsican apartments sector is analyzed using a number of hierarchical models in Bayesia. In all candidate models, the predictive accuracy is comparable, involving individual and joint spatial and temporal random components. A robust model is chosen to detect the position of accommodation, geographical hot spots and reliable forecasts on the valuation of the market. The analysis has the following form. In Section 2, we analyze briefly several recent approaches to calculating spatial and temporal associations in property values, in particular. In Section 3, we define the spatiotemporal hierarchical model and present the INLA system and SPDE approach briefly. In Section 4, we provide the findings of the analysis with evidence, empirical strategies and estimates. -->


<!-- Hedonic modeling refers to the well recognized goal of defining regression models to expound property sale values. There is a rich literature on these models, which we will look at below. This models usually use regressors for residential buildings that capture both the property and the characteristics of the environment in which the property is situated. -->
<!-- In addition to this, awareness has been added of the old maximum spatial random effects for property transactions - "location, location and location." Such modelling is typically complex, with sales of properties aggregated in quarters or years to see the temporal growth of the sales market.. These random effects are characteristic of Gaussian processes and can be called substitution predictors for unmet or unusual predictors that hold spatial and/or temporal information.The random results, however, provide local model modification and Gaussian processes' versatility contributes to better sale price forecasts. The fact that places and times of housing purchases are random is a crucial feature overlooked in such hedonistic modelling. In other terms, sale prices are modeled on a place and time basis; randomness is only added in the regression given locality and time predictors. In comparison, random effects are correlated with fixed positions and periods that these effects are added. Paci et al. (2017) acknowledged in this connection that the compilation of real estate transactions requires the application, for a given market and a given duration, of a random point trend over time and space. That is, there is a random number of transactions and the positions and times of the transactions are random, provided the number. Investigation into the selling price of properties in the sense of a sales route leads to the so-called preferential sampling environment. Preferential sampling (Diggle et al., 2010) is a relatively newly recognized problem, which is discussed below. The underlying principle is that. If the sampling sites exist on a "biased" basis, this will impact our ability to interpolate the surrounding terrain correctly in an effort to learn about a geographic surface over an area by using data from a certain number of locations. -->


