# Conclusions{#conclusions}

<!-- with UI build with free tool for front end design ion shiny [fomantic-ui](https://fomantic-ui.com/). prendi shiny app e rifai interface. in questo blog vedi Hacaton tirato e vincitori [blog](https://blog.rstudio.com/2020/11/10/the-appsilon-shiny-semantic-pocontest/).  -->

<!-- Senno app paula moraga che ha già simil modello dentro, -->

<!-- senno flexdashboard paula moraga. -->

<!-- [this inspiration](https://demo.appsilon.ai/apps/polluter/) -->

<!-- Reacttable blended in leaflet -->
<!-- https://glin.github.io/reactable/articles/examples.html -->

Ideally the entire work can be partitioned in two different pieces i.e. API and Spatial modeling communicating through a common language: data and  offering an end-to-end project. For what it concerns the API it consistently delivers data and avoids of all sorts of failures. From some of the statistics proposed throughout the analysis (\@ref(parallelscraping)), webscarping general performances are quite high in term of API response time, on which some further analysis are conducted. Results communicate that for 250 data points (6 columns per 10 web pages scrapped) the time taken for the first endpoint i.e. _scrapefast_ is 5 seconds in the very first interrogation (compatible with the background sessions openings \@ref(parallelscraping)). For all the following queries it requires a mean time of approximately 1.2 sec. The behavior is not well understood since the delay happens also for all the new queries even though sessions are already running in background (i.e. close on exit from application). The time spent might be attributed to a bottleneck caused in some helpers function, but the traceback has not been pursued yet. For larger amount of data (i.e. 2500 observation) the running time behaves linearly and the resulting response mean time is approximately 15 secs. Note that when arguments are the same the cache is able to return back data requested in no time.
Indeed for what concerns the second endpoint i.e. _completescrape_ for 250 data points (40 covariates including the spatials one) the time spent is 15 secs, and for bigger datasets it will occupy a couple of minutes. Results are summarized in table below.
Under the security umbrella no malicious attacks have been registered since the is its is being OPA (Open Public API), this was also due to the sought-after anonymity of the project. API Logs have registered in 4 months span time a timid usage from open source contributors and curious people which based on th query received tested the service. NGINX did its job with credentials even though it has never been required to load balance the api traffic since there were not traffic at all. Anyway the worst case scenario is accounted thanks to loadtest that has clearly shown which is the maximum treshold that the api handles previous crashing. In the end legal profiles have not been raised, thankfully. However given the progressive importance of web scraping and its market capitalization it is expected that major players will take precautions and manage their servers with stricter rules. The market orientation according to the author seems to subdivide players into two categories, the bigger ones ([Idealista.com](https://www.idealista.com/), [trovit](https://www.trovit.it/)) operating on more than one country i.e. Italy, Spain for which barriers have already been raised. And the smaller ones i.e. [casa.it](https://www.casa.it/) [soloaffitti.it](https://www.soloaffitti.it/) whose domain is ending with .it and whose barriers are not existing yet. Though jurisprudential guidelines lately have judged in favor of scrapers preventing the contestor from disallowing the access to their platform. As a matter of fact both of the parts need to meet halfway since there are a palette of greys between who is sneaking sensible data and who is benefiting from open data and be to the advantage of others through its externalities. Aside from this fact a major effort was put into making the disseration documents portable (\@ref(Infrastructure)) and available into a [website](https://niccolosalvini.github.io/thesis/), strechting to the maximun the concept of collaborative environment and open source. As a curiosity the author has also monitored through time the website audience embedding Google Analytics. What it has been observed in 5 months (Sep to Jan) is a modest international crowd coming from three continents (central panel fig. \@ref(fig:gaanalytics)) the most operating on desktop devices, even though a substantial slice browsed the thesis from their smartphone. The top location was Florence (hometown) with 19 access, immediately followed by Rome, Milan and Quincy (Massachusetts, USA). 

<!-- - la api funziona riesce quasi sempre a deliverare i dati ed ad evitare in incorrere in fails. il secondo endpoint tuttavia richiede ragionevolmente più tempo perchè le covariate sono di più, tuttavia rispetto al rpimo èin media iuù accurato nel reperire i dati. DA alcuen delle sattistiche proposte in sede di trattazione del web scarping si possonod desumenre le performance generali in termini di latenza, su cui per sicurezza è stata fatta un'analisi benchmark. i risulatati comunicano che per 250 osservazioni per 6 colonne (10 pagine specificate, parametro npages) il primo endpoint richiede per la prima richiesra circa 5 secondi (4.42 sec elapsed, compatibile con l'pertura delle sessioni in parallelo) nelle successive una media di 1.20 (ricordando che la cache tiene conto dell'ultima richiesta e se richiamta viene restituita). questo andamento non spiegato viene evidenziato in tutte le query successive alla prima per cuo viene specificato iun nuovo numero di pagine. per query che interrogano 100 pagine equivelnti a 2500 osservazioni il temp linermente richiedendo 15 secondi. il secondo endpoint, più lento, chiede 15 secondi per 10 pagine a parità di osservazioni ma con 40 covariate e poco oltre 1 minuto per 100 pagine. Le performances vengono riassunte nella tabella sottostante. TABELLA -->

<!-- The total cost on the infrastructure results in a considerably low amount of money, a breakdown of the cost comprises a couple of expenses: domain name AWS Route 53 (api addres) and 5 euro total of hosting on AWS in 6 months. The overrun was due an intense testing phase that justifies the extra charge on a free tier service machine. Other cloud solutions have been contemples (Googple Platform,  Azure), however AWS is well suited and well integrated upstream and downstream. Intellectual regour would say also that AWS hAs been first choice since the author have already been exposed to the technology.  -->


<!-- Il costo totale dell'infrastruttura risulta essere basso (grazie all'ampio utilizzo di Sosftware open source) e attribuinbile ad una sola voce di spesa, i.e. aws per cui un dominio 15 euro, e uno sforamento mensile dello usage (fase di testing) in rifereimento al contratto free tier circa 5 euro. Altre soluzioni come GCP o come Azure avrebbero richiesto stesse cifre per medesimi servizi, tuttavia amazon offre una stack di servizi integrata a monte e a valle, i.e. l'acquisto del dominio che l'ha resa preferitA rispetto ai competittors. la preferenza, per onestà intellettuale, rigurda anche la ffamiliarità con i servizi. -->

<!-- utilizzo per ora dell'api: l'implementazione di logginin in \@ref(logging) ha permesso un monitoraggio passivo del servizio eed ha evideniato un timido utilizzo da parte di soggetti relativi alla tesi e qualche contributore curioso, tuttavia non è ancora stata pubblicizzata. l'impiego di loadtest in ogni caso ha permesso di stimar ela latenza media per varie richiesto quando l'api è sotto stress, evidenziando anche il buon lavoro svolto da nginx. non sono stati registrati attacchi malicious di nessun tipo.  -->

<!-- profili legali: grazie al precauzioni prese e alla quasi totale assenza di controllo l'api runna smooth, tuttavia vista la forte crescita del mercato dello scraping è possibile supporre che immobiliare.it prenda provvedimenti sul lato security. Le accortezze server side in ogni caso accomodano immobiliare permettendo akll'api di non pesare al sito (overload) e come sottolineato nelle sezione \@ref(legal)  \@ref(best-practices) è necessario trovare un compromesso tra chi ruba e chi rende libero un mercato che nasce con l'idea di essere libero. La giurisprudenza italiana non si è ancora trivata a fare i conti con cause civili e penali di questo genere, ma i tribunali internazioni hanno dato di recente un orientamento giurisprudenziale in favore di parte passiva, lo scraper. per l'esperienza dell'autore di questa analisi i siti che hanno più apertura intenazionale (si muovono su più mercati oin più paesi) è più facile che abbiano un meccanismo di protezione.  -->

<!-- il sito della tesie monitoraggio. Il sito della tesi come espresso in breve all'inizio di \@ref(Infrastructure) ha visto invece una notevole platea da diverse regioni del mondo. Infatti l'utilizzo di google analytics sulla stess ha permesso di monitorare anche gli utenti che curiosavano sulla scrittura in itinere della tesi. Il traffico sul sito della tesi sfrutta l'api di google analytics in figura \@ref(gaanalytics). Le analoitiche degli utenti sono sporcate per quanto riguarda il segmento italia dall'autore dell'analisi che usa diversi devices per entrare sul solito sito, di conseguenza viene registrato due volte. -->


![(#fig:gaanalytics)Google Analyitics Dashoaboard for site logs, author's source](images/analytics_dashbboard.jpg)

```{r googleanalytics, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
library(googleAnalyticsR, quietly = F, warn.conflicts = F)
library(dplyr, quietly = F, warn.conflicts = F)
library(ggplot2, quietly = F, warn.conflicts = F)
library(lubridate, quietly = F, warn.conflicts = F)
library(reactable, quietly = F, warn.conflicts = F)
library(stringr, quietly = F, warn.conflicts = F)


## follow this link belo
## http://code.markedmondson.me/googleAnalyticsR/articles/setup.html


ga_auth(email="niccolo.salvini27@gmail.com")
ga_auth_setup() 

googleAuthR::gar_set_client(json = ".secrets/oauth-account-key.json")

## Now, provide the service account email and private key
ga_auth(email = "ga-analysis@test-api-for-reporting.iam.gserviceaccount.com",
        json_file = ".secrets/test-api-for-reporting.json")

## At this point, we should be properly authenticated and ready to go. We can test this
## by getting a list of all the accounts that this test project has access to. Typically,
## this will be only one if you've created your own service key. If it isn't your only
## account, select the appropriate viewId from your list of accounts.

my_accounts <- ga_account_list()
my_id <- my_accounts$viewId     ## Modify this if you have more than one account

## Let's look at all the visitors to our site. This segment is one of several provided
## by Google Analytics by default.

all_users <- segment_ga4("AllTraffic", segment_id = "gaid::-1")

## Let's look at just one day.

ga_start_date <- today()
ga_end_date <- today()

## Make the request to GA
data_fetch <- google_analytics(my_id,
                               segments = all_users,
                               date_range = c(ga_start_date, ga_end_date),
                               metrics = c("pageviews"),
                               dimensions = c("landingPagePath"),
                               anti_sample = TRUE)

## Let's just create a table of the most viewed posts

most_viewed_posts <- data_fetch %>% 
  mutate(Path = str_trunc(landingPagePath, width=40)) %>% 
  count(Path, wt=pageviews, sort=TRUE)
head(most_viewed_posts, n=5)


```


considerazione sul modello INLA e risultati finali...


